---
title: "Appliquer les concepts étudiés à un projet de data science"
author: "Romain Avouac et Lino Galiana"
draft: false
# layout options: single, single-sidebar
layout: single
from: markdown+emoji
---



L'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.

Nous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science.
On a un notebook un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :

- Import de données ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entraînement d'un modèle ;
- Evaluation du modèle

**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée.** 

::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines étapes peuvent être rapides, d'autres plus fastidieuses ;
certaines être assez guidées, d'autres vous laisser plus de liberté.
Si vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à
l'étape suivante qui en dépend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous 
recommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les 
outils nécessaires sont pré-installés et pré-configurés. 
:::






# Partie 1 : qualité du script

Cette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours.

Elle fait intervenir les notions suivantes : 

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.html)) ;
- **Qualité du code** (voir [Qualité du code](/chapters/code-quality.html)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contrôle de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).

Le plan de la partie est le suivant :

<!----
0. :zero: _Forker_ le dépôt et créer une branche de travail
1. :one: S'assurer que le _notebook_ s'exécute correctement
2. :two: Modularisation : mise en fonctions et mise en module
3. :three: Utiliser un `main` script
4. :four:  Appliquer les standards de qualité de code
5. :five: Adopter une architecture standardisée de projet
6. :six: Fixer l'environnement d'exécution
7. :seven: Stocker les données de manière externe
8. :eight: Nettoyer le projet `Git`
9. :nine: Ouvrir une *pull request* sur le dépôt du projet.
------------->

Nous allons partir de ce _Notebook_ `Jupyter`,
que vous pouvez prévisualiser voire tester
en cliquant sur l'un des liens suivants:




<a href="https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=false&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fensae-reproductibilite-website%2Fmaster%2Fpreview-notebook.sh%C2%BB" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSPcloud-Tester%20notebook%20sur%20SSP--cloud-informational&amp;color=yellow?logo=Python" alt="Onyxia"></a>
<a href="http://colab.research.google.com/github/linogaliana/ensae-reproductibilite-application/blob/main/titanic.ipynb" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>


## Etape 0: forker le dépôt d'exemple et créer une branche de travail



- Ouvrir un service `VSCode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller
dans la page `My Services` et cliquer sur `New service`. Sinon, vous
pouvez lancer le service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=false).

- Générer un jeton d'accès (*token*) sur `GitHub` afin de permettre l'authentification en ligne de commande à votre compte.
La procédure est décrite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). Garder le jeton généré de côté.

- Forker le dépôt `Github` : https://github.com/linogaliana/ensae-reproductibilite-application

- Clôner __votre__ dépôt `Github` en utilisant le
terminal depuis `Visual Studio` (`Terminal > New Terminal`) :

```shell
$ git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application.git
```

où `<TOKEN>` et `<USERNAME>` sont à remplacer, respectivement, 
par le jeton que vous avez généré précédemment et votre nom d'utilisateur.

- Se placer avec le terminal dans le dossier en question : 

```shell
$ cd ensae-reproductibilite-application
```

- Créez une branche `nettoyage` :

```shell
$ git checkout -b nettoyage
Switched to a new branch 'nettoyage'
```

## Etape 1 : s'assurer que le script s'exécute correctement

On va partir du fichier `notebook.py` qui reprend le contenu 
du _notebook_[^jupytext] mais dans un script classique.

[^jupytext]: L'export dans un script `.py` a été fait
        avec [`Jupytext`](https://jupytext.readthedocs.io/en/latest/index.html). Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette étape et fournissons
        directement le script. Mais n'oubliez
        pas que cette démarche, fréquente quand on a démarré sur un _notebook_ et
        qu'on désire consolider en faisant la transition vers des 
        scripts, nécessite d'être attentif pour ne pas risquer de faire une erreur. 

La première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**. 


::: {.callout-tip}
## Application 1: corriger les erreurs

- Ouvrir dans `VSCode` le script `titanic.py` ;
- Exécuter le script ligne à ligne pour détecter les erreurs ;
- Corriger les deux erreurs qui empêchent la bonne exécution ;
- Vérifier le fonctionnement du script en utilisant la ligne de commande

```shell
python titanic.py
```

:::


Il est maintenant temps de *commit* les changements effectués avec `Git`[^2] :

[^2]: Essayez de *commit* vos changements à chaque étape de l'exercice, c'est une bonne habitude à prendre.

```shell
$ git add titanic.py
$ git commit -m "Corrige l'erreur qui empêchait l'exécution"
$ git push
```

::: {.callout-caution collapse="true"}
## Checkpoint

[Script _checkpoint_](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application1/titanic.py)
:::


## Etape 2: utiliser un _linter_ puis un _formatter_

On va maintenant améliorer la qualité de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/). 

::: {.callout-note}
N'hésitez pas à taper un code d'erreur sur un moteur de recherche pour obtenir plus d'informations si jamais le message n'est pas clair !
:::

Pour appliquer le _linter_ à un script `.py`,
la syntaxe à entrer dans le terminal est la suivante : 

```shell
$ pylint mon_script.py
```

::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)
sont des _packages_ `Python` qui 
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui suggère
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)
ou [`Black`](https://black.readthedocs.io/en/stable/),
n'oubliez pas d'exécuter la commande `pip install pylint` ou `pip install black`.
:::


Le _linter_ renvoie alors une série d'irrégularités,
en précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualité du code à l'aune des standards communautaires évoqués
dans la partie [Qualité du code](/chapters/code-quality.html).


::: {.callout-tip}
## Application 2: rendre lisible le script

- Diagnostiquer et évaluer la qualité de `titanic.py` avec [`PyLint`](https://pylint.readthedocs.io/en/latest/). Regarder la note obtenue.
- Utiliser `black titanic.py --diff --color` pour observer les changements de forme que va induire l'utilisation du _formatter_ [`Black`](https://black.readthedocs.io/en/stable/)
- Appliquer le _formatter_ [`Black`](https://black.readthedocs.io/en/stable/)
- Réutiliser [`PyLint`](https://pylint.readthedocs.io/en/latest/) pour diagnostiquer l'amélioration de la qualité du script et le travail qui reste à faire. 
- Comme la majorité du travail restant est à consacrer aux imports:
    - Mettre tous les _imports_ ensemble en début de script
    - Retirer les _imports_ redondants en s'aidant des diagnostics de votre éditeur
    - Réordonner les _imports_ si [`PyLint`](https://pylint.readthedocs.io/en/latest/) vous indique de le faire
    - Corriger les dernières fautes formelles suggérées par [`PyLint`](https://pylint.readthedocs.io/en/latest/)
- Délimiter des parties dans votre code pour rendre sa structure plus lisible 
:::

Le code est maintenant lisible, il obtient à ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment 
beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. 
Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script: 
jetons d'API et chemin des fichiers.


::: {.callout-caution collapse="true"}
## Checkpoint

[`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application2/titanic.py)
:::

## Etape 3: gestion des paramètres

L'exécution du code et les résultats obtenus
dépendent de certains paramètres. L'étude de résultats
alternatifs, en jouant sur 
des variantes des paramètres, est à ce stade compliquée
car il est nécessaire de parcourir le code pour trouver
ces paramètres. De plus, certains paramètres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation à 
être présents dans le code. 

Il est plus judicieux de considérer ces paramètres comme des
variables d'entrée du script. Cela peut être fait de deux
manières:

1. Avec des arguments optionnels appelés depuis la ligne de commande.
Cela peut être pratique pour mettre en oeuvre des tests automatisés[^noteCI] mais
n'est pas forcément pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un fichier de configuration dont les valeurs sont importées dans
le script principal. Nous allons le mettre en oeuvre pour deux types de fichiers:
les éléments de configuration à partager et ceux à conserver pour soi mais 
pouvant servir.

[^noteCI]: Nous le verrons lorsque nous mettrons en oeuvre l'intégration continue.

::: {.callout-tip}
## Application 3: Paramétrisation du script

1. En s'inspirant de [cette réponse](https://stackoverflow.com/a/69377311/9197726), 
créer une variable `n_trees` qui peut éventuellement être paramétrée en ligne de commande
et dont la valeur par défaut est 20.
2. Tester cette paramétrisation en ligne de commande avec la valeur par défaut
puis 2, 10 et 50 arbres
3. Repérer le jeton d’API dans le code. Retirer le jeton d’API du code
et créer à la racine du projet un fichier YAML nommé `secrets.yaml`
où vous écrivez ce secret sous la forme `key: value`
4. Pour éviter d'avoir à le faire plus tard,
créer une fonction `import_yaml_config` qui prend en argument le
chemin d'un fichier `YAML`
et renvoie le contenu de celui-ci en _output_. Vous pouvez suivre
le conseil du chapitre sur la [Qualité du code](/chapters/code-quality.html)
en adoptant le _type hinting_.
5. Créer la variable `API_TOKEN` ayant la valeur stockée dans `secrets.yaml`[^fileexist].
5. Tester en ligne de commande que l'exécution du fichier est toujours
sans erreur
6. Refaire un diagnostic avec [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et corriger les éventuels messages. 
7. Créer un fichier `config.yaml` stockant trois informations: le chemin des données
d'entraînement, des données de test et la répartition train/test utilisée dans le code. 
Créer les variables correspondantes dans le code après avoir utilisé `import_yaml_config`
8. Créer un fichier `.gitignore`. Ajouter dans ce fichier `secrets.yaml`
car il ne faut pas committer ce fichier.
8. Créer un fichier `README.md` où vous indiquez qu'il faut créer un fichier `secrets.yaml` pour
pouvoir utiliser l'API. 

[^fileexist]: Ici, le jeton d'API n'est pas indispensable pour que le code
    fonctionne. Afin d'éviter une erreur non nécessaire
    lorsqu'on automatisera le processus, on peut
    créer une condition qui vérifie la présence ou non de ce fichier. 
    
    Cela peut être fait avec la fonction `os.path.exists` :

        if os.path.exists('secrets.yaml'):
            secrets = import_yaml_config("secrets.yaml")

    La variable `secrets` n'existera que dans le cas où un fichier `secrets.yaml` existe. 
    Le script reste donc reproductible même pour un utilisateur n'ayant pas le fichier
    `secrets.yaml`. 

<details>
<summary>Indice si vous ne trouvez pas comment lire un fichier `YAML`</summary>
Si le fichier s'appelle `toto.yaml`, vous pouvez l'importer de cette manière:
```python
with open("toto.yaml", "r", encoding="utf-8") as stream:
    dict_config = yaml.safe_load(stream)
```
</details>

:::


::: {.callout-caution collapse="true"}
## Checkpoint

- [`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/titanic.py)
- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)


:::


## Etape 4 : Adopter la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook**.

Cet exercice étant chronophage, il n'est __pas obligatoire de le réaliser en entier__. L'important est de
comprendre la démarche et d'adopter fréquemment une approche fonctionnelle[^POO]. Pour obtenir 
une chaine entièrement fonctionnalisée, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orientée objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au développeur. L'arbitrage coût-avantage est négatif pour notre
exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d'un modèle,
il est recommandé de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/). 

::: {.callout-tip}
## Application 4: adoption des standards de programmation fonctionnelle 

- Créer une fonction qui importe les données d'entraînement (`train.csv`) et de test (`test.csv`) et renvoie des `DataFrames` `Pandas` ;
- En fonction du temps disponible, créer plusieurs fonctions pour réaliser les étapes de *feature engineering*:
    + La création de la variable _"Title"_ peut être automatisée en vertu du principe _"do not repeat yourself"_[^notepandas].
    + Regrouper ensemble les `fillna` et essayer de créer une fonction généralisant l'opération. 
    + Les _label encoders_ peuvent être transformés en deux fonctions: une première pour encoder une colonne puis une seconde qui utilise
    la première de manière répétée pour encoder plusieurs colonnes. _Remarquez les erreurs de copier-coller que cela corrige_
    + Finaliser les dernières transformations avec des fonctions
- Créer une fonction qui réalise le *split train/test* de validation en fonction d'un paramètre représentant la proportion de l'échantillon de test.
- Créer une fonction qui entraîne et évalue un classifieur `RandomForest`, et qui prend en paramètre le nombre d'arbres (`n_estimators`). La fonction doit imprimer à la fin la performance obtenue et la matrice de confusion.
- Déplacer toutes les fonctions ensemble, en début de script.
<!----- plus tard ?
- Mettre ces fonctions dans un module `functions.py`
- importer les fonctions via le module dans le notebook et vérifier que l'on retrouve bien les différents résultats en utilisant les fonctions.
------->
:::

[^notepandas]: Au passage vous pouvez noter que mauvaises pratiques discutables,
    peuvent
    être corrigées, notamment l'utilisation excessive de `apply` là où
    il serait possible d'utiliser des méthodes embarquées par `Pandas`.
    Cela est plutôt de l'ordre du bon style de programmation que de la
    qualité formelle du script. Ce n'est donc pas obligatoire mais c'est mieux. 


::: {.callout-important}
Le fait d'appliquer des fonctions a déjà amélioré la fiabilité du processus
en réduisant le nombre d'erreurs de copier-coller. Néanmoins, pour vraiment
fiabiliser le processus, il faudrait utiliser un _pipeline_ de transformations
de données. 

Ceci n'est pas encore au programme du cours mais le sera dans une prochaine 
version. 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

- [`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application4/titanic.py)

Les autres fichiers inchangés:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::



# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est déjà considérablement rapprochés d'un
possible partage du code : celui-ci est lisible et intelligible. 
Le code est proprement versionné sur un
dépôt `GitHub`.
 
Néanmoins,
la structure du projet n'est pas encore normalisée. 
De plus, 
l'adoption d'une structure plus modulaire facilitera
la compréhension de la chaine de traitement.


## Etape 1 : modularisation

Fini le temps de l'expérimentation : on va maintenant essayer de se passer complètement du _notebook_.
Pour cela, on va utiliser un `main` script, c'est à dire un script qui reproduit l'analyse en important et en exécutant les différentes fonctions dans l'ordre attendu.


::: {.callout-tip}
## Application 5: modularisation

- Déplacer les fonctions dans une série de fichiers dédiés:
    +  `import_data.py`: fonctions d'import de données 
    +  `build_features.py`: fonctions regroupant les étapes de _feature engineering_ 
    +  `train_evaluate.py`: fonctions d'entrainement et d'évaluation du modèle
- Spécifier les dépendances (i.e. les packages à importer)
dans les modules pour que ceux-ci puissent s'exécuter indépendamment ;
- Renommer `titanic.py` en `main.py` pour suivre la convention de nommage des projets `Python` ;
- Importer les fonctions nécessaires à partir des modules. ⚠️ Ne pas utiliser `from XXX import *`, ce n'est pas une bonne pratique ! 
- Vérifier que tout fonctionne bien en exécutant le _script_ `main` à partir de la ligne de commande :

```shell
$ python main.py
```
:::

On dispose maintenant d'une application `Python` fonctionnelle. 
Néanmoins, le projet est certes plus fiable mais sa structuration
laisse à désirer et il serait difficile de rentrer à nouveau
dans le projet dans quelques temps. 

<details>
<summary>Etat actuel du projet 🙈</summary>

```shell
├── README.md
├── train.csv
├── test.csv
├── .gitignore
├── config.yaml
├── secrets.yaml
├── import_data.py
├── build_features.py
├── train_evaluate.py
└──main.py
```

</details>

Comme cela est expliqué dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va 
faciliter l'autodocumentation de notre projet.  

De plus, une telle structure va faciliter des évolutions optionnelles
comme la packagisation du projet. Passer d'une structure modulaire
bien faite à un _package_ est quasi-immédiat en `Python`. 

::: {.callout-caution collapse="true"}
## Checkpoint

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers inchangés:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::

## Etape 2 : adopter une architecture standardisée de projet

On va maintenant modifier l'architecture de notre projet pour la rendre plus standardisée.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui génèrent des _templates_ de projet.

On va s'inspirer de la structure du [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
développé par la communauté.

::: {.callout-note}
L'idée de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante : 

```shell
$ pip install cookiecutter
$ cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

```shell
ensae-reproductibilite-application
├── main.py
├── README.md
├── data
│   └── raw
│       ├── test.csv
│       └── train.csv
├── configuration
│   ├── secrets.yaml
│   └── config.yaml
├── notebooks
│   └── titanic.ipynb
└── src
    ├── data
    │   └── import_data.py
    ├── features
    │   └── build_features.py
    └── models
        └── train_evaluate.py
```

::: {.callout-tip}

## Application 6: adopter une structure lisible

- _(optionnel)_ Analyser et comprendre la [structure de projet](https://drivendata.github.io/cookiecutter-data-science/#directory-structure) proposée par le template
- Modifier l'arborescence du projet selon le modèle
- Adapter les scripts et les fichiers de configuration à la nouvelle arborescence
- Ajouter le dossier __pycache__ au `.gitignore`[^pycache] et le dossier `data`
:::

[^pycache]: Il est normal d'avoir des dossiers `__pycache__` qui traînent : ils se créent automatiquement à l'exécution d'un script en `Python`.

::: {.callout-caution collapse="true"}
## Checkpoint

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers sont inchangés, à l'exception de leur emplacement.

:::

### Etape 3: indiquer l'environnement minimal de reproductibilité

Le script `main.py` nécessite un certain nombre de packages pour
être fonctionnel. Chez vous les packages nécessaires sont
bien sûr installés mais êtes-vous assuré que c'est le cas 
chez la personne qui testera votre code ? 

Afin de favoriser la portabilité du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-à-dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version.
Nous proposons de créer un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacrée aux environnements reproductibles. 

Le fichier `requirements.txt` est conventionnellement localisé à la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier plus tard.

::: {.callout-tip}

## Application 7: création du `requirements.txt`

- Créer un fichier `requirements.txt` avec la liste des packages nécessaires
- Ajouter une indication dans `README.md` sur l'installation des _packages_ grâce au fichier `requirements.txt` 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

- [`requirements.txt`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/requirements.txt)
- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/README.md)

:::

## Etape 4 : stocker les données de manière externe {#stockageS3}


::: {.callout-warning}
Pour mettre en oeuvre cette étape, il peut être utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://docs.sspcloud.fr/onyxia-guide/stockage-de-donnees) pour la réaliser. Une aide-mémoire est également disponible dans le cours
de 2e année de l'ENSAE [Python pour la data science](https://linogaliana-teaching.netlify.app/reads3/#)
:::

Comme on l'a vu dans le cours ([partie structure des projets](/chapters/project-structure.html)),
les données ne sont pas censées être versionnées sur un projet `Git`.

L'idéal pour éviter cela tout en maintenant la reproductibilité est d'utiliser une solution de stockage externe.
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. 

::: {.callout-tip}

## Application 8: utilisation d'un système de stockage distant

A partir de la ligne de commande,
utiliser l'utilitaire [MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html)
pour copier les données `data/raw/train.csv` et `data/raw/test.csv` vers votre
bucket personnel, respectivement dans les dossiers `ensae-reproductibilite/data/raw/train.csv`
et `ensae-reproductibilite/data/raw/test.csv`. 

<details>
<summary>Indice</summary>

Structure à adopter:

```shell
$ mc cp data/raw/train.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv
$ mc cp data/raw/test.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv
```

en modifiant l'emplacement de votre bucket personnel
</details>

- Pour se simplifier la vie, on va utiliser des URL de téléchargement des fichiers
(comme si ceux-ci étaient sur n'importe quel espace de stockage) plutôt que d'utiliser
une librairie `S3` compatible comme `boto3` ou `s3fs`. Pour cela, en ligne de
commande, faire:

```shell
mc anonymous set download s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/
```

en modifiant `<BUCKET_PERSONNEL>`. Les URL de téléchargement seront de la forme 
`https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv`
et `https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv`

- Modifier `configuration.yaml` pour utiliser directement les URL dans l'import 
- Supprimer les fichiers `.csv` du dossier `data` de votre projet, on n'en a plus besoin vu qu'on les importe de l'extérieur
- Vérifier le bon fonctionnement de votre application
:::

::: {.callout-caution collapse="true"}
## Checkpoint

- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application8/config.yaml)

:::

# Partie 2bis: packagisation de son projet (optionnel)

Cette série d'actions n'est pas forcément pertinente pour tous
les projets. Elle fait un peu la transition entre la modularité
et la portabilité. 

## Etape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions génériques.
On peut vouloir tester leur usage sur des données standardisées,
différentes de celles du Titanic.

Même si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut être pédagogique. 

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
et `pytest`
pour effectuer des tests unitaires. Cette approche nécessite une maîtrise 
de la programmation orientée objet.

::: {.callout-tip}

## Application 9: test unitaire _(optionnel)_

Dans le dossier `src/data/`, créer un fichier `test_create_variable_title.py`[^emplacement].

En s'inspirant de l'[exemple de base](https://docs.python.org/3/library/unittest.html#basic-example),
créer une classe `TestCreateVariableTitle` qui effectue les opérations suivantes:

- Création d'une fonction `test_create_variable_title_default_variable_name` qui permet 
de comparer les objets suivants:

    + Création d'un `DataFrame` de test :  

    ```python
    df = pd.DataFrame({
                'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',
                        'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath (Lily May Peel)',
                        'Allen, Mr. William Henry', 'Moran, Mr. James',
                        'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',
                        'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',
                        'Nasser, Mrs. Nicholas (Adele Achem)'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

    + Utilisation de la fonction `create_variable_title` sur ce `DataFrame`
    + Comparaison au `DataFrame` attendu:

    ```python
    expected_result = pd.DataFrame({
                'Title': ['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.', 'Mr.', 'Mr.', 'Master.', 'Mrs.', 'Mrs.'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

- Effectuer le test unitaire en ligne de commande avec `unittest`. Corriger le test unitaire en cas d'erreur. 
- Si le temps le permet, proposer des variantes pour tenir compte de paramètres (comme la variable `variable_name`)
ou d'exceptions (comme la gestion du cas _"Dona"_)
:::

::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche généralement
à tester le plus de lignes possibles de son code. On parle de
taux de couverture (_coverage rate_) pour désigner
la statistique mesurant cela. 

Cela peut s'effectuer de la manière suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```shell
$ coverage run -m pytest test_create_variable_title.py
$ coverage report -m

Name                            Stmts   Miss  Cover   Missing
-------------------------------------------------------------
import_data.py                     15      6    60%   16-19, 31-34
test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------
TOTAL                              36      7    81%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualité. Il existe d'ailleurs
des badges `Github` dédiés. 
:::

[^emplacement]: L'emplacement de ce fichier est amené à évoluer dans le cadre
    d'une packagisation. Dans un package, ces tests seront dans un dossier
    spécifique `/tests` car `Python` sait gérer de manière plus formelle
    les imports de fonctions depuis des modules. Ici, on est dans une 
    situation transitoire, raison pour laquelle les tests
    sont dans les mêmes dossiers que les fonctions. 

::: {.callout-caution collapse="true"}
## Checkpoint

- [`test_create_variable_title.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application9/test_create_variable_title.py)

Les autres fichiers sont inchangés.

:::


## Etape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple à transformer
en package, en s'inspirant du `cookiecutter` adapté, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

<details>
<summary>Structure visée</summary>

```shell
ensae-reproductibilite-application
├── docs                                    ┐ 
│   ├── main.py                             │ 
│   └── notebooks                           │ Package documentation and examples
│       ├── titanic.ipynb                   │ 
├── README.md                               ┘ 
├── pyproject.toml                          ┐ 
├── requirements.txt                        │
├── src                                     │
│   └── titanicml                           │ Package source code, metadata,
│       ├── __init__.py                     │ and build instructions 
│       ├── config.yaml                     │
│       ├── import_data.py                  │
│       ├── build_features.py               │ 
│       └── train_evaluate.py               ┘
└── tests                                   ┐
    └── test_create_variable_title.py       ┘ Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```shell
ensae-reproductibilite-application
├── notebooks                                 
│   └── titanic.ipynb                  
├── configuration                                 
│   └── config.yaml                  
├── main.py                              
├── README.md                 
├── requirements.txt                      
└── src 
    ├── data                                
    │   ├── import_data.py                    
    │   └── test_create_variable_title.py      
    ├── features                           
    │   └── build_features.py      
    └── models                          
        └── train_evaluate.py              
```
</details>

::: {.callout-tip}

## Application 10: packagisation _(optionnel)_

- Déplacer les fichiers dans le dossier `src` pour respecter la nouvelle
arborescence ;
- Dans `src/titanicml`, créer un fichier vide `__init__.py`[^init] ;
- Déplacer le fichier de configuration dans le _package_ (nécessaire à la reproductibilité) ;
- Créer le dossier `docs` et mettre les fichiers indiqués dedans
- Modifier `src/titanicml/import_data.py` :
    + Ajouter la variable `config_file = os.path.join(os.path.dirname(__file__), "config.yaml")`. Cela permettra d'utiliser directement le fichier ;
    + Proposer un argument par défaut à la fonction `import_config_yaml` égal à `config_file`
- Créer un fichier `pyproject.toml` à partir du contenu de [ce modèle de `pyproject`](https://github.com/linogaliana/ensae-reproductibilite-application/blob/main/checkpoints/application10/pyproject.toml)[^setuptools]
- Installer le package en local avec `pip install .`
- Modifier le contenu de `docs/main.py` pour importer les fonctions de notre _package_ `titanicml` et tester en 
ligne de commande notre fichier `main.py`
:::

[^init]: Le fichier `__init__.py` indique à `Python` que le dossier
est un _package_. Il permet de proposer certaines configurations
lors de l'import du _package_. Il permet également de contrôler
les objets exportés (c'est-à-dire mis à disposition de l'utilisateur)
par le _package_ par rapport aux objets internes au _package_. 
En le laissant vide, nous allons utiliser ce fichier 
pour importer l'ensemble des fonctions de nos sous-modules. 
Ce n'est pas la meilleure pratique mais un contrôle plus fin des
objets exportés demanderait un investissement qui ne vaut, ici, pas
le coût. 


[^setuptools]: Ce `pyproject.toml` est un modèle qui utilise `setuptools`
    pour _build_ le _package_. C'est l'outil classique. 
    Néanmoins, pour des usages plus raffinés, 
    il peut être utile d'utiliser [`poetry`](https://python-poetry.org/)
    qui propose des fonctionnalités plus complètes.  


::: {.callout-note}

Pour créer la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapté,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a déjà une structure très modulaire, on va plutôt recréer cette
structure dans notre projet déjà existant. En fait, il ne manque qu'un fichier essentiel, 
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```shell
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>Dérouler pour voir les choix possibles</summary>
```shell
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]: 
python_version [3.9]: 
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]: 
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::


<!-----
## Etape 9 : ouvrir une *pull request* sur le dépôt du projet

Enfin terminé ! Enfin presque... On s'est donné beaucoup de mal à nettoyer ce dépôt et le mettre aux standards, autant valoriser ce travail. On va pour cela faire une *pull request* sur le [dépôt du projet initial](https://github.com/avouacr/ensae-reproductibilite-projet), c'est à dire proposer à l'auteur d'intégrer tous les changements que vous avez effectué en committant à chaque étape. 

Suivre la procédure décrite dans la [documentation GitHub](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) pour créer une *pull request* à partir de votre *fork*. Pour la branche *upstream* (le dépôt cible), on va choisir `master`. Par contre, pour la branche locale (celle sur votre dépôt), on va choisir la branche `nettoyage`.

Si tout s'est bien passé, vous devriez à présent voir votre *pull request* sur le dépôt cible ([ici](https://github.com/avouacr/ensae-reproductibilite-projet/pulls)). Bravo, vous venez de faire votre première contribution à l'open source !

{{% box status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Faire une *pull request* via la branche `master` d’un *fork* est très mal vu. En effet, il faut souvent faire des contorsionnements pour réussir à faire coïncider deux histoires qui n’ont pas de raison de coïncider. On s'évite beaucoup de problèmes en prenant l'habitude de toujours faire ses *pull requests* à partir d'une autre branche que `master`.
{{% /box %}}
------------------>

# Partie 2 : construction d'un projet portable et reproductible {#partie3}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est déjà considérablement rapprochés d'une
possible mise en production : le code est lisible,
la structure du projet est normalisée et évolutive,
et le code est proprement versionné sur un
dépôt `GitHub` <i class="fab fa-github"></i>.

A présent, nous avons une version du projet qui est largement partageable.
Du moins en théorie, car la pratique est souvent plus compliquée : il y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.

Dans cette seconde partie, nous allons voir comment **normaliser l'environnement d'exécution afin de produire un projet portable**. On sera alors tout proche de pouvoir mettre le projet en production.
On progressera dans l'échelle de la reproductibilité 
de la manière suivante: 
- :one: [**Gérer des variables d'environnement hors du code**](#configyaml) ;
- :two: [**Environnements virtuels**](#anaconda) ;
- :three: [**Images et conteneurs `Docker`**](#docker).


## Etape 1: créer un répertoire de variables servant d'input {#configyaml}

### Enjeu

Lors de l'[étape 7](#stockageS3), nous avons amélioré la qualité du script en 
séparant stockage et code. Cependant, peut-être avez-vous remarqué
que nous avons introduit un nom de _bucket_ personnel dans le script
(voir [le fichier `main.py`](https://github.com/linogaliana/ensae-reproductibilite-projet-1/blob/v7/main.py#L9)).
Il s'agit typiquement du genre de petit vice caché d'un script qui peut 
générer une erreur: vous n'avez pas accès au bucket en question donc
si vous essayez de faire tourner ce script en l'état, vous allez rencontrer
une erreur.

Une bonne pratique pour gérer ce type de configuration est d'utiliser un 
fichier `YAML` qui stocke de manière hiérarchisée les variables globales
[^3].

[^3]: Le format `YAML` est un format de fichier où les informations sont 
hiérarchisées. Avec le _package_ `YAML` on peut très facilement le transformer
en `dict`, ce qui est très pratique pour accéder à une information.

En l'occurrence, nous n'avons besoin que de deux éléments pour pouvoir
dé-personnaliser ce script :

- le nom du bucket
- l'emplacement dans le bucket

### Application

Dans `VSCode`, créer un fichier nommé `config.yaml` et le localiser à la racine
de votre dépôt. Voici, une proposition de hiérarchisation de l'information
que vous devez adapter à votre nom d'utilisateur :

```yaml
input:
  bucket: "lgaliana"
  path: "ensae-reproductibilite"
```

Dans `main.py`, importer ce fichier et remplacer la ligne précédemment
évoquée par les valeurs du fichier. Tester en faisant tourner `main.py`
<!-----
https://github.com/linogaliana/ensae-reproductibilite-projet-1/commit/4a9d935223b6af366d4cf2a2a208d98a25407fc6
----->

## Etape 2 :  créer un environnement conda à partir du fichier `environment.yml` {#anaconda}

L'environnement `conda` créé avec `conda env export` ([étape 6](#conda-export))
contient énormément
de dépendances, dont de nombreuses qui ne nous sont pas nécessaires (il 
en serait de même avec `pip freeze`). 
Nous n'avons en effet besoin que des _packages_ présents dans la
section `import` de nos scripts et les dépendances nécessaires
pour que ces _packages_ soient fonctionnels.


Vous allez chercher à obtenir
un `environment.yml` beaucoup plus parcimonieux
que celui généré par `conda env export`

{{< panelset class="simplification" >}}

{{% panel name="Approche générale :koala: " %}}

Le tableau récapitulatif présent dans
la [partie portabilité](/portability/#aide-mémoire)
peut être utile dans cette partie. L'idée est 
de partir _from scratch_ et figer l'environnement qui
permet d'avoir une appli fonctionnelle. 

* Créer un environnement vide avec `Python 3.10`
<!---
conda create -n monenv python=3.10.0
---->

* Activer cet environnement

* Installer en ligne de commande avec `pip` les packages nécessaires
pour faire tourner votre code

<!---
pip install pandas PyYAML s3fs scikit-learn
---->

* Faire un `pip freeze > requirements.txt` ou 
`conda env export > environment.yml` (privilégier la deuxième option)

* Retirer la section `prefix` (si elle est présente)
et changer la section `name` en `monenv`


{{% /panel %}}


{{% panel name="Approche fainéante :sloth:" %}}

Nous allons générer une version plus minimaliste grâce à
l'utilitaire [`pipreqs`](https://github.com/bndr/pipreqs)

* Installer `pipreqs` en `pip install`
* En ligne de commande, depuis la racine du projet, faire `pipreqs`
* Ouvrir le `requirements.txt` automatiquement généré. Il est beaucoup plus
minimal que celui que vous obtiendriez avec `pip freeze` ou
l'`environment.yml` obtenu à [l'étape 6](#conda-export). 
* Remplacer toute la section `dependencies` du `environment.yml`
par le contenu du `requirements.txt`
(:warning: ne pas oublier l'indentation et le tiret en début de ligne)
* :warning: Modifier le tiret à `scikit learn`. Il ne faut pas un _underscore_ mais
un tiret
* Ajouter la version de python (par exemple `python=3.10.0`)
au début de la section `dependencies`
* Retirer la section `prefix` du fichier `environment.yml` (si elle est présente)
et changer le contenu de la section `name` en `monenv`
* Créer l'environnement
([voir le tableau récapitulatif dans la partie portabilité](/portability/#aide-mémoire))

<!----
conda env create -f environment.yml
------>


{{% /panel %}}

{{% /panelset %}}


Maintenant, il reste à tester si tout fonctionne bien dans notre 
environnement plus minimaliste:

* Activer l'environnement
* Tester votre script en ligne de commande
* Faire un `commit` quand vous êtes contents


## Etape 3: conteneuriser avec Docker <i class="fab fa-docker"></i> {#docker}

### Préliminaire

- Se rendre sur l'environnement bac à sable [Play with Docker](https://labs.play-with-docker.com)
- Dans le terminal `Linux`, cloner votre dépôt `Github`  <i class="fab fa-github"></i>
- Créer via la ligne de commande un fichier `Dockerfile`. Il y a plusieurs manières
de procéder, en voici un exemple:

```shell
echo "#Dockerfile pour reproduire mon super travail" > Dockerfile
```

- Ouvrir ce fichier via l'éditeur proposé par l'environnement bac à sable. 


### Création d'un premier Dockerfile



- :one: Comme couche de départ, partir d'une image légère comme `ubuntu:20.04`
- :two: Dans une deuxième couche, faire un `apt get -y update` et
installer `wget` qui va être nécessaire pour télécharger `Miniconda`
depuis la ligne
de commande 
- :three: Dans la troisième couche, nous allons installer `Miniconda` :
    + Télécharger la dernière version de `Miniconda` avec `wget` depuis
l'url de téléchargement direct https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
    + Installer `Miniconda` dans le chemin `/home/coder/local/bin/conda`
    + Effacer le fichier d'installation pour libérer de la place sur l'image
- :four: En quatrième couche, on va installer `mamba` pour accélérer l'installation
des packages dans notre environnement. 
- :five: En cinquième couche, nous allons créer l'environnement `conda`:
    + Utiliser `COPY` pour que `Docker` soit en mesure d'utiliser
le fichier `environment.yml` (sinon `Docker`
renverra une erreur)
    + Créer l'environnement vide `monenv` (présentant uniquement `Python` 3.10) avec
la commande  `conda` adéquate
    + Mettre à jour l'environnement en utilisant `environment.yml` avec `mamba`
- :six: Utiliser `ENV` pour ajouter l'environnement `monenv` au `PATH` et utiliser le _fix_ suivant:

```python
RUN echo "export PATH=$PATH" >> /home/coder/.bashrc  # Temporary fix while PATH gets overwritten by code-server
```

- :seven: Exposer sur le port `5000`
- :eight: En dernière étape, utiliser `CMD` pour reproduire le comportement de `python main.py`


{{% box status="hint" title="Hint: `mamba`" icon="fa fa-lightbulb" %}}
`mamba` est une alternative à `conda` pour installer des _packages_ dans un
environnement `Miniconda`/`Anaconda`. `mamba` n'est pas obligatoire, `conda`
peut suffire. Cependant, `mamba` est beaucoup plus rapide
que `conda` pour installer des packages à installer ; il s'agit donc
d'un utilitaire très pratique. 
{{% /box %}}

{{< panelset class="nommage" >}}

{{% panel name="Indications supplémentaires" %}}

Cliquer sur les onglets ci-dessus :point_up_2: pour bénéficier
d'indications supplémentaires, pour vous aider. Cependant, essayez
de ne pas les consulter immédiatement: n'hésitez pas à tâtonner. 


{{% /panel %}}

{{% panel name="Installation de Miniconda" %}}

```shell
# INSTALL MINICONDA -------------------------------
ARG CONDA_DIR=/home/coder/local/bin/conda
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
RUN bash Miniconda3-latest-Linux-x86_64.sh -b -p $CONDA_DIR
RUN rm -f Miniconda3-latest-Linux-x86_64.sh
```

{{% /panel %}}

{{% panel name="Installation de mamba" %}}

```shell
ENV PATH="/home/coder/local/bin/conda/bin:${PATH}"
RUN conda install mamba -n base -c conda-forge
```

{{% /panel %}}


{{% panel name="Création de l'environnement" %}}

```shell
COPY environment.yml .
RUN conda create -n monenv python=3.10
RUN mamba env update -n monenv -f environment.yml
```

{{% /panel %}}


{{< /panelset >}}

### Construire l'image

Maintenant, nous avons défini notre recette. Il nous reste à
faire notre plat et à le goûter

- Utiliser `docker build` pour créer une image avec le tag `my-python-app`
- Vérifier les images dont vous disposez. Vous devriez avoir un résultat
proche de celui-ci

<!---
docker build . -t my-python-app
docker images
---->

```shell
REPOSITORY      TAG       IMAGE ID       CREATED         SIZE
my-python-app   latest    c0dfa42d8520   6 minutes ago   2.23GB
ubuntu          20.04     825d55fb6340   6 days ago      72.8MB
```

### Tester l'image: découverte du cache

Il ne reste plus qu'à goûter la recette et voir si le plat est bon. 

Utiliser `docker run` avec l'option `it` pour pouvoir appeler l'image
depuis son tag

<!----
docker run -it my-python-app
---->

:warning: :bomb: :fire: 
`Docker` ne sait pas où trouver le fichier `main.py`. D'ailleurs,
il ne connait pas d'autres fichiers de notre application qui sont nécessaires
pour faire tourner le code: `config.yaml` et le dossier `src`

- Avant l'étape `EXPOSE` utiliser plusieurs `ADD` et/ou `COPY` pour que l'application
dispose de tous les éléments minimaux pour être en mesure de fonctionner

- Refaire tourner `docker run`
<!---
docker run -it my-python-app
--->

{{% box status="tip" title="Note" icon="fa fa-hint" %}}
Ici, le _cache_ permet d'économiser beaucoup de temps. Par besoin de 
refaire tourner toutes les étapes, `Docker` agit de manière intelligente
en faisant tourner uniquement les nouvelles étapes.
{{% /box %}}

### Corriger une faille de reproductibilité


Vous devriez rencontrer une erreur liée à la variable d'environnement
`AWS_ENDPOINT_URL`. C'est normal, elle est inconnue de cet environnement
minimaliste. D'ailleurs, `Docker` n'a aucune raison de connaître
votre espace de stockage sur le `S3` du `SSP-Cloud` si vous ne lui dites
pas. 
Donc cet environnement ne sait pas
comment accéder aux fichiers présents dans votre `minio`.

Vous allez régler ce problème avec les étapes suivantes, :


- :one: Naviguer dans l'[interface du SSP-Cloud](https://datalab.sspcloud.fr/mes-fichiers)
pour retrouver les liens d'accès direct de vos fichiers
- :two: Dans `VSCode`, les mettre dans `config.yaml` (faire de nouvelles clés)
- :three: Dans `VSCode`, modifier la fonction d'import pour s'adapter à ce changement.
- :four: Faire un `commit` et pusher les fichiers
- :five: Dans l'environnement bac à sable, faire un `pull` pour récupérer ces
modifications
- :six: Tester à nouveau le `build` (là encore le _cache_ est bien pratique !)

<!---
cf. 
https://github.com/linogaliana/ensae-reproductibilite-projet-1/commit/56946b4c5cb860d50b908d98a87fb549624314a6
----->


:tada: A ce stade, la matrice de confusion doit fonctionner. Vous avez créé
votre première application reproductible !

# Partie 3 : mise en production

Une image `Docker` est un livrable qui n'est pas forcément intéressant
pour tous les publics. Certains préféreront avoir un plat bien préparé
qu'une recette. Nous allons donc proposer d'aller plus loin en proposant
plusieurs types de livrables. Cela va nous amener à découvrir les outils
du CI/CD (_Continuous Integration / Continuous Delivery_)
qui sont au coeur de l'approche `DevOps`. Notre approche appliquée
au _machine learning_ va nous entraîner plutôt du côté du `MLOps` qui devient
une approche de plus en plus fréquente dans l'industrie de la 
_data science_.

Nous allons améliorer notre approche de trois manières:

- Automatisation de la création de l'image `Docker` et tests
automatisés de la qualité du code ;
- Production d'un site _web_ automatisé permettant de documenter et
valoriser le modèle de _Machine Learning_ ;
- Mise à disposition du modèle entraîné par le biais d'une API pour
ne pas le ré-entraîner à chaque fois et faciliter sa réutilisation ;

A chaque fois, nous allons d'abord tester en local notre travail puis
essayer d'automatiser cela avec les outils de `Github`.

On va ici utiliser l'intégration continue pour deux objectifs distincts:

- la mise à disposition de l'image `Docker` ;
- la mise en place de tests automatisés de la qualité du code
sur le modèle de notre `linter` précédent 

Nous allons utiliser `Github Actions` pour cela. 

## Etape préliminaire

Pour ne pas risquer de tout casser sur notre branche `master`, nous allons 
nous placer sur une branche nommée `dev`:

- si dans l'étape suivante vous appliquez la méthode la plus simple, vous
allez pouvoir la créer depuis l'interface de `Github` ;
- si vous utilisez l'autre méthode, vous allez devoir la créer en local (
via la commande `git checkout -b dev`)

## Etape 1: mise en place de tests automatisés

Avant d'essayer de mettre en oeuvre la création de notre image
`Docker` de manière automatisée, nous allons présenter la logique
de l'intégration continue en généralisant les évaluations de
qualité du code avec le `linter`

{{< panelset class="nommage" >}}
{{% panel name="Utilisation d'un _template_ `Github` :cat:" %}}

__Methode la plus simple: utilisation d'un _template_ Github__

Si vous cliquez sur l'onglet `Actions` de votre dépôt, `Github` vous propose des _workflows_ standardisés reliés à `Python`. Choisir l'option `Python Package using Anaconda`.

warning: Nous n'allons modifier que deux éléments de ce fichier.

:one: La dernière étape (`Test with pytest`) ne nous est pas nécessaire car nous n'avons pas de tests unitaires Nous allons donc remplacer celle-ci par l'utilisation de `pylint` pour avoir une note de qualité du package.

+ Utiliser `pylint` à cette étape pour noter les scripts ;
+ Vous pouvez fixer un score minimal à 5 (option `--fail-under=5`)

:two: Mettre entre guillements la version de `Python` pour que celle-ci soit reconnue.

:three: Enfin, finaliser la création de ce script:

- En cliquant sur le bouton `Start Commit`, choisir la méthode
`Create a new branch for this commit and start a pull request`
en nommant la branche `dev`
- Créer la `Pull Request` en lui donnant un nom signifiant

{{% /panel %}}

{{% panel name="Méthode manuelle" %}}

:warning: On est plutôt sur une méthode de galérien. Il vaut
mieux privilégier l'autre approche

On va éditer
depuis `VisualStudio` nos fichiers.

- Créer une branche `dev` en ligne de commande
- Créer un dossier `.github/workflows` via la ligne de commande ou l'explorateur de fichier 
<!---mkdir .github/workflows -p ---->
- Créer un fichier `.github/workflows/quality.yml`. 


Nous allons construire, par étape, une version simplifiée du `Dockerfile` présent
dans [ce post](https://medium.com/swlh/enhancing-code-quality-with-github-actions-67561c6f7063) et
dans [celui-ci](https://autobencoder.com/2020-08-24-conda-actions/)

:one: D'abord, définissons des paramètres pour indiquer à `Github`
quand faire tourner notre script:

- Commencez par nommer votre _workflow_ par exemple `Python Linting` avec la clé `name`
- Nous allons faire tourner ce _workflow_ dans la branche `master` et dans la branche actuelle (`dev`). Ici, nous laissons de côté les autres éléments (par exemple le fait de faire tourner à chaque _pull request_). La clé `on` est dédiée à cet usage

:two: Ensuite, défnissons le contexte d'exécution des tâches (`jobs`)
de notre script dans
les options de la partie `build`:

- Utilisons une machine `ubuntu-latest`. Nous verrons plus tard
comment améliorer cela. 
    
:three: Nous allons ensuite mélanger des étapes pré-définies (des actions du _marketplace_) et des instructions que nous faisons :

- Le _runner_ `Github` doit récupérer le contenu de notre dépôt, pour cela utiliser l'action `checkout`. Par rapport à l'exemple, il convient d'ajouter, pour le moment, un paramètre `ref` avec le nom de la branche (par exemple `dev`)
- ~~On installe ensuite `Python` avec l'action `setup-python`~~ Pas besoin d'installer `Python`, on va utiliser l'option `conda-incubator/setup-miniconda@v2`
- Pour installer `Python` et l'environnement `conda`, on va plutôt utiliser l'astuce de [ce blog](https://autobencoder.com/2020-08-24-conda-actions/) avec l'option `conda-incubator/setup-miniconda@v2`
- On utilise ensuite `flake8` et `pylint` (option `--fail-under=5`)
pour effectuer des diagnostics de qualité

Il ne reste plus qu'à faire un `commit` et espérer que cela fonctionne.
Cela devrait donner le fichier suivant : 


```yaml
name: Python Linting
on:
  push:
    branches: [master, dev]
jobs:
  build:
    runs-on: ubuntu-latest    
    steps:
      - uses: actions/checkout@v3
        with:
          ref: "dev"
      - uses: conda-incubator/setup-miniconda@v2
        with:
          activate-environment: monenv
          environment-file: environment.yml
          python-version: '3.10'
          auto-activate-base: false
      - shell: bash -l {0}
        run: |
          conda info
          conda list
      - name: Lint with flake8
        run: |
          pip install flake8
          flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src --count --max-complexity=10 --max-line-length=79 --statistics
      - name: Lint with Pylint
        run: |
          pip install pylint
          pylint src
``` 


{{% /panel %}}


{{< /panelset >}}

 
Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entraîner une action pour
tester nos scripts.

Si la note est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi détecter,
en développant son projet, les moments où on dégrade la qualité du script 
afin de la rétablir immédiatemment. 


{{% box status="hint" title="Un `linter` sous forme de _hook_ pre-commit" icon="fa fa-lightbulb" %}}

`Git` offre une fonctionalité intéressante lorsqu'on est puriste: les 
_hooks_. Il s'agit de règles qui doivent être satisfaites pour que le 
fichier puisse être committé. Cela assurera que chaque `commit` remplisse
des critères de qualité afin d'éviter le problème de la procrastination.

La [documentation de pylint](https://pylint.pycqa.org/en/latest/user_guide/pre-commit-integration.html)
offre des explications supplémentaires. 

{{% /box %}}


## Etape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise à disposition de notre image
sur `DockerHub`. Cela facilitera sa réutilisation mais aussi des
valorisations ultérieures.

Là encore, nous allons utiliser une série d'actions pré-configurées.

:one: Pour que `Github` puisse s'authentifier auprès de `DockerHub`, il va 
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
sécurisé associé à votre dépôt `Github`. Cette démarche sera là même
ultérieurement lorsque nous connecterons notre dépôt à un autre
service tiers, à savoir `Netlify`:

- Se rendre sur
https://hub.docker.com/ et créer un compte.
- Aller dans les paramètres (https://hub.docker.com/settings/general)
et cliquer, à gauche, sur `Security`
- Créer un jeton personnel d'accès, ne fermez pas l'onglet en question,
vous ne pouvez voir sa valeur qu'une fois. 
- Dans votre dépôt `Github`, cliquer sur l'onglet `Settings` et cliquer,
à gauche, sur `Actions`. Sur la page qui s'affiche, cliquer sur `New repository secret`
- Donner le nom `DOCKERHUB_TOKEN` à ce jeton et copier la valeur. Valider
- Créer un deuxième secret nommé `DOCKERHUB_USERNAME` ayant comme valeur le nom d'utilisateur
que vous avez créé sur `Dockerhub`

:two: A ce stade, nous avons donné les moyens à `Github` de s'authentifier avec
notre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action
en s'inspirant de https://github.com/docker/build-push-action/#usage.
On ne va modifier que trois éléments dans ce fichier. Effectuer les 
actions suivantes:

- Créer depuis `VSCode` un fichier
`.github/workflows/docker.yml` et coller le
contenu du _template_ dedans ; 
- Changer le nom en un titre plus signifiant (par exemple _"Production de l'image Docker"_)
- Ajouter `master` et `dev` à la liste des branches sur lesquelles tourne
le pipeline ;
- Changer le tag à la fin pour mettre `<username>/ensae-repro-docker:latest`
où `username` est le nom d'utilisateur sur `DockerHub`;
- Faire un `commit` et un `push` de ces fichiers

:four: Comme on est fier de notre travail, on va afficher ça avec un badge sur le 
`README`. Pour cela, on se rend dans l'onglet `Actions` et on clique sur
un des scripts en train de tourner. 

- En haut à droite, on clique sur `...`
- Sélectionner `Create status badge`
- Récupérer le code `Markdown` proposé
- Copier dans le `README` depuis `VSCode`
- Faire de même pour l'autre _workflow_

:five: Maintenant, il nous reste à tester notre application dans l'espace bac à sable:

- Se rendre sur l'environnement bac à sable
- Créer un fichier `Dockerfile` ne contenant que l'import et le déploiement
de l'appli:

```yaml
FROM <username>/ensae-repro-docker:latest

EXPOSE 5000
CMD ["python", "main.py"]
```

- Comme précédemment, faire un _build_
- Tester l'image avec `run`

:tada: La matrice de confusion doit s'afficher ! Vous avez grandement
facilité la réutilisation de votre image. 

## Etape 3: création d'un rapport automatique

Maintenant, nous allons créer et déployer un site web pour valoriser notre
travail. Cela va impliquer trois étapes:

- Tester en local le logiciel `quarto` et créer un rapport minimal qui sera compilé par `quarto` ;
- Enrichir l'image docker avec le logiciel `quarto` ;
- Compiler le document en utilisant cette image sur les serveurs de `Github` ;
- Déployer ce rapport minimal pour le rendre disponible à tous sur le _web_.

Le but est de proposer un rapport minimal qui illustre la performance
du modèle est la _feature importance_. Pour ce dernier élément, le
rapport qui sera proposé utilise `shap` qui est une librairie dédiée
à l'interprétabilité des modèles de _machine learning_

### 1. Rapport minimal en local

:one: La première étape consiste à installer
`quarto` sur notre machine `Linux` sur laquelle
tourne `VSCode`:

- Dans un terminal, installer `quarto` avec les commandes suivantes:

```shell
QUARTO_VERSION="0.9.287"
wget "https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb"
sudo apt install "./quarto-${QUARTO_VERSION}-linux-amd64.deb"
```

- S'assurer qu'on travaille bien depuis l'environnement `conda` `monenv`. Sinon
l'activer

:two: Il va être nécessaire d'enrichir l'environnement `conda`.
Certaines dépendances sont nécessaires pour que `quarto` fonctionne bien avec
`Python` (`jupyter`, `nbclient`...)
alors que d'autres ne sont nécessaires que parce qu'ils sont utilisés dans
le document (`seaborn`, `shap`...). Changer la section `dependencies` avec
la liste suivante:

```yaml
dependencies:
  - python=3.10.0
  - ipykernel==6.13.0
  - jupyter==1.0.0
  - matplotlib==3.5.1
  - nbconvert==6.5.0
  - nbclient==0.6.0
  - nbformat==5.3.0
  - pandas==1.4.1
  - PyYAML==6.0
  - s3fs==2022.2.0
  - scikit-learn==1.0.2
  - seaborn==0.11.2
  - shap==0.40.0
```


:three: Créer un fichier nommé `report.qmd`

~~~markdown
  ---
  title: "Comprendre les facteurs de survie sur le Titanic"
  subtitle: "Un rapport innovant"
  format:
    html:
      self-contained: true
    ipynb: default
  jupyter: python3
  ---

Voici un rapport présentant quelques intuitions issues d'un modèle 
_random forest_ sur le jeu de données `Titanic` entraîné et 
déployé de manière automatique. 

Il est possible de télécharger cette page sous format `Jupyter Notebook` <a href="report.ipynb" download>ici</a>


```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
import main
X_train = main.X_train
y_train = main.y_train
training_data = main.training_data
rdmf = RandomForestClassifier(n_estimators=20)
rdmf.fit(X_train, y_train)
```

# Feature importance

La @fig-feature-importance représente l'importance des variables :

```python
feature_imp = pd.Series(rdmf.feature_importances_, index=training_data.iloc[:,1:].columns).sort_values(ascending=False)
```

```python
#| label: fig-feature-importance
#| fig-cap: "Feature importance"
plt.figure(figsize=(10,6))
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.tight_layout()
plt.show()
```

Celle-ci peut également être obtenue grâce à la librairie
`shap`:

```python
#| echo : true
import shap
shap_values = shap.TreeExplainer(rdmf).shap_values(X_train)
shap.summary_plot(shap_values, X_train, plot_type="bar", feature_names = training_data.iloc[:,1:].columns)
```

On peut également utiliser cette librairie pour
interpréter la prédiction de notre modèle:


```python
# explain all the predictions in the test set
explainer = shap.TreeExplainer(rdmf)
# Calculate Shap values
choosen_instance = main.X_test[15]
shap_values = explainer.shap_values(choosen_instance)
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, feature_names = training_data.iloc[:,1:].columns)
```

# Qualité prédictive du modèle

La matrice de confusion est présentée sur la
@fig-confusion

```python
#| label: fig-confusion
#| fig-cap: "Matrice de confusion"
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(main.y_test, rdmf.predict(main.X_test))
plt.figure(figsize=(8,5))
sns.heatmap(conf_matrix, annot=True)
plt.title('Confusion Matrix')
plt.tight_layout()
```

Ou, sous forme de tableau:


```python
pd.DataFrame(conf_matrix, columns=['Predicted','Observed'], index = ['Predicted','Observed']).to_html()
```
~~~

:four: On va tenter de compiler ce document

- Le compiler en local avec la commande `quarto render report.qmd`

- Vous devriez rencontrer l'erreur suivante:

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Input In [1], in <cell line: 6>()
      4 from sklearn.ensemble import RandomForestClassifier
      5 import main
----> 6 X_train = main.X_train
      7 y_train = main.y_train
      8 training_data = main.training_data

AttributeError: module 'main' has no attribute 'X_train'
AttributeError: module 'main' has no attribute 'X_train'
```

- Refactoriser `main.py` pour que toutes les opérations, à l'exception
du print de la matrice de confusion ne soient plus dans la section `__main__`
afin qu'ils soient systématiquement exécutés. 

- Tenter à nouveau `quarto render report.qmd`

- Deux fichiers ont été générés: 
    + un `Notebook` que vous pouvez ouvrir et dont vous pouvez exécuter
des cellules
    + un fichier `HTML` que vous pouvez télécharger et ouvrir

:five: On a déjà un résultat assez esthétique en ce qui concerne la page `HTML`.
Cependant, on peut se dire que certains paramètres par défaut, comme l'affichage
des blocs de code, ne conviennent pas au public ciblé. De même, certains
paramètres de style, comme l'affichage des tableaux peuvent ne pas convenir
à notre charte graphique. On va remédier à cela en deux étapes:

- enrichir le _header_ d'options globales contrôlant le comportement de `quarto`
- créer un fichier `CSS` pour avoir de beaux tableaux

:six: Changer la section `format` du _header_ avec les options suivantes:

```yaml
format:
  html:
    echo: false
    code-fold: true
    self-contained: true
    code-summary: "Show the code"
    warning: false
    message: false
    theme:
      - cosmo
      - css/custom.scss
  ipynb: default
```

:seven: Créer le fichier `css/custom.scss` avec le contenu suivant:

```css
/*-- scss:rules --*/

table {
    border-collapse: collapse;
    margin: 25px 0;
    font-size: 0.9em;
    font-family: sans-serif;
    min-width: 400px;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);  
}

thead tr {
    background-color: #516db0;
    color: #ffffff;
    text-align: center;
}

th, td {
    padding: 12px 15px;
}

tbody tr {
    border-bottom: 1px solid #dddddd;
}

tbody tr:nth-of-type(even) {
    background-color: #f3f3f3;
}

tbody tr:last-of-type {
    border-bottom: 2px solid #516db0;
}

tbody tr.active-row {
    font-weight: bold;
    color: #009879;
}
```

:eight: Compiler à nouveau et observer le changement d'esthétique du `HTML`

:nine: Commit des nouveaux fichier `report.qmd`, `custom.scss` et des fichiers
déjà existants.

{{% box status="hint" title="Un `linter` sous forme de _hook_ pre-commit" icon="fa fa-lightbulb" %}}

On ne `commit` pas les _output_, ici le notebook et le fichier html.
Les mettre sur le dépôt `Github` n'est pas la bonne manière de les mettre
à disposition. On va le voir, on va utiliser l'approche CI/CD pour cela.

Idéalement, on ajoute au `.gitignore` les fichiers concernés, ici `report.ipynb`
et `report.html`

{{% /box %}}

### 3. Enrichir l'image `Docker`

On va vouloir mettre à jour notre image pour automatiser, à terme, la production
de nos livrables (le notebook et la page web). 

Pour cela, il est nécessaire que notre image intègre le logiciel `quarto`.

:one: A partir du script précédent d'installation de `quarto`, enrichir l'image
`Docker`[^1]

<!----
ENV QUARTO_VERSION="0.9.287"
RUN wget "https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb"
RUN apt install "./quarto-${QUARTO_VERSION}-linux-amd64.deb"
----->

[^1]: Le `sudo` n'est pas nécessaire puisque vous êtes déjà en `root`


### 4. Automatisation avec `Github Actions`

:one: Créer un nouveau fichier `.github/workflows.report.yml`


Si les dépendances et l'image ont bien été enrichis, cette étape est quasi directe
avec 

{{< panelset class="simplification" >}}

{{% panel name="Version autonome :car: " %}}

- Donner comme nom `Deploy as website`
- Effectuer cette action à chaque `push` sur les branches `main`, `master` et `dev`
- Le job doit tourner sur une machine `ubuntu`
- Cependant, il convient d'utiliser comme `container` votre image Docker 
- Les `steps`:
    + Récupérer le contenu du dossier avec `checkout`
    + Faire un `quarto render`
    + Récupérer le notebook sous forme d'artefact

{{% /panel %}}

{{% panel name="Version guidée :map: " %}}

```yaml
name: Deploy as website

on:
  push:
    branches:
      - main
      - master
      - dev

jobs:
  build:
    runs-on: ubuntu-latest
    container: linogaliana/ensae-repro-docker:latest
    steps:
      - uses: actions/checkout@v3
      - name: Render site
        run: quarto render report.qmd
      - uses: actions/upload-artifact@v1
        with:
          name: Report
          path: report.ipynb
```

{{% /panel %}}

{{< /panelset >}}

Si vous êtes fier de vous, vous pouvez ajouter le badge de ce workflow
sur le `README` :sunglasses:

Cette étape nous a permis d'automatiser la construction de nos livrables.
Mais la mise à disposition de ce livrable est encore assez manuelle: il 
faut aller chercher à la main la dernière version du notebook pour
la partager. 

On va améliorer cela en déployant automatiquement un site _web_ présentant
en page d'accueil notre rapport et permettant le téléchargement du notebook. 


## Etape 4: Déploiement de ce rapport automatique sur le web

:one: Dans un premier temps, nous allons connecter notre dépôt `Github` au
service tiers `Netlify`

- Aller sur https://www.netlify.com/ et faire `Sign up` (utiliser son compte `Github`)
- Dans la page d'accueil de votre profil, vous pouvez cliquer sur `Add new site > Import an existing project`
- Cliquer sur `Github`. S'il y a des autorisations à donner, les accorder. Rechercher votre projet dans la liste de vos projets `Github`
- Cliquer sur le nom du projet et laisser les paramètres par défaut (nous allons modifier par la suite)
- Cliquer sur `Deploy site`

:two: A ce stade, votre déploiement devrait échouer.
C'est normal, vous essayez de déployer depuis `master` qui ne comporte pas de html.
Mais le rapport n'est pas non plus présent dans la branche `dev`.
En fait, aucune branche ne comporte le rapport:
celui-ci est généré dans votre _pipeline_ mais n'est jamais présent dans le
dépôt car il s'agit d'un _output_. On va désactiver le déploiement automatique 
pour privilégier un déploiement depuis `Github Actions`:

- Aller dans `Site Settings` puis, à gauche, cliquer sur `Build and Deploy`
- Dans la section `Build settings`, cliquer sur `Stop builds` et valider

On vient de désactiver le déploiement automatique par défaut. On va faire
communiquer notre dépôt `Github` et `Netlify` par le biais de l'intégration
continue.

:three: Pour cela, il faut créer un jeton `Netlify` pour que les serveurs
de `Github`, lorsqu'ils disposent d'un rapport, puissent l'envoyer à `Netlify`
pour la mise sur le _web_. Il va être nécessaire de créer deux variables
d'environnement pour connecter `Github` et `Netlify`: l'identifiant du site
et le _token_

- Pour le token : 
    + Créer un jeton en cliquant, en haut à droite, sur l'icone de votre profil. Aller
dans `User settings`. A gauche, cliquer sur `Applications` et créer un jeton personnel d'accès
avec un nom signifiant (par exemple `PAT_ENSAE_reproductibilite`)
    + Mettre de côté (conseil : garder l'onglet ouvert)
- Pour l'identifiant du site:
    + cliquer sur `Site Settings` dans les onglets en haut
    + Garder l'onglet ouvert pour copier la valeur quand nécessaire
    

- Il est maintenant nécessaire d'aller dans le dépôt `Github` et de créer 
les secrets (`Settings > Secrets > Actions`):
    + Créer le secret `NETLIFY_AUTH_TOKEN` en collant la valeur du jeton d'authentification `Netlify`
    + Créer le secret `NETLIFY_SITE_ID` en collant l'identifiant du site


:four: Nous avons effectué toutes les configurations nécessaires. On va
maintenant mettre à jour l'intégration continue afin de mettre à disposition
sur le _web_ notre rapport. On va utiliser l'interface en ligne de commande
(CLI) de `Netlify`. Celle-ci attend que le site _web_ se trouve dans un
dossier `public` et que la page d'accueil soit nommée `index.html`:

{{< panelset class="simplification" >}}

{{% panel name="Vision d'ensemble " %}}

- une installation de `npm`
- une étape de déploiement via la CLI de netlify

```yaml
- name: Install npm
  uses: actions/setup-node@v2
  with:
    node-version: '14'
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
```

{{% /panel %}}

{{% panel name="Détails npm " %}}

{{< highlight yaml "hl_lines=1-4" >}}
- name: Install npm
  uses: actions/setup-node@v2
  with:
    node-version: '14'
{{< / highlight >}}

`npm` est le gestionnaire de paquet de JS. Il est nécessaire de le configurer,
ce qui est fait automatiquement grâce à l'action `actions/setup-node@v2`

{{% /panel %}}

{{% panel name="Détails `Netlify CLI`" %}}

- On rappelle à `Github Actions` nos paramètres d'authentification
sous forme de variables d'environnement. Cela permet de les garder
secrètes

{{< highlight yaml "hl_lines=3-5" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}

- On déplace les rapports de la racine vers le dossier `public`

{{< highlight yaml "hl_lines=7-9" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}

- On installe et initialise `Netlify`

{{< highlight yaml "hl_lines=10-11" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}

- On déploie sur l'url par défaut (`-- prod`) depuis le dossier `public`

{{< highlight yaml "hl_lines=10-12" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}
{{% /panel %}}

{{< /panelset >}}


Au bout de quelques minutes, le rapport est disponible en ligne sur
l'URL `Netlify` (par exemple https://spiffy-florentine-c913b9.netlify.app)


