---
title: "Application"
subtitle: "Appliquer pas à pas les concepts étudiés à un projet de data science"
author: "Romain Avouac et Lino Galiana"
image: images/rocket.png
description: |
  Une application fil rouge pour illustrer l'intérêt d'appliquer graduellement les bonnes pratiques dans une optique de mise en production d'une application de data science.
order: 8
href: chapters/application.html
---

<details>
<summary>
Dérouler les _slides_ ci-dessous ou [cliquer ici](https://ensae-reproductibilite.github.io/slides/#/title-slide)
pour afficher les slides en plein écran.
</summary>


<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://ensae-reproductibilite.github.io/slides/#/title-slide"></iframe></div>

</details>

L'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.

Celle-ci est un tutoriel pas à pas pour avoir un projet reproductible et disponible sous plusieurs livrables. 
Toutes les étapes ne sont pas indispensables à tous les projets de _data science_. 

Nous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science.
On a un _notebook_ un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :

- Import de données ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entraînement d'un modèle ;
- Evaluation du modèle.

**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée.** 


<details>
<summary>
Illustration de notre point de départ
</summary>
![](/workflow1.png)
</details>

<details>
<summary>
Illustration de l'horizon vers lequel on se dirige
</summary>
![](/workflow2.png)
</details>

::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines étapes peuvent être rapides, d'autres plus fastidieuses ;
certaines être assez guidées, d'autres vous laisser plus de liberté.
Si vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à
l'étape suivante qui en dépend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous 
recommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les 
outils nécessaires sont pré-installés et pré-configurés. Le service `VSCode`
ne sera en effet que le point d'entrée pour l'utilisation d'outils plus exigeants
sur le plan de l'infrastructure: _Argo_, _MLFLow_, etc.
:::


# Partie 0 : initialisation du projet

::: {.callout-tip}
## Application préliminaire: forker le dépôt d'exemple

Les premières étapes consistent à mettre en place son environnement de travail sur `Github`:

- Générer un jeton d'accès (*token*) sur `GitHub` afin de permettre l'authentification en ligne de commande à votre compte.
La procédure est décrite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). 
__Vous ne voyez ce jeton qu'une fois, ne fermez pas la page de suite__. 

- Mettez de côté ce jeton en l'enregistrant dans un gestionnaire de mot de passe ou dans 
l'espace _["Mon compte"](https://datalab.sspcloud.fr/account/third-party-integration)_
du `SSP Cloud`. 

- Forker le dépôt `Github` : [https://github.com/ensae-reproductibilite/application-correction](https://github.com/ensae-reproductibilite/application-correction) en faisant attention à deux choses:
    + Renommer le dépôt en `ensae-reproductibilite-application-correction.git` ;
    + Décocher la case _"Copy the `main` branch only"_ afin de copier également les _tags_ `Git` qui nous permettront de faire les _checkpoint_


<details>

<summary>
Ce que vous devriez voir sur la page de création du _fork_
</summary>

![](/fork-example.png)

</details>

Il est maintenant possible de ce lancer dans la création de l'environnement de travail:

- Ouvrir un service `VSCode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller
dans la page `My Services` et cliquer sur `New service`. Sinon, vous
pouvez initialiser la création du service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=false). __Modifier les options suivantes__:
    + Dans l'onglet `Kubernetes`, sélectionner le rôle `Admin` ;
    + Dans l'onglet `Networking`, cliquer sur "Enable a custom service port" et laisser la valeur par défaut 5000 pour le numéro du port

- Clôner __votre__ dépôt `Github` en utilisant le
terminal depuis `Visual Studio` (`Terminal > New Terminal`) et
en passant directement le token dans l'URL selon cette structure:

```shell
$ git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application-correction.git
```

où `<TOKEN>` et `<USERNAME>` sont à remplacer, respectivement, 
par le jeton que vous avez généré précédemment et votre nom d'utilisateur.

- Se placer avec le terminal dans le dossier en question : 

```shell
$ cd ensae-reproductibilite-application-correction
```

- Se placer sur une branche de travail en faisant:

```shell
$ git checkout -b dev
```

:::


# Partie 1 : qualité du script

Cette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours.

Elle fait intervenir les notions suivantes : 

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.html)) ;
- **Qualité du code** (voir [Qualité du code](/chapters/code-quality.html)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contrôle de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).

Nous allons partir de ce _Notebook_ `Jupyter`,
que vous pouvez prévisualiser voire tester
en cliquant sur l'un des liens suivants:

_to do bouton onyxia_
<a href="https://github.com/ensae-reproductibilite/application-correction/blob/main/titanic.ipynb" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>

Le plan de la partie est le suivant :

1. S'assurer que le script fonctionne ;
2. Nettoyer le code des scories formelles avec un _linter_ et un _formatter_ ;
3. Paramétrisation du script ;
4. Utilisation de fonctions.


## Étape 1 : s'assurer que le script s'exécute correctement

On va partir du fichier `notebook.py` qui reprend le contenu 
du _notebook_[^jupytext] mais dans un script classique.
Le travail de nettoyage en sera facilité. 

[^jupytext]: L'export dans un script `.py` a été fait
        directement depuis `VSCode`. Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette étape et fournissons
        directement le script expurgé du texte intermédiaire. Mais n'oubliez
        pas que cette démarche, fréquente quand on a démarré sur un _notebook_ et
        qu'on désire consolider en faisant la transition vers des 
        scripts, nécessite d'être attentif pour ne pas risquer de faire une erreur. 

La première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**. 


{{< include "./applications/_appli1.qmd" >}}


## Étape 2: utiliser un _linter_ puis un _formatter_

On va maintenant améliorer la qualité de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et le _formatter_ [`Black`](https://github.com/psf/black).

::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)
sont des _packages_ `Python` qui 
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui suggère
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)
ou [`Black`](https://black.readthedocs.io/en/stable/),
n'oubliez pas d'exécuter la commande `pip install pylint` ou `pip install black`.
:::


Le _linter_ renvoie alors une série d'irrégularités,
en précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualité du code à l'aune des standards communautaires évoqués
dans la partie [Qualité du code](/chapters/code-quality.html).

{{< include "./applications/_appli2.qmd" >}}

Le code est maintenant lisible, il obtient à ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment 
beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. 
Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script: 
jetons d'API et chemin des fichiers.


## Étape 3: gestion des paramètres

L'exécution du code et les résultats obtenus
dépendent de certains paramètres définis dans le code. L'étude de résultats
alternatifs, en jouant sur 
des variantes des (hyper)paramètres, est à ce stade compliquée
car il est nécessaire de parcourir le code pour trouver
ces paramètres. De plus, certains paramètres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation à 
être présents dans le code. 

Il est plus judicieux de considérer ces paramètres comme des
variables d'entrée du script. Cela peut être fait de deux
manières:

1. Avec des __arguments optionnels__ appelés depuis la ligne de commande _(Application 3a)_.
Cela peut être pratique pour mettre en oeuvre des tests automatisés[^noteCI] mais
n'est pas forcément pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un __fichier de configuration__ dont les valeurs sont importées dans
le script principal _(Application 3b)_. 


<details>
<summary>
Un exemple de définition d'un argument pour l'utilisation en ligne de commande
</summary>

```{python}
#| eval: false
#| file: prenom.py
#| filename: prenom.py
#| code-summary: prenom.py
import argparse
parser = argparse.ArgumentParser(description="Qui êtes-vous?")
parser.add_argument(
    "--prenom", type=str, default="Toto", help="Un prénom à afficher"
)
args = parser.parse_args()
print(args.prenom)
```

Exemples d'utilisations en ligne de commande

```bash
python prenom.py
python prenom.py --prenom "Zinedine"
```

</details>

{{< include "./applications/_appli3.qmd" >}}


## Étape 4 : Adopter la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook**.

Cet exercice étant chronophage, il n'est __pas obligatoire de le réaliser en entier__. L'important est de
comprendre la démarche et d'adopter fréquemment une approche fonctionnelle[^POO]. Pour obtenir 
une chaine entièrement fonctionnalisée, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orientée objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au développeur. L'arbitrage coût-avantage est négatif pour notre
exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d'un modèle,
il est recommandé de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/). 

::: {.callout-tip}
## Application 4: adoption des standards de programmation fonctionnelle 

- Créer une fonction qui importe les données d'entraînement (`train.csv`) et de test (`test.csv`) et renvoie des `DataFrames` `Pandas` ;
- En fonction du temps disponible, créer plusieurs fonctions pour réaliser les étapes de *feature engineering*:
    + La création de la variable _"Title"_ peut être automatisée en vertu du principe _"do not repeat yourself"_[^notepandas].
    + Regrouper ensemble les `fillna` et essayer de créer une fonction généralisant l'opération. 
    + Les _label encoders_ peuvent être transformés en deux fonctions: une première pour encoder une colonne puis une seconde qui utilise
    la première de manière répétée pour encoder plusieurs colonnes. _Remarquez les erreurs de copier-coller que cela corrige_
    + Finaliser les dernières transformations avec des fonctions
- Créer une fonction qui réalise le *split train/test* de validation en fonction d'un paramètre représentant la proportion de l'échantillon de test.
- Créer une fonction qui entraîne et évalue un classifieur `RandomForest`, et qui prend en paramètre le nombre d'arbres (`n_estimators`). La fonction doit imprimer à la fin la performance obtenue et la matrice de confusion.
- Déplacer toutes les fonctions ensemble, en début de script.
:::

[^notepandas]: Au passage vous pouvez noter que mauvaises pratiques discutables,
    peuvent
    être corrigées, notamment l'utilisation excessive de `apply` là où
    il serait possible d'utiliser des méthodes embarquées par `Pandas`.
    Cela est plutôt de l'ordre du bon style de programmation que de la
    qualité formelle du script. Ce n'est donc pas obligatoire mais c'est mieux. 


::: {.callout-important}
Le fait d'appliquer des fonctions a déjà amélioré la fiabilité du processus
en réduisant le nombre d'erreurs de copier-coller. Néanmoins, pour vraiment
fiabiliser le processus, il faudrait utiliser un _pipeline_ de transformations
de données. 

Ceci n'est pas encore au programme du cours mais le sera dans une prochaine 
version. 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli4
```

ou

- [`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application4/titanic.py)

Les autres fichiers inchangés:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::



# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est déjà considérablement rapprochés d'un
possible partage du code : celui-ci est lisible et intelligible. 
Le code est proprement versionné sur un
dépôt `GitHub`.

<details>
<summary>
Illustration de l'état actuel du projet 
</summary>
![](/schema_post_appli4.png)
</details>


Néanmoins,
la structure du projet n'est pas encore normalisée. 
De plus, 
l'adoption d'une structure plus modulaire facilitera
la compréhension de la chaine de traitement.


## Étape 1 : modularisation

Fini le temps de l'expérimentation : on va maintenant essayer de se passer complètement du _notebook_.
Pour cela, on va utiliser un `main` script, c'est à dire un script qui reproduit l'analyse en important et en exécutant les différentes fonctions dans l'ordre attendu.


::: {.callout-tip}
## Application 5: modularisation

- Déplacer les fonctions dans une série de fichiers dédiés:
    +  `import_data.py`: fonctions d'import de données 
    +  `build_features.py`: fonctions regroupant les étapes de _feature engineering_ 
    +  `train_evaluate.py`: fonctions d'entrainement et d'évaluation du modèle
- Spécifier les dépendances (i.e. les packages à importer)
dans les modules pour que ceux-ci puissent s'exécuter indépendamment ;
- Renommer `titanic.py` en `main.py` pour suivre la convention de nommage des projets `Python` ;
- Importer les fonctions nécessaires à partir des modules. ⚠️ Ne pas utiliser `from XXX import *`, ce n'est pas une bonne pratique ! 
- Vérifier que tout fonctionne bien en exécutant le _script_ `main` à partir de la ligne de commande :

```shell
$ python main.py
```
:::

On dispose maintenant d'une application `Python` fonctionnelle. 
Néanmoins, le projet est certes plus fiable mais sa structuration
laisse à désirer et il serait difficile de rentrer à nouveau
dans le projet dans quelques temps. 

<details>
<summary>Etat actuel du projet 🙈</summary>

```shell
├── README.md
├── train.csv
├── test.csv
├── .gitignore
├── config.yaml
├── secrets.yaml
├── import_data.py
├── build_features.py
├── train_evaluate.py
└──main.py
```

</details>

Comme cela est expliqué dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va 
faciliter l'autodocumentation de notre projet.  

De plus, une telle structure va faciliter des évolutions optionnelles
comme la packagisation du projet. Passer d'une structure modulaire
bien faite à un _package_ est quasi-immédiat en `Python`. 

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli5
```

ou

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers inchangés:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::

## Étape 2 : adopter une architecture standardisée de projet

On va maintenant modifier l'architecture de notre projet pour la rendre plus standardisée.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui génèrent des _templates_ de projet.

On va s'inspirer de la structure du [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
développé par la communauté.

::: {.callout-note}
L'idée de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante : 

```shell
$ pip install cookiecutter
$ cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

```shell
ensae-reproductibilite-application
├── main.py
├── README.md
├── data
│   └── raw
│       ├── test.csv
│       └── train.csv
├── configuration
│   ├── secrets.yaml
│   └── config.yaml
├── notebooks
│   └── titanic.ipynb
└── src
    ├── data
    │   └── import_data.py
    ├── features
    │   └── build_features.py
    └── models
        └── train_evaluate.py
```

::: {.callout-tip}

## Application 6: adopter une structure lisible

- _(optionnel)_ Analyser et comprendre la [structure de projet](https://drivendata.github.io/cookiecutter-data-science/#directory-structure) proposée par le template
- Modifier l'arborescence du projet selon le modèle
- Adapter les scripts et les fichiers de configuration à la nouvelle arborescence
- Ajouter le dossier __pycache__ au `.gitignore`[^pycache] et le dossier `data`
:::

[^pycache]: Il est normal d'avoir des dossiers `__pycache__` qui traînent : ils se créent automatiquement à l'exécution d'un script en `Python`.

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli6
```

ou

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers sont inchangés, à l'exception de leur emplacement.

:::

### Étape 3: indiquer l'environnement minimal de reproductibilité

Le script `main.py` nécessite un certain nombre de packages pour
être fonctionnel. Chez vous les packages nécessaires sont
bien sûr installés mais êtes-vous assuré que c'est le cas 
chez la personne qui testera votre code ? 

Afin de favoriser la portabilité du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-à-dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version.
Nous proposons de créer un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacrée aux environnements reproductibles. 

Le fichier `requirements.txt` est conventionnellement localisé à la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier plus tard.

::: {.callout-tip}

## Application 7: création du `requirements.txt`

- Créer un fichier `requirements.txt` avec la liste des packages nécessaires
- Ajouter une indication dans `README.md` sur l'installation des _packages_ grâce au fichier `requirements.txt` 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli7
```

ou

- [`requirements.txt`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/requirements.txt)
- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/README.md)

:::

## Étape 3 : stocker les données de manière externe {#stockageS3}


::: {.callout-warning}
Pour mettre en oeuvre cette étape, il peut être utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://docs.sspcloud.fr/onyxia-guide/stockage-de-donnees) pour la réaliser. Une aide-mémoire est également disponible dans le cours
de 2e année de l'ENSAE [Python pour la data science](https://linogaliana-teaching.netlify.app/reads3/#)
:::

Comme on l'a vu dans le cours ([partie structure des projets](/chapters/project-structure.html)),
les données ne sont pas censées être versionnées sur un projet `Git`.

L'idéal pour éviter cela tout en maintenant la reproductibilité est d'utiliser une solution de stockage externe.
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. 

::: {.callout-tip}

## Application 8: utilisation d'un système de stockage distant

A partir de la ligne de commande,
utiliser l'utilitaire [MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html)
pour copier les données `data/raw/train.csv` et `data/raw/test.csv` vers votre
bucket personnel, respectivement dans les dossiers `ensae-reproductibilite/data/raw/train.csv`
et `ensae-reproductibilite/data/raw/test.csv`. 

<details>
<summary>Indice</summary>

Structure à adopter:

```shell
$ mc cp data/raw/train.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv
$ mc cp data/raw/test.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv
```

en modifiant l'emplacement de votre bucket personnel
</details>

- Pour se simplifier la vie, on va utiliser des URL de téléchargement des fichiers
(comme si ceux-ci étaient sur n'importe quel espace de stockage) plutôt que d'utiliser
une librairie `S3` compatible comme `boto3` ou `s3fs`. Pour cela, en ligne de
commande, faire:

```shell
mc anonymous set download s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/
```

en modifiant `<BUCKET_PERSONNEL>`. Les URL de téléchargement seront de la forme 
`https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv`
et `https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv`

- Modifier `configuration.yaml` pour utiliser directement les URL dans l'import 
- Supprimer les fichiers `.csv` du dossier `data` de votre projet, on n'en a plus besoin vu qu'on les importe de l'extérieur
- Vérifier le bon fonctionnement de votre application
:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli8
```

ou

- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application8/config.yaml)

:::

# Partie 2bis: packagisation de son projet (optionnel)

Cette série d'actions n'est pas forcément pertinente pour tous
les projets. Elle fait un peu la transition entre la modularité
et la portabilité. 

## Étape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions génériques.
On peut vouloir tester leur usage sur des données standardisées,
différentes de celles du Titanic.

Même si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut être pédagogique. 

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
pour effectuer des tests unitaires. Cette approche nécessite une maîtrise 
de la programmation orientée objet.

::: {.callout-tip}

## Application 9: test unitaire _(optionnel)_

Dans le dossier `src/data/`, créer un fichier `test_create_variable_title.py`[^emplacement].

En s'inspirant de l'[exemple de base](https://docs.python.org/3/library/unittest.html#basic-example),
créer une classe `TestCreateVariableTitle` qui effectue les opérations suivantes:

- Création d'une fonction `test_create_variable_title_default_variable_name` qui permet 
de comparer les objets suivants:

    + Création d'un `DataFrame` de test :  

    ```python
    df = pd.DataFrame({
                'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',
                        'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath (Lily May Peel)',
                        'Allen, Mr. William Henry', 'Moran, Mr. James',
                        'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',
                        'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',
                        'Nasser, Mrs. Nicholas (Adele Achem)'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

    + Utilisation de la fonction `create_variable_title` sur ce `DataFrame`
    + Comparaison au `DataFrame` attendu:

    ```python
    expected_result = pd.DataFrame({
                'Title': ['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.', 'Mr.', 'Mr.', 'Master.', 'Mrs.', 'Mrs.'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

- Effectuer le test unitaire en ligne de commande avec `unittest`. Corriger le test unitaire en cas d'erreur. 
- Si le temps le permet, proposer des variantes pour tenir compte de paramètres (comme la variable `variable_name`)
ou d'exceptions (comme la gestion du cas _"Dona"_)
:::

::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche généralement
à tester le plus de lignes possibles de son code. On parle de
taux de couverture (_coverage rate_) pour désigner
la statistique mesurant cela. 

Cela peut s'effectuer de la manière suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```shell
$ coverage run -m pytest test_create_variable_title.py
$ coverage report -m

Name                            Stmts   Miss  Cover   Missing
-------------------------------------------------------------
import_data.py                     15      6    60%   16-19, 31-34
test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------
TOTAL                              36      7    81%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualité. Il existe d'ailleurs
des badges `Github` dédiés. 
:::

[^emplacement]: L'emplacement de ce fichier est amené à évoluer dans le cadre
    d'une packagisation. Dans un package, ces tests seront dans un dossier
    spécifique `/tests` car `Python` sait gérer de manière plus formelle
    les imports de fonctions depuis des modules. Ici, on est dans une 
    situation transitoire, raison pour laquelle les tests
    sont dans les mêmes dossiers que les fonctions. 

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli9
```

ou

- [`test_create_variable_title.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application9/test_create_variable_title.py)

Les autres fichiers sont inchangés.

:::


## Étape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple à transformer
en package, en s'inspirant du `cookiecutter` adapté, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

<details>
<summary>Structure visée</summary>

```shell
ensae-reproductibilite-application
├── docs                                    ┐ 
│   ├── main.py                             │ 
│   └── notebooks                           │ Package documentation and examples
│       ├── titanic.ipynb                   │ 
├── README.md                               ┘ 
├── pyproject.toml                          ┐ 
├── requirements.txt                        │
├── src                                     │
│   └── titanicml                           │ Package source code, metadata,
│       ├── __init__.py                     │ and build instructions 
│       ├── config.yaml                     │
│       ├── import_data.py                  │
│       ├── build_features.py               │ 
│       └── train_evaluate.py               ┘
└── tests                                   ┐
    └── test_create_variable_title.py       ┘ Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```shell
ensae-reproductibilite-application
├── notebooks                                 
│   └── titanic.ipynb                  
├── configuration                                 
│   └── config.yaml                  
├── main.py                              
├── README.md                 
├── requirements.txt                      
└── src 
    ├── data                                
    │   ├── import_data.py                    
    │   └── test_create_variable_title.py      
    ├── features                           
    │   └── build_features.py      
    └── models                          
        └── train_evaluate.py              
```
</details>

::: {.callout-tip}

## Application 10: packagisation _(optionnel)_

- Déplacer les fichiers dans le dossier `src` pour respecter la nouvelle
arborescence ;
- Dans `src/titanicml`, créer un fichier vide `__init__.py`[^init] ;
- Déplacer le fichier de configuration dans le _package_ (nécessaire à la reproductibilité) ;
- Créer le dossier `docs` et mettre les fichiers indiqués dedans
- Modifier `src/titanicml/import_data.py` :
    + Ajouter la variable `config_file = os.path.join(os.path.dirname(__file__), "config.yaml")`. Cela permettra d'utiliser directement le fichier ;
    + Proposer un argument par défaut à la fonction `import_config_yaml` égal à `config_file`
- Créer un fichier `pyproject.toml` à partir du contenu de [ce modèle de `pyproject`](https://github.com/linogaliana/ensae-reproductibilite-application/blob/main/checkpoints/application10/pyproject.toml)[^setuptools]
- Installer le package en local avec `pip install .`
- Modifier le contenu de `docs/main.py` pour importer les fonctions de notre _package_ `titanicml` et tester en 
ligne de commande notre fichier `main.py`
:::

[^init]: Le fichier `__init__.py` indique à `Python` que le dossier
est un _package_. Il permet de proposer certaines configurations
lors de l'import du _package_. Il permet également de contrôler
les objets exportés (c'est-à-dire mis à disposition de l'utilisateur)
par le _package_ par rapport aux objets internes au _package_. 
En le laissant vide, nous allons utiliser ce fichier 
pour importer l'ensemble des fonctions de nos sous-modules. 
Ce n'est pas la meilleure pratique mais un contrôle plus fin des
objets exportés demanderait un investissement qui ne vaut, ici, pas
le coût. 


[^setuptools]: Ce `pyproject.toml` est un modèle qui utilise `setuptools`
    pour _build_ le _package_. C'est l'outil classique. 
    Néanmoins, pour des usages plus raffinés, 
    il peut être utile d'utiliser [`poetry`](https://python-poetry.org/)
    qui propose des fonctionnalités plus complètes.  


::: {.callout-note}

Pour créer la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapté,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a déjà une structure très modulaire, on va plutôt recréer cette
structure dans notre projet déjà existant. En fait, il ne manque qu'un fichier essentiel, 
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```shell
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>Dérouler pour voir les choix possibles</summary>
```shell
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]: 
python_version [3.9]: 
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]: 
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli10
```

:::



# Partie 3 : construction d'un projet portable et reproductible {#partie3}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues
dans les chapitres [Qualité du code](/chapters/code-quality.html)
et [Structure des projets](/chapters/projects-architecture.html)
tout au long du cours.

Ce faisant, on s'est déjà considérablement rapprochés d'une
possible mise en production : le code est lisible,
la structure du projet est normalisée et évolutive,
et le code est proprement versionné sur un
dépôt `GitHub`.


<details>
<summary>
Illustration de l'état actuel du projet 
</summary>
![](/schema_post_appli8.png)
</details>



A présent, nous avons une version du projet qui est largement partageable.
Du moins en théorie, car la pratique est souvent plus compliquée :
il y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.

Dans cette seconde partie, nous allons voir 
comment **normaliser l'environnement d'exécution afin de produire un projet portable**.
Autrement dit, nous n'allons plus nous contenter de modularité mais allons rechercher
la portabilité.
On sera alors tout proche de pouvoir mettre le projet en production.
On progressera dans l'échelle de la reproductibilité 
de la manière suivante: 

- :one: [**Environnements virtuels**](#anaconda) ;
- :two: Créer un script shell qui permet, depuis un environnement minimal, de construire l'application de A à Z ;
- :three: [**Images et conteneurs `Docker`**](#docker).


Nous allons repartir de l'application 8, c'est-à-dire d'un projet
modulaire mais qui n'est pas, à strictement parler, un package
(objet des applications optionnelles suivantes 9 et 10). 

Pour se replacer dans l'état du projet à ce niveau,
il est possible d'utiliser le _tag_ _ad hoc_.

```shell
git checkout appli8
```


## Étape 1 : un environnement pour rendre le projet portable {#anaconda}

Pour qu'un projet soit portable, il doit remplir deux conditions:

- Ne pas nécessiter de dépendance
qui ne soient pas renseignées quelque part
- Ne pas proposer des dépendances inutiles, qui ne
sont pas utilisées dans le cadre du projet. 

::: {.panel-tabset}

## Environnement virtuel

L'approche la plus légère est l'environnement virtuel. 
Nous avons en fait implicitement déjà commencé à aller vers
cette direction
en créant un fichier `requirements.txt`. 

:::: {.callout-tip}

## Application 11a: environnement virtuel `venv` 

1. Exécuter `pip freeze` en ligne de commande et observer la (très) longue
liste de package
2. Créer l'environnement virtuel `titanic` en s'inspirant de [la documentation officielle](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)[^pythonversion]
3. Utiliser `ls` pour observer et comprendre le contenu du dossier `titanic/bin` installé
4. Activer l'environnement et vérifier l'installation de `Python` maintenant utilisée par votre machine <!---source titanic/bin/activate && which python---->
5. Vérifier directement depuis la ligne de commande que `Python` exécute bien une commande avec:

    ```shell
    python -c "print('Hello')"
    ```

6. Faire la même chose mais avec `import pandas as pd`
7. Installer les packages à partir du `requirements.txt`. Tester à nouveau `import pandas as pd` pour comprendre la différence. 
8. Exécuter `pip freeze` et comprendre la différence avec la situation précédente.
9. Vérifier que le script `main.py` fonctionne bien. Sinon ajouter les _packages_ manquants dans le `requirements.txt` et reprendre de manière itérative à partir de la question 7
10. Ajouter le dossier `titanic/` au `.gitignore` pour ne pas ajouter ce dossier à `Git`
::::

[^pythonversion]: Si vous désirez aussi contrôler la version de `Python`, ce qui peut être important
dans une perspective de portabilité, vous pouvez ajouter une option, par exemple `-p python3.9`. 

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli11a
```

::::

## Environnement conda

Les environnements `conda` sont plus lourds à mettre en oeuvre que les 
environnements virtuels mais peuvent permettre un contrôle
plus formel des dépendances. 

`conda` est à la fois un gestionnaire de packages (alternative à `pip`)
et d'environnements virtuels. L'inconvénient de l'utilisation de `conda`
pour gérer les environnements virtuels est que cet outil est assez lent 
car l'algorithme de vérification des conflits de version n'est pas
extrêmement rapide.

Pour cette raison, nous allons
utiliser [`mamba`](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html),
un utilitaire de gestion des environnements `conda` qui est plus rapide. 

:::: {.callout-tip}

## Application 11b: environnement `conda` 

1. Exécuter `conda env export` en ligne de commande et observer la (très) longue
liste de package
2. Tester l'utilisation d'un package qu'on n'utilise pas dans notre chaine de
production, par exemple `seaborn`:

    ```shell
    python -c "import seaborn as sns"
    ```

3. Créer un environnement `titanic`
avec [`mamba create`](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html#quickstart)
en listant les packages que vous aviez mis dans le `requirements.txt` et en ajoutant
l'option `-c conda-forge` à la fin pour utiliser [la _conda forge_](https://conda-forge.org/) 
4. Activer l'environnement et vérifier l'installation de `Python` maintenant utilisée par votre machine <!---mamba activate titanic && which python---->
5. _(optionnel)_ Utiliser `ls` dans le dossier parent de `Python`
pour observer et comprendre le contenu de celui-ci

6. Vérifier que cette fois `seaborn` n'est pas installé dans l'environnement :

    ```shell
    python -c "import seaborn as sns"
    ```

7. Exécuter à nouveau `conda env export` et comprendre la différence avec la situation précédente[^splitscreen].
8. Vérifier que le script `main.py` fonctionne bien. Sinon utiliser `mamba install` avec les _packages_ manquants jusqu'à ce 
que la chaine de production fonctionne
9. Créer le fichier `environment.yaml` à partir de `conda env export`:

    ```shell
    conda env export > environment.yaml
    ```

10. Ajouter le dossier `titanic/` au `.gitignore` pour ne pas ajouter ce dossier à `Git`

::::

[^splitscreen]: Pour comparer les deux listes, vous pouvez utiliser la fonctionnalité de _split_ 
du terminal sur `VSCode` pour comparer les outputs de `conda env export` en les mettant 
en face à face. 

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli11b
```

::::


:::

## Étape 2: construire l'environnement de notre application via un script `shell`

Les environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L'idée est de construire une machine, en partant d'une base quasi-vierge, qui permette de construire étape par étape l'environnement nécessaire au bon fonctionnement de notre projet. C'est le principe des conteneurs `Docker`.

Leur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production. 

- Nous allons d'abord créer un script `shell`, c'est à dire une suite de commandes `Linux` permettant de construire l'environnement à partir d'une machine vierge ;
- Nous transformerons celui-ci en `Dockerfile` dans un deuxième temps. C'est l'objet de l'étape suivante. 

::: {.panel-tabset}

## Environnement virtuel

:::: {.callout-tip}

## Application 12a : créer un fichier d'installation de A à Z

1. Créer un service `ubuntu` sur le SSP Cloud
2. Ouvrir un terminal
3. Cloner le dépôt 
4. Se placer dans le dossier du projet avec `cd`
5. Se placer au niveau du checkpoint 11a avec `git checkout appli11a`
6. Via l'explorateur de fichiers, créer le fichier `install.sh` à la racine du projet avec le contenu suivant:

<details>
<summary>Script à créer sous le nom `install.sh` </summary>
```shell
#!/bin/bash

# Install Python
apt-get -y update
apt-get install -y python3-pip python3-venv

# Create empty virtual environment
python3 -m venv titanic
source titanic/bin/activate

# Install project dependencies
pip install -r requirements.txt
```
</details>

6. Changer les permissions sur le script pour le rendre exécutable

```shell
chmod +x install.sh
```

7. Exécuter le script depuis la ligne de commande avec des droits de super-utilisateur (nécessaires pour installer des *packages* via `apt`)

```shell
sudo ./install.sh
```

8. Vérifier que le script `main.py` fonctionne correctement dans l'environnement virtuel créé 

```shell
source titanic/bin/activate
python3 main.py
```

::::

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli12a
```

::::


## Environnement `conda`

:::: {.callout-tip}

## Application 12b : créer un fichier d'installation de A à Z

1. Créer un service `ubuntu` sur le SSP Cloud en cliquant sur [ce lien](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/ubuntu?autoLaunch=false&git.cache=%C2%AB36000%C2%BB&git.repository=%C2%ABhttps%3A%2F%2Fgithub.com%2Flinogaliana%2Fensae-reproductibilite-application-correction.git%C2%BB)
2. Cloner le dépôt et se placer au niveau du checkpoint 11b avec `git checkout appli11b`
3. Se placer dans le dossier du projet avec `cd`
4. On va se placer en super-utilisateur dans la ligne de commande en tapant

```shell
sudo bash
```

5. Créer le fichier `install.sh` avec le contenu suivant:

<details>
<summary>Script à créer sous le nom `install.sh` </summary>
```shell
apt-get -y update && apt-get -y install wget

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \
    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \
    rm -f Miniconda3-latest-Linux-x86_64.sh

PATH="/miniconda/bin:${PATH}"

# Create environment
conda install mamba -c conda-forge
mamba create -n titanic pandas PyYAML scikit-learn -c conda-forge
mamba activate titanic

PATH="/miniconda/envs/titanic/bin:${PATH}"

python main.py
```
</details>

6. Changer les permissions sur le fichier 

```shell
chmod u+x ./install.sh
```

7. Exécuter le script depuis la ligne de commande

::::

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli12b
```

::::

:::


## Étape 3: conteneuriser l'application avec `Docker` {#docker}


::: {.callout-note}
Cette application nécessite l'accès à une version interactive de `Docker`.
Il n'y a pas beaucoup d'instances en ligne disponibles.

Nous proposons deux solutions:

- [Installer `Docker`](https://docs.docker.com/get-docker/) sur sa machine ;
- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_
:::

Maintenant qu'on sait que ce script préparatoire fonctionne, on va le transformer en `Dockerfile` (la syntaxe `Docker` est légèrement différente de la syntaxe `Linux` classique).
Puis on va le tester dans un environnement bac à sable (pour ensuite
pouvoir plus facilement automatiser la construction de l'image
`Docker` par la suite).


::: {.callout-tip}

## Application 13: création de l'image `Docker` 

Se placer dans un environnement avec `Docker`

- Dans le terminal `Linux`, cloner votre dépôt `Github` 
- Créer via la ligne de commande un fichier texte vierge nommé `Dockerfile` (la majuscule au début du mot est importante)

<details><summary>Commande pour créer un `Dockerfile` vierge depuis la ligne de commande</summary>
```shell
touch Dockerfile
```
</details>

- Ouvrir ce fichier via un éditeur de texte et copier le contenu suivant dedans:

<details><summary>Premier `Dockerfile`</summary>
```shell
FROM ubuntu:22.04

WORKDIR ${HOME}/titanic

# Install Python
RUN apt-get -y update && \
    apt-get install -y python3-pip

# Install project dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

CMD ["python3", "main.py"]
```
</details>

### Construire l'image

Le `Dockerfile` est la recette de construction de l'image. La construction effective de l'image à partir de cette recette s'appelle l'étape de `build`.

- Utiliser `docker build` pour créer une image avec le tag `my-python-app`

```shell
docker build . -t my-python-app
```

- Vérifier les images dont vous disposez. Vous devriez avoir un résultat proche de celui-ci :

```shell
$ docker images

REPOSITORY      TAG       IMAGE ID       CREATED         SIZE
my-python-app   latest    c0dfa42d8520   6 minutes ago   836MB
ubuntu          25.04     825d55fb6340   6 days ago      77.8MB
```

### Tester l'image: découverte du cache

L'étape de `build` a fonctionné: une image a été construite.

Mais fait-elle effectivement ce que l'on attend d'elle ?

Pour le savoir, il faut passer à l'étape suivante, l'étape de `run`.

```shell
$ docker run -it my-python-app

python3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory
```

Le message d'erreur est clair : `Docker` ne sait pas où trouver le fichier `main.py`. D'ailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont nécessaires pour faire tourner le code: `config.yaml` et le dossier `src`.

- Avant l'étape `CMD`, copier les fichiers nécessaires sur l'image afin que l'application dispose de tous les éléments nécessaires pour être en mesure de fonctionner.

<details>
<summary>Nouveau `Dockerfile` </summary>
```shell
FROM ubuntu:22.04

WORKDIR ${HOME}/titanic

# Install Python
RUN apt-get -y update && \
    apt-get install -y python3-pip

# Install project dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY main.py .
COPY src ./src
COPY configuration ./configuration
CMD ["python3", "main.py"]
```
</details>

- Refaire tourner l'étape de `build`

- Refaire tourner l'étape de `run`. A ce stade, la matrice de confusion doit fonctionner 🎉.
Vous avez créé votre première application reproductible !

:::

::: {.callout-note}

Ici, le _cache_ permet d'économiser beaucoup de temps. Par besoin de 
refaire tourner toutes les étapes, `Docker` agit de manière intelligente
en faisant tourner uniquement les étapes qui ont changé.

:::


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli13
```

:::

# Partie 4 : automatisation avec l'intégration continue

Une image `Docker` est un livrable qui n'est pas forcément intéressant
pour tous les publics. Certains préféreront avoir un plat bien préparé
qu'une recette. Nous allons donc proposer d'aller plus loin en proposant
plusieurs types de livrables.

Cela va nous amener à découvrir les outils
du `CI/CD` (_Continuous Integration / Continuous Delivery_)
qui sont au coeur de l'approche `DevOps`.

Notre approche appliquée
au _machine learning_ va nous entraîner plutôt du côté du `MLOps` qui devient
une approche de plus en plus fréquente dans l'industrie de la 
_data science_.

Nous allons améliorer notre approche de trois manières:

- Automatisation de la création de l'image `Docker` et tests
automatisés de la qualité du code ;
- Production d'un site _web_ automatisé permettant de documenter et
valoriser le modèle de _Machine Learning_ ;
- Mise à disposition du modèle entraîné par le biais d'une API pour
ne pas le ré-entraîner à chaque fois et faciliter sa réutilisation ;

A chaque fois, nous allons d'abord tester en local notre travail puis
essayer d'automatiser cela avec les outils de `Github`.

On va ici utiliser l'intégration continue pour deux objectifs distincts:

- la mise à disposition de l'image `Docker` ;
- la mise en place de tests automatisés de la qualité du code
sur le modèle de notre `linter` précédent 

Nous allons utiliser `Github Actions` pour cela. 


## Étape 1: mise en place de tests automatisés

Avant d'essayer de mettre en oeuvre la création de notre image
`Docker` de manière automatisée, nous allons présenter la logique
de l'intégration continue en testant de manière automatisée
notre script `main.py`.

Pour cela, nous allons partir de la structure proposée dans l'[action officielle](https://github.com/actions/setup-python). 
La documentation associée est [ici](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)


::: {.callout-tip}

## Application 14: premier script d'intégration continue

A partir de l'exemple présent
dans la [documentation officielle](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)
de `Github`, on a déjà une base de départ qui peut être modifiée:

1. Créer un fichier `.github/workflows/ci.yaml` avec le contenu de l'exemple de la documentation
2. Retirer la `strategy matrix` et ne tester qu'avec la version `3.10` de `Python`
3. Utiliser le fichier `requirements.txt` pour installer les dépendances. 
4. Remplacer `russ` par `pylint` pour vérifier la qualité du code. Ajouter l'argument `--fail-under=6` pour
renvoyer une erreur en cas de note trop basse[^hook]
5. Plutôt que `pytest`, utiliser `python main.py` pour tester que la matrice de confusion s'affiche bien.

[^hook]: Il existe une approche alternative pour faire des tests
    réguliers: les _hooks_ `Git`.
    Il s'agit de règles qui doivent être satisfaites pour que le 
    fichier puisse être committé. Cela assurera que chaque `commit` remplisse
    des critères de qualité afin d'éviter le problème de la procrastination.
    
    La [documentation de pylint](https://pylint.pycqa.org/en/latest/user_guide/pre-commit-integration.html) offre des explications supplémentaires. 


<details>
<summary>Fichier `.github/workflows/ci.yaml` obtenu</summary>

```yaml
name: Build, test and push

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint
          pip install -r requirements.txt
      - name: Lint
        run: |
          pylint src --fail-under=6
      - name: Test workflow
        run: |
          python main.py
```
</details>

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli14
```

:::

 
Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entraîner une action pour
tester nos scripts.

Si la note est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi détecter,
en développant son projet, les moments où on dégrade la qualité du script 
afin de la rétablir immédiatemment. 




## Étape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise à disposition de notre image
sur `DockerHub`. Cela facilitera sa réutilisation mais aussi des
valorisations ultérieures.

Là encore, nous allons utiliser une série d'actions pré-configurées.

Pour que `Github` puisse s'authentifier auprès de `DockerHub`, il va 
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
sécurisé associé à votre dépôt `Github`.

::: {.callout-tip}

## Application 15a: configuration

- Se rendre sur
https://hub.docker.com/ et créer un compte.
- Créer un dépôt public `application-correction`
- Aller dans les paramètres (https://hub.docker.com/settings/general)
et cliquer, à gauche, sur `Security`
- Créer un jeton personnel d'accès, ne fermez pas l'onglet en question,
vous ne pouvez voir sa valeur qu'une fois. 
- Dans votre dépôt `Github`, cliquer sur l'onglet `Settings` et cliquer,
à gauche, sur `Actions`. Sur la page qui s'affiche, cliquer sur `New repository secret`
- Donner le nom `DOCKERHUB_TOKEN` à ce jeton et copier la valeur. Valider
- Créer un deuxième secret nommé `DOCKERHUB_USERNAME` ayant comme valeur le nom d'utilisateur
que vous avez créé sur `Dockerhub`
:::


A ce stade, nous avons donné les moyens à `Github` de s'authentifier avec
notre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action
en s'inspirant de https://github.com/docker/build-push-action/#usage.
On ne va modifier que trois éléments dans ce fichier. Effectuer les 
actions suivantes:

::: {.callout-tip}

## Application 15b: automatisation de l'image `Docker`

- En s'inspirant de ce [_template_](https://github.com/marketplace/actions/build-and-push-docker-images), ajouter un nouveau job `docker` dans le fichier `ci.yaml` qui va *build* et *push* l'image sur le `DockerHub`
- Définir le job `test` comme prérequis du job `docker` en vous référant à cette [documentation](https://docs.github.com/en/actions/using-jobs/using-jobs-in-a-workflow#defining-prerequisite-jobs)
- Changer le tag à la fin pour mettre `<username>/application-correction:latest`
où `username` est le nom d'utilisateur sur `DockerHub`;

<details>
<summary>Fichier `.github/workflows/ci.yaml` obtenu</summary>

```yaml
name: Build, test and push

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint
          pip install -r requirements.txt
      - name: Lint
        run: |
          pylint src --fail-under=6
      - name: Test workflow
        run: |
          python main.py
  docker:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          push: true
          tags: linogaliana/application-correction:latest
```
</details>

- Faire un `commit` et un `push` de ces fichiers
:::

Comme on est fier de notre travail, on va afficher ça avec un badge sur le 
`README`. 

::: {.callout-tip}

## Application 15c: Afficher un badge dans le `README`

- Se rendre dans l'onglet `Actions` et cliquer sur un des scripts en train de tourner. 
- En haut à droite, cliquer sur `...`
- Sélectionner `Create status badge`
- Récupérer le code `Markdown` proposé
- Copier dans le `README` depuis `VSCode`
- Faire de même pour l'autre _workflow_

:::

Maintenant, il nous reste à tester notre application dans l'espace bac à sable
ou en local, si `Docker` est installé.


::: {.callout-tip}

## Application 15d: Tester l'application

- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_
- Récupérer l'image :

```yaml
docker pull <username_dockerhub>/application-correction:latest
```

- Tester le bon fonctionnement de l'image

```yaml
docker run -it <username_dockerhub>/application-correction:latest
```

:tada: La matrice de confusion doit s'afficher ! Vous avez grandement
facilité la réutilisation de votre image. 

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli15
```

:::


# Partie 5: mise en production d'une API servant un modèle de machine learning

## Étape 1: création d'un pipeline `scikit`


Notre code respecte des bonnes pratiques formelles. Cependant, la mise en
production nécessite d'être exigeant sur la mise en oeuvre opérationnelle
de notre _pipeline_. 

Quand  on utilise `scikit`, la bonne pratique est d'utiliser
les [_pipelines_](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
qui sécurisent les étapes de _feature engineering_ avant la mise en oeuvre d'un modèle (que
ce soit pour l'entraînement ou le test sur un nouveau jeu de données). 

On va donc devoir refactoriser notre application pour utiliser un _pipeline_ `scikit`. 
Les raisons sont expliquées [ici](https://scikit-learn.org/stable/common_pitfalls.html).
Cela aura
aussi l'avantage de rendre les étapes plus lisibles. 


::: {.callout-tip}

## Application 16: Un _pipeline_ de _machine learning_

- Refactoriser le code de `random_forest_titanic` pour créer 
un vrai pipeline de _preprocessing_ avant la modélisation

- Simplifier la fonction `split_train_test_titanic`
en la réduisant au découpage train/test

- Modifier `main.py` pour que ce soit à ce niveau
qu'a lieu le découpage en train/test, l'entrainement
et l'évaluation
du modèle

:::


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli16
```

:::

## Étape 2: développer une API en local

::: {.callout-tip}

## Application 17: Mise à disposition sous forme d'API locale

- Créer un nouveau service `SSPCloud` en paramétrant dans l'onglet
`Networking` le port 5000 ;
- Cloner le dépôt et se placer au niveau de l'application précédente (`git checkout appli16`)
- Installer `fastAPI` et `uvicorn` puis les ajouter au `requirements.txt`
- Renommer le fichier `main.py` en `train.py` et insérer le contenu suivant dedans :

<details>
<summary>
Fichier `train.py`
</summary>
Récupérer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/appli17/train.py)
</details>

- Créer le fichier `api.py` permettant d'initialiser l'API:

<details>
<summary>
Fichier `api.py`
</summary>
Récupérer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/appli17/api.py)
</details>

- Exécuter `train.py` pour stocker en local le modèle entraîné
- Ajouter `model.joblib` au `.gitignore`
- Déployer en local l'API avec la commande

```shell
uvicorn api:app --reload --host "0.0.0.0" --port 5000
```

- A partir du `README` du service, se rendre sur l'URL de déploiement, 
ajouter `/docs/` à celui-ci et observer la documentation de l'API 
- Se servir de la documentation pour tester les requêtes `/predict`
- Récupérer l'URL d'une des requêtes proposées. La tester dans le navigateur
et depuis `Python` avec `requests` (`requests.get(url).json()`)

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli17
```

:::

## Étape 3: déployer l'API

A ce stade, nous avons déployé l'API seulement localement, dans le cadre d'un service. Ce mode de déploiement est très pratique pour la phase de développement, afin de s'assurer que l'API fonctionne comme attendu. A présent, il est temps de passer à l'étape de déploiement, qui permettra à notre API d'être accessible via une URL sur le web, et donc aux utilisateurs potentiels de la requêter. Pour se faire, on va utiliser les possibilités offertes par `Kubernetes`, sur lequel est basé le [SSP Cloud](https://datalab.sspcloud.fr).

::: {.callout-tip}

## Application 18: Dockeriser l'API

- Modifier le `Dockerfile` pour tenir compte des changements dans les noms de fichier effecutés dans l'application précédente

- Créer un script `run.sh` à la racine du projet qui lance le script `train.py` puis déploie localement l'API 

<details>
<summary>Fichier `run.sh`</summary>

```shell
#/bin/bash

python3 train.py
uvicorn api:app --reload --host "0.0.0.0" --port 5000
```
</details>

- Donner au script `run.sh` des permissions d'exécution : `chmod +x run.sh`

- Changer l'instruction `CMD` du `Dockerfile` pour exécuter le script `run.sh` au lancement du conteneur

- *Commit* et *push* les changements

- Une fois le CI terminé, récupérer la nouvelle image dans l'environnement de test et vérifier que l'API se déploie correctement

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli18
```
:::

::: {.callout-tip}

## Application 19: Déployer l'API

- Créer un dossier `deployment` à la racine du projet qui va contenir les fichiers de configuration nécessaires pour déployer sur un cluster `Kubernetes`

- En vous inspirant de la [documentation](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), y ajouter un premier fichier `deployment.yaml` qui va spécifier la configuration du *Pod* à lancer sur le cluster

<details>
<summary>Fichier `deployment/deployment.yaml`</summary>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: titanic-deployment
  labels:
    app: titanic
spec:
  replicas: 1
  selector:
    matchLabels:
      app: titanic
  template:
    metadata:
      labels:
        app: titanic
    spec:
      containers:
      - name: titanic
        image: linogaliana/application-correction:latest
        ports:
        - containerPort: 5000
```
</details>

- En vous inspirant de la [documentation](https://kubernetes.io/fr/docs/concepts/services-networking/service/#d%C3%A9finition-d-un-service), y ajouter un second fichier `service.yaml` qui va créer une ressource `Service` permettant de donner une identité fixe au `Pod` précédemment créé au sein du cluster

<details>
<summary>Fichier `deployment/service.yaml`</summary>

```yaml
apiVersion: v1
kind: Service
metadata:
  name: titanic-service
spec:
  selector:
    app: titanic
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000
```
</details>

- En vous inspirant de la [documentation](https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource), y ajouter un troisième fichier `ingress.yaml` qui va créer une ressource `Ingress` permettant d'exposer le service via une URL en dehors du cluster

<details>
<summary>Fichier `deployment/ingress.yaml`</summary>

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: titanic-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - titanic.kub.sspcloud.fr
  rules:
  - host: titanic.kub.sspcloud.fr
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: titanic-service
            port:
              number: 80
```
</details>

- Appliquer ces fichiers de configuration sur le cluster : `kubectl apply -f deployement/`

- Si tout a correctement fonctionné, vous devriez pouvoir accéder à l'API à l'URL spécifiée dans le fichier `deployment/ingress.yaml`

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli19
```

:::


# Partie 6: un workflow complet de MLOps

Ce sera l'an prochain, désolé !

# Partie 7: livrer un site web de manière automatisée

On va proposer un nouveau livrable pour parler à un public plus large.
Pour cela, on va déployer un site web statique qui permet de visualiser
rapidement les résultats du modèle.

On propose de créer un site web qui permet de comprendre, avec l'appui
des [valeurs de Shapley](https://christophm.github.io/interpretable-ml-book/shapley.html),
les facteurs qui auraient pu nous mettre la puce
à l'oreille sur les destins de Jake et de Rose. 

Pour faire ce site web,
on va utiliser `Quarto` et déployer sur `Github Pages`.
Des étapes préliminaires sont réalisées en `Python` 
puis l'affichage interactif 
sera contrôlé par du `JavaScript` grâce
à des [blocs `Observable`](https://quarto.org/docs/interactive/ojs/). 


::: {.callout-tip}

## Application 19: Déploiement automatisé d'un site web

Dans un premier temps, on va créer un projet `Quarto`
au sein de notre dépôt: 

- Installer `Quarto` dans votre environnement local (s'il n'est pas déjà disponible) ;
- Dans le projet, utiliser la commande `quarto create-project` pour initialiser le projet `Quarto` ;
- Supprimer le fichier automatiquement généré avec l'extension `.qmd` ;
- Récupérer le contenu du modèle de fichier `Quarto Markdown` [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/index.qmd). Celui-ci permet de générer la page d'accueil de notre site. Enregistrer dans un fichier nommé `index.qmd`

On teste ensuite la compilation en local du fichier:

- Modifier le fichier `train.py` à partir de [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/train.py) pour être en mesure de compiler le fichier 
- Exécuter le fichier `train.py`
- En ligne de commande, faire `quarto preview` (ajouter les arguments `--port 5000 --host 0.0.0.0` si vous passez par le `SSPCloud`)
- Observer le site web généré en local

Enfin, on va construire et déployer automatiquement ce site web grâce au
combo `Github Actions` et `Github Pages`:

- Créer une branche `gh-pages` à partir du contenu de [cette page](https://quarto.org/docs/publishing/github-pages.html)
- Créer un fichier `.github/workflows/website.yaml` avec le contenu de [ce fichier](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/.github/workflows/publish.yaml)

:::

::: {.callout-note}

On doit dans cette application modifier le fichier `train.py`
pour enregistrer en local une duplication du modèle
de _machine learning_ et de l'ensemble d'entraînement
car pour ces deux éléments
on n'est pas allé au bout de la démarche MLOps
d'enregistrement dans un _model registry_ et un
_feature store_.

Dans la prochaine version de ce cours, qui
intègrera `MLFlow`, on aura une démarche plus 
propre car on utilisera bien le modèle de production
et le jeu d'entrainement associé. 
:::


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli20
```

:::