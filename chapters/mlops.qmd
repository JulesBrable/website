---
title: "Vers le MLOps"
author: "Romain Avouac et Lino Galiana"
description: |
  <br>
  Présentation de l'approche MLOps.
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/mise-en-prod/army.png
order: 6
href: chapters/mlops.html
---


Grâce aux chapitres précédents on sait comment structurer et rendre son projet de de data science reproductible et portable grâce à l'approche DevOps. Pour de nombreux projets de data science cela peut s'avérer suffisant. Cependant, dès lors que l'on intègre un modèle de machine learning à notre projet l'approche DevOps devient alors une condition nécessaire mais plus suffisante pour réussir à mettre en production notre modèle. Aujourd'hui, une très grande majorité des modèles de machine learning ne dépassent pas le stade de l'expérimentation et très peu parviennent à atteindre le stade de la production. Pour simplifier ce passage vers la production une nouvelle approche a émergé en intégrant les spécificités des modèles de machine learning : le MLOps. On peut voir le MLOps comme la somme de 3 buzzwords à savoir :

MLOps = DataOps + DevOps + ModelOps

Ce chapitre définie cette approche et propose présente plusieurs outils pour l'adopter à ces projets de data science.



# Du DevOps vers le MLOps

## L'approche DevOps


L'approche DevOps, contraction des termes "Development" et "Operations", est une méthodologie de développement logiciel qui vise à améliorer la collaboration et la communication entre les équipes de développement (Dev) et l'administration système (Ops). Son objectif principal est d'accélérer et optimiser le cycle de vie du développement logiciel, de la conception à la production, en favorisant l'automatisation, l'intégration continue et le déploiement continu (cf. chapitre sur la mise en production).

Cette approche, datant de 2007, vise à éliminer les barrières traditionnelles entre le développement et l'informatique, favorisant ainsi une culture de responsabilité partagée. Les principes fondamentaux de DevOps incluent l'**automatisation** des processus, la **surveillance** continue des performances, la **gestion agile** des changements, et la création d'une **boucle de rétroaction** rapide pour améliorer en permanence les processus de développement et d'exploitation.

En résumé, DevOps vise à créer un écosystème de développement collaboratif, automatisé et axé sur l'amélioration continue. Le cycle de vie DevOps est souvent résumé par ces 8 phases inter-connectées : 

![](/devops.png){fig-align="center"}

## L'approche MLOps 

L'approche MLOps s'est construite sur les bases de l'approche DevOps. En cela on peut considérer qu'il s'agit simplement d'une extension à l'approche DevOps qui s'est développée pour répondre aux défis spécifiques liés à la gestion du cycle de vie des modèles de machine learning. Le MLOps intègre les principes de collaboration et d'automatisation propre au DevOps mais prend également en compte tous les aspects liés aux données et aux modèles de machine learning. 

Le MLOps implique l'**automatisation des tâches** telles que la gestion des données, le suivi des **versions des modèles**, leurs **déploiements**, ainsi que l'**évaluation continue** de la performance des modèles en production. De la même manière que le DevOps, le MLOps met l'accent sur la collaboration étroite entre les équipes de développement et de l'administration système d'une part ainsi que les équipes de data science d'autre pas. Cette collaboration est clé pour garantir une communication efficace tout au long du cycle de vie du modèle de machine learning.

Ainsi on définit généralement les 5 piliers du MLOps comme étant :

  - **la reproductibilité** : chaque expérimentation doit pouvoir être reproduite sans coût (gestion des packages, gestion des environnements, gestion des librairies système, gestion de version du code ...)
  - **la collaboration** : les différentes parties prenantes au projet doivent communiquer afin d'optimiser le cycle de vie du modèle en production
  - **l'automatisation** : le cycle de vie du modèle doit être automatisé au maximum avec des *pipeline* de CI/CD
  - **le contrôle de version des modèles** : chaque modèle doit avoir une version spécifique permettant de connaître sa situation tout au long de son cycle de vie (expérimental, en production, archivé...). De plus, un modèle ne dépend pas seulement des scripts associés, mais également des données utilisées lors de l'entraînement ce qui rend d'autant plus nécessaire une bonne gestion des versions
  - **la surveillance** : une fois déployé, il est important de s'assurer que le modèle fonctionne bien comme attendu en évaluant ses performances en temps réel

![](/mlops.png){fig-align="center"}

# Implémentation de l'approche MLOps avec MLflow

## Pourquoi MLflow ?

Il existe aujourd'hui de nombreux outils pour orchestrer des tâches et des pipelines de données. Parmi les plus populaires (selon leur ⭐ Github) on peut citer [Airflow](https://github.com/apache/airflow), [Luigi](https://github.com/spotify/luigi), [MLFlow](https://github.com/mlflow/mlflow), [Argo Workflow](https://github.com/argoproj/argo-workflows), [Prefect](https://github.com/PrefectHQ/prefect) ou encore [Kubeflow](https://github.com/kubeflow/kubeflow). Il est difficile d'affirmer s'il y en a un meilleur qu'un autre, en réalité votre choix dépend surtout de votre infrastructure informatique et de votre projet. En l’occurrence ici, nous avons fait le choix d'utiliser MLflow pour la simplicité d'utilisation mais aussi car il nous semble être le plus spécialisé pour les projet de machine learning. De plus, il est présent dans le catalogue du SSP Cloud ce qui simplifie grandement son installation. Nous utiliserons également [Argo CD](https://github.com/argoproj/argo-cd) et [Argo Workflow](https://github.com/argoproj/argo-workflows) plutôt qu'[Airflow](https://github.com/apache/airflow) car ils sont optimisés pour les clusters kubernetes (comme le SSP Cloud !).

MLflow est une plateforme qui permet d'optimiser le développement du cycle de vie d'un modèle de machine learning. Il permet de suivre en détails les différentes expérimentations, de *packager* son code pour garantir la reproductibilité, et de servir un modèle à des utilisateurs. MLFlow possède aussi une API qui permet d'être compatible avec la majorité des librairies de machine learning (PyTorch, Scikit learn, XGBoost, etc) mais également différents languages (python, R et Java).

## Les projets MLflow

MLflow propose un format pour *packager* son projet de data science afin de favoriser la réutilisation et la reproductibilité du code. Ce format s'appelle tout simplement *[MLflow Project](https://mlflow.org/docs/latest/projects.html)*. Concrètement un *MLflow project* n'est rien d'autre qu'un répertoire contenant le code et les ressources nécessaires (données, fichiers de configuration...) nécessaire à l'exécution de votre projet. Il est résumé par un ficher `MLproject` qui résume les différentes commandes pour exécuter une pipeline ainsi que les dépendances nécessaires. De manière général un projet MLflow a la structure suivante : 

```
Projet_ML/
├── artifacts/
│   ├── model.bin
│   └── train_text.txt
├── code/
│   ├── main.py
│   └── preprocessing.py
├── MLmodel
├── conda.yaml
├── python_env.yaml
├── python_model.pkl
└── requirements.txt
```


## Les modèles MLflow

En plus de *packager* son projet, MLflow permet de également de *packager* son modèle **quelque que soit** la librairie de machine learning sous-jacente utilisée (parmi celles [compatibles avec MLflow](https://mlflow.org/docs/latest/models.html#built-in-model-flavors), aka toutes les librairies que vous utilisez !). Ainsi, deux modèles entraînés avec des librairies différentes, disons PyTorch et Keras, peuvent être ainsi déployé et requêté de la même manière grâce à cette surcouche rajoutée par MLflow.

![](/mlflow-models.png){fig-align="center"}

::: {.callout-tip}
## Tip

Il est également possible de *packager* son propre modèle personnalisé ! Pour cela vous pouvez suivre le tutoriel présent dans la [documentation](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html?highlight=custom%20models).
:::

## Le serveur de suivi (*tracking server*)

MLflow fourni un serveur de suivi qui contient à la fois une interface graphique ergonomique ainsi qu'une API permettant de *logger* différents paramètres, métriques, fichiers, etc durant l’entraînement de votre modèle de machine learning.
Le *tracking server* est très utile pour comparer les différentes expérimentations que vous avez effectué, pour les stocker et également pour être capable de les reproduire. En effet, chaque *run* sauvegarde la source des données utilisées mais également le commit sur lequel le *run* est basé. 

![](/tracking-server.png){fig-align="center"}


## L'entrepôt de modèles (*model registry*)

Une fois que l'on a effectué différentes expérimentations et pu sélectionner les modèles qui nous satisfont il est temps de passer à l'étape suivante du cycle de vie d'un modèle. En effet, le modèle choisi doit pouvoir ensuite passer dans un environnement de production ou de pré-production. Or, connaître l'état d'un modèle dans son cycle de vie nécessite une très bonne organisation et n'est pas si aisé. MLflow a développé une fonctionnalité qui permet justement de simplifier cette gestion des versions des modèles grâce à son [Model Registry](https://mlflow.org/docs/latest/model-registry.html). Cet entrepôt permet d'ajouter des tags et des alias à nos modèles pour définir leur position dans leur cycle de vie et ainsi pouvoir les récupérer de manière efficace. 

De manière générale un modèle de machine learning passe par 4 stades qu'il est nécessaire de pouvoir connaître en tout temps :

1. **Expérimental**
2. **En évaluation**
3. **En production**
4. **Archivé**

![](/mlflow-model-registry.png){fig-align="center"}

## MLflow en résumé

MLflow est donc un projet open-source qui fournit une plateforme pour suivre le cycle de vie d'une modèle de machine learning de bout en bout. Ce n'est pas le seul outil disponible et il n'est peut être pas le plus adapté à certains de vos projets précis. En revanche, il présente selon nous plusieurs avantages en premier desquels sa très simple prise en main et sa faculté à répondre aux besoins de l'approche MLOps. Il faut garder en tête que cet environnement est encore très récent et de nouveaux projets open-source émergent chaque jour, il est nécessaire de faire une veille sur les dernières évolutions. 

Pour résumer, MLFlow permet :

- de simplifier le suivi de l’entraînement des modèles de machine learning grâce à son API et son *tracking server*
- d'intégrer les principaux framework de machine learning de manière simple
- d'intégrer son propre framework si besoin
- de standardiser son script d'entraînement et donc pouvoir l'industrialiser, pour réaliser un *fine-tuning* des hyper-paramètres par exemple
- de *packager* ses modèles, de sorte à pouvoir les requêter de manière simple et harmonisée entre les différents frameworks
- stocker ses modèles de manières pertinentes en leur affectant des tags et favorisant le suivi de leur cycle de vie

# Spécificités liées à la mise en production de modèles de ML

## 1️⃣ Entraînements des modèles


La première étape d'un projet de machine learning corresponds à tout ce que l'on effectue jusqu'à l'entraînement des premiers modèles. Cette étape est un - fastidieux - processus itératif qui ne suit pas un développement linéaire : les méthodes de récupération des données peuvent être changeantes, le preprocessing peut varier de même que la selection des features pour le modèle (*feature engineering*), les algorithmes testés peuvent être nombreux... Garder une trace de tous les essais effectués apparaît indispensable afin de savoir ce qui a fonctionné ou non. Cette étape barbante est rendue très simple grâce au *Tracking Server* de MLFlow. 
Lors de l'exécution d'un *run* MLFlow enregistre tout un tas de métadonnées qui permet de retrouver toutes les informations relatives à ce *run* : la date, le hash du commit, les paramètres du modèles, le dataset utilisé, les métriques spécifiées etc. Cela permet, non seulement de comparer les différents essais réalisés mais aussi d'être capable de reproduire une *run* passé.

De manière générale cette phase exploratoire est réalisée par le data-scientist dans des notebooks. Ces notebooks sont effet parfaitement adaptés pour cette étape puisqu'ils permettent une grande flexibilité et sont particulièrement commode pour effectuer des tests. En revanche, lorsque l'on souhaite aller plus loin et que l'on vise une mise en production de son projet, les notebooks ne sont plus adaptés et cela pour diverses raisons : 

- la collaboration est grandement limitée à cause d'une compatibilité très faible avec les outils de contrôle de version standard (git).
- l'automatisation de pipeline est beaucoup plus compliqué et peu lisible. Il existe cependant des packages qui permettent d'automatiser des *pipelines* de notebooks comme [Elyra](https://github.com/elyra-ai/elyra) par exemple, mais ce n'est clairement pas l'approche que nous vous recommandons.
- Les *workflows* sont souvent moins clairs et peu reproductibles avec souvent une organisation peu optimale (toutes les fonctions définies dans le même fichiers affectant la lisibilité du code par exemple)
- les notebooks offrent généralement une modularité insuffisante lorsque l'on veut travailler avec des composants de machine learning complexes

Toutes ces raisons nous amènent à vous conseiller de réduire au maximum votre utilisation de notebook et de restreindre leur utilisation à la phase exploratoire ou à la diffusion de résultats/rapports. Passer le plus tôt possible à des script `.py` vous permettra de réduire le coût de la mise en production. Pour reprendre ce qui a déjà été évoqué dans le chapitre [Architecture des projets](/chapters/projects-architecture.qmd), nous vous invitons à favoriser une structure modulaire de sorte à pouvoir industrialiser votre projet.

Une autre spécificité pouvant impacter la mise en production concerne la manière dont l'entraînement est réalisé. Il existe pour cela 2 écoles qui ont chacune leurs avantages et désavantages : le *batch training* et l'*online training*. 

### Batch training

Le *batch training* est la manière usuelle d'entraîner un modèle de machine learning. Cette méthode consiste à entraîner son modèle sur un jeu de données fixe d'une seule traite. Le modèle est entraîné sur l'intégralité des données disponibles et les prédictions sont réalisées sur de nouvelles données. Cela signifie que le modèle n'est pas mis à jour une fois qu'il est entraîné, et qu'il est nécessaire de le ré-entraîner si l'on souhaite ajuster ses poids. Cette méthode est relativement simple à mettre en œuvre : il suffit d'entraîner le modèle une seule fois, de le déployer, puis de le ré-entraîner ultérieurement en cas de besoin. Cependant, cette simplicité comporte des inconvénients : le modèle reste statique, nécessitant un ré-entraînement fréquent pour intégrer de nouvelles données. Par exemple, dans le cas de la détection de spams, si un nouveau type de spam apparaît, le modèle entraîné en batch ne sera pas capable de le détecter sans un ré-entraînement complet. De plus, cette méthode peut rapidement exiger une grande quantité de mémoire en fonction de la taille du jeu de données, ce qui peut poser des contraintes sur l'infrastructure et prolonger considérablement le temps d'entraînement.

### Online training

L'*online training* se présente comme l'antithèse du *batch training*, car il se déroule de manière incrémentale. Dans ce mode d'entraînement, de petits lots de données sont envoyés séquentiellement à l'algorithme, ce qui permet à celui-ci de mettre à jour ses poids à chaque nouvelle donnée reçue. Cette approche permet au modèle de détecter efficacement les variations dans les données en temps réel. Il est toutefois crucial de bien ajuster le learning rate afin d'éviter que le modèle oublie les informations apprises sur les données précédentes. L'un des principaux avantages de cette méthode est sa capacité à permettre un entraînement continu même lorsque le modèle est en production, ce qui se traduit par une réduction des coûts computationnels. De plus, l'*online training* est particulièrement adapté aux situations où les données d'entrée évoluent fréquemment, comme dans le cas des prédictions de cours de bourse. Cependant, sa mise en œuvre dans un contexte de production est bien plus complexe que celle du *batch training*, et les frameworks traditionnels de machine learning tels que Scikit-learn, PyTorch, TensorFlow et Keras ne sont pas compatibles avec cette approche.

### Distribuer l'optimisation des hyperparamètres

Une autre spécificité des modèles de machine learning est le nombre important d'hyperparamètres à optimiser qui peuvent impacter sensiblement la performance du modèle. L'approche standard dans pour réaliser cette optimisation est de réaliser ce qu'on appelle un *Grid Search*. Il s'agit simplement de lister toutes les combinaisons d'hyperparamètres que l'on souhaite tester et entraîner successivement des modèles avec ces combinaisons prédéfinie. Il n'est pas difficile de comprendre que cette technique est très coûteuse en temps de calcul dès lors qu'on a beaucoup d'hyperparamètres à optimiser et que ceux ci ont beaucoup de modalité à tester. Cependant cette optimisation est indispensable pour entraîner le meilleur modèle pour notre tâche, et si s'inspirer de la littérature est crucial pour limiter le domaine d'optimisation de nos hyperparamètres, réaliser un *Grid Search* est une étape incontournable. 

Ainsi pour coller à l'approche du MLOps, une bonne méthode est d'automatiser cette optimisation d'hyperparamètres en la distribuant sur un cluster (cela va sans dire que l'approche MLOps nécessite une infrastructure adéquate !). L'idée est de définir un *workflow* qui effectue l'entraînement d'un modèle et *loggue* les informations dans MLFlow. Pour réaliser cela, il existe un moteur de workflow populaire pour orchestrer des tâches parallèles sur un cluster Kubernetes : [Argo Workflow](https://github.com/argoproj/argo-workflows). Le principe est de définir un *workflow* dans lequel chaque étape correspond à un conteneur qui ne contiendrait que ce qui est strictement nécessaire à l'exécution de cette étape. On n'est ainsi pas loin de la perfection en ce qui concerne la reproductibilité puisque l'on est totalement maître des installations nécessaires à l'exécution de notre entraînement. Un *workflow* à plusieurs étapes peut ainsi être modélisé comme un graphe acyclique orienté et la l'exemple ci-dessous représente un cas d'optimisation d'hyperparamètres :

![Workflow d'optimisation d'hyperparamètres en parallèle](/dag-argo-workflow.png){fig-align="center"}

Cette approche permet d'exécuter facilement en parallèle des tâches intensives en calcul de manière totalement reproductible. Évidemment, l'utilisation de tels *workflow* ne se limite pas à l'optimisation d'hyperparamètres mais peut également être utilisé pour du preprocessing de données, pour créer des pipeline d'ETL etc.


## 2️⃣ Servir un modèle ML à des utilisateurs

Une partie très importante, et parfois négligée, des projets de machine learning est la mise à disposition des modèles entraînés à d'autres utilisateurs. Puisque vous avez parfaitement suivi les différents chapitres de ce cours, votre projet est en théorie totalement reproductible. Une manière triviale de transmettre le modèle que vous avez sélectionné serait de partager votre code et toutes les informations nécessaires pour qu'une personne tierce ré-entraîne votre modèle de son côté. Évidemment, ce procédé n'est pas optimal puisqu'il suppose que tous les utilisateurs ont les ressources/infrastructures/connaissances pour réaliser l'entraînement. 

L'objectif est donc de mettre à disposition de manière simple et efficace votre modèle. Pour cela, plusieurs possibilités s'offrent à vous en fonction de votre projet et il est important de se poser quelques questions préalables :

- Quel est le format pertinent à mettre à disposition des utilisateurs ?
- Les prédictions du modèles doivent-elles être réalisées par lots (*batchs*) ou en ligne (*online*) ?
- Quelle infrastructure utiliser pour déployer notre modèle de machine learning ?

Dans le cadre de ce cours nous avons choisi d'utiliser une API REST pour mettre à disposition un modèle de machine learning. Cela nous semble être la méthode la plus adaptée dans une grande majorité des cas puisqu'il répond à plusieurs critères : 

- **Simplicité** : les API REST permettent de créer une une porte d'entrée qui peut cacher la complexité sous-jacente du modèle facilitant ainsi la mise à disposition de celui-ci.
- **Standardisation** : un des principaux avantages des API REST est qu'elles reposent sur le standard HTTP. Cela signifie qu'elles sont agnostique au langage de programmation utilisé et que les requêtes peuvent être réalisée en XML, JSON, HTML etc.
- **Modularité** : Le client et le serveur sont indépendants. En d'autres mots, le stockage des données, l'interface graphique ou encore la gestion du modèle sont complètement séparés de la mise à disposition (le serveur). 
- **Passage à l’échelle** : la séparation en le serveur et le client permet aux API REST d'être très flexible et facilite le passage à l'échelle (*scalability*). Elles peuvent ainsi s'adapter à la charge de requêtes concurrentes.


L'exposition d'un modèle de machine learning peut être résumer par ce schéma :

![Exposer un modèle de ML via une API](/API-diag.png){fig-align="center"}


Comme le montre le schéma, l'API est exécutée dans un conteneur afin de garantir un environnement totalement autonome et isolé. Seules les dépendances nécessaires à l'exécution du modèle et au fonctionnement de l'API ne sont intégrées à ce conteneur. Travailler avec des *images docker* légères présentent plusieurs avantages. Tous d'abord, créer une image ne contenant que le strict nécessaire au fonctionnement de votre application permet justement de savoir ce qui est strictement nécessaire et ce qui est superflu. Aussi, plus votre image est légère plus le temps de téléchargement depuis votre Hub d'images (e.g. *Dockerhub*) sera rapide à chaque création de conteneur de votre application. Les conteneurs ont l'avantage d'être totalement portable et offre la possibilité de mettre à l'échelle votre application de manière simple et efficace. Par exemple, si l'on imagine que vous avez déployé votre modèle et que vous souhaitez le requêter un grand nombre de fois dans un laps de temps court, alors il est préférable de créer plusieurs instances de votre application pour que les calculs puissent être effectués en parallèle. L'avantage des conteneurs est qu'une fois qu'on a créé l'image il est très simple de lancer une multitude de répliques de conteneur toutes basées sur l'image en question.


Il faut savoir que MLflow permet également de déployer directement un modèle MLflow. Cependant, nous avons décider de ne pas passer cette étape sous le tapis et de la détailler

- Pourquoi exposer un modèle via une API Rest / FastAPI

On a un code rangé dans app/

fichiers deployment/service/ingress





- Déployer sur Kubernetes
<!-- j'ai vu que dans les slides vous parlez de kubernetes vous pensez faut mettre un paragraphe dessus ? j'y connais R moi -->

- Batch vs real-time prediction 


## 3️⃣ Observabilité en temps réel d'un modèle de ML

- Différence monitoring informaticien vs monitoring data-scientist business unit
- Data Drifts / concept drift
- Quelles métriques monitorer
- Systeme d'alerting 
- dashboard


## 4️⃣ Réentraînement d'un modèle ML

- Amélioration continue Réentraînement : périodique vs continu
- Feedback loops expérimentation/déploiement/monitoring

## 5️⃣ Défis organisationnels du MLOps

Pas ouf : 

Dans la plupart des organisations, les équipes data transverses ou intégrées dans différents départements métier sont relativement jeunes et manquent de ressources qualifiées pour gérer le déploiement et le maintien en condition opérationnelle de systèmes ML complexes. En effet, ces équipes se composent généralement principalement de data scientists qui se concentrent sur le développement des modèles de machine learning, mais n’ont pas les compétences nécessaires pour gérer le déploiement et la maintenance d’applications complètes. De plus, les équipes data évoluent encore trop souvent en silo, sans communiquer avec les différentes équipes techniques avec lesquelles elles devraient interagir pour mettre en production leurs modèles.














Batch learning and online learning are two different types of machine learning techniques that are used to train models on data.

Batch learning, also known as offline learning, involves training a model on a fixed dataset, or a batch of data, all at once. The model is trained on the entire dataset, and then used to make predictions on new data. This means that batch learning requires a complete dataset before training can begin, and the model cannot be updated once it has been trained without retraining the entire model. Batch learning is commonly used in situations where the dataset is relatively small and can be processed quickly.

On the other hand, online learning, also known as incremental learning or streaming learning, involves training a model on new data as it arrives, one observation at a time. The model is updated each time a new observation is received, allowing it to adapt to changes in the data over time. Online learning is commonly used in situations where the data is too large to be processed all at once, or where the data is constantly changing, such as in stock market data or social media data.

The key difference between batch learning and online learning is that batch learning requires a fixed dataset, while online learning can adapt to new data as it arrives. Batch learning is typically faster and requires less computational resources than online learning, but may not be as flexible in handling changing or large datasets. Online learning, on the other hand, can be more flexible and adaptable, but may require more resources and be slower to process data. Both techniques have their advantages and disadvantages, and the choice between them depends on the specific problem being solved and the characteristics of the data.