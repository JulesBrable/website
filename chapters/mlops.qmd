---
title: "Vers le MLOps"
author: "Romain Avouac et Lino Galiana"
description: |
  <br>
  Présentation de l'approche MLOps.
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/mise-en-prod/army.png
order: 6
href: chapters/mlops.html
---


Grâce aux chapitres précédents on sait comment structurer et rendre son projet de de data science reproductible et portable grâce à l'approche DevOps. Pour de nombreux projets de data science cela peut s'avérer suffisant. Cependant, dès lors que l'on intègre un modèle de machine learning à notre projet l'approche DevOps devient alors une condition nécessaire mais plus suffisante pour réussir à mettre en production notre modèle. Aujourd'hui, une très grande majorité des modèles de machine learning ne dépassent pas le stade de l'expérimentation et très peu parviennent à atteindre le stade de la production. Pour simplifier ce passage vers la production une nouvelle approche a émergé en intégrant les spécificités des modèles de machine learning : le MLOps. On peut voir le MLOps comme la somme de 3 buzzwords à savoir :

MLOps = DataOps + DevOps + ModelOps

Ce chapitre définie cette approche et propose présente plusieurs outils pour l'adopter à ces projets de data science.



# Du DevOps vers le MLOps

## L'approche DevOps


L'approche DevOps, contraction des termes "Development" et "Operations", est une méthodologie de développement logiciel qui vise à améliorer la collaboration et la communication entre les équipes de développement (Dev) et l'administration système (Ops). Son objectif principal est d'accélérer et optimiser le cycle de vie du développement logiciel, de la conception à la production, en favorisant l'automatisation, l'intégration continue et le déploiement continu (cf. chapitre sur la mise en production).

Cette approche, datant de 2007, vise à éliminer les barrières traditionnelles entre le développement et l'informatique, favorisant ainsi une culture de responsabilité partagée. Les principes fondamentaux de DevOps incluent l'**automatisation** des processus, la **surveillance** continue des performances, la **gestion agile** des changements, et la création d'une **boucle de rétroaction** rapide pour améliorer en permanence les processus de développement et d'exploitation.

En résumé, DevOps vise à créer un écosystème de développement collaboratif, automatisé et axé sur l'amélioration continue. Le cycle de vie DevOps est souvent résumé par ces 8 phases inter-connectées : 

![](/devops.png){fig-align="center"}

## L'approche MLOps 

L'approche MLOps s'est construite sur les bases de l'approche DevOps. En cela on peut considérer qu'il s'agit simplement d'une extension à l'approche DevOps qui s'est développée pour répondre aux défis spécifiques liés à la gestion du cycle de vie des modèles de machine learning. Le MLOps intègre les principes de collaboration et d'automatisation propre au DevOps mais prend également en compte tous les aspects liés aux données et aux modèles de machine learning. 

Le MLOps implique l'**automatisation des tâches** telles que la gestion des données, le suivi des **versions des modèles**, leurs **déploiements**, ainsi que l'**évaluation continue** de la performance des modèles en production. De la même manière que le DevOps, le MLOps met l'accent sur la collaboration étroite entre les équipes de développement et de l'administration système d'une part ainsi que les équipes de data science d'autre pas. Cette collaboration est clé pour garantir une communication efficace tout au long du cycle de vie du modèle de machine learning.

Ainsi on définit généralement les 5 piliers du MLOps comme étant :

  - **la reproductibilité** : chaque expérimentation doit pouvoir être reproduite sans coût (gestion des packages, gestion des environnements, gestion des librairies système, gestion de version du code ...)
  - **la collaboration** : les différentes parties prenantes au projet doivent communiquer afin d'optimiser le cycle de vie du modèle en production
  - **l'automatisation** : le cycle de vie du modèle doit être automatisé au maximum avec des *pipeline* de CI/CD
  - **le contrôle de version des modèles** : chaque modèle doit avoir une version spécifique permettant de connaître sa situation tout au long de son cycle de vie (expérimental, en production, archivé...). De plus, un modèle ne dépend pas seulement des scripts associés, mais également des données utilisées lors de l'entraînement ce qui rend d'autant plus nécessaire une bonne gestion des versions
  - **la surveillance** : une fois déployé, il est important de s'assurer que le modèle fonctionne bien comme attendu en évaluant ses performances en temps réel

![](/mlops.png){fig-align="center"}

# Implémentation de l'approche MLOps avec MLflow

## Pourquoi MLflow ?

Il existe aujourd'hui de nombreux outils pour orchestrer des tâches et des pipelines de données. Parmi les plus populaires (selon leur ⭐ Github) on peut citer [Airflow](https://github.com/apache/airflow), [Luigi](https://github.com/spotify/luigi), [MLFlow](https://github.com/mlflow/mlflow), [Argo Workflow](https://github.com/argoproj/argo-workflows), [Prefect](https://github.com/PrefectHQ/prefect) ou encore [Kubeflow](https://github.com/kubeflow/kubeflow). Il est difficile d'affirmer s'il y en a un meilleur qu'un autre, en réalité votre choix dépend surtout de votre infrastructure informatique et de votre projet. En l’occurrence ici, nous avons fait le choix d'utiliser MLflow pour la simplicité d'utilisation mais aussi car il nous semble être le plus spécialisé pour les projet de machine learning. De plus, il est présent dans le catalogue du SSP Cloud ce qui simplifie grandement son installation. Nous utiliserons également [Argo CD](https://github.com/argoproj/argo-cd) et [Argo Workflow](https://github.com/argoproj/argo-workflows) plutôt qu'[Airflow](https://github.com/apache/airflow) car ils sont optimisés pour les clusters kubernetes (comme le SSP Cloud !).

MLflow est une plateforme qui permet d'optimiser le développement du cycle de vie d'un modèle de machine learning. Il permet de suivre en détails les différentes expérimentations, de *packager* son code pour garantir la reproductibilité, et de servir un modèle à des utilisateurs. MLFlow possède aussi une API qui permet d'être compatible avec la majorité des librairies de machine learning (PyTorch, Scikit learn, XGBoost, etc) mais également différents languages (python, R et Java).

## Les projets MLflow

MLflow propose un format pour *packager* son projet de data science afin de favoriser la réutilisation et la reproductibilité du code. Ce format s'appelle tout simplement *[MLflow Project](https://mlflow.org/docs/latest/projects.html)*. Concrètement un *MLflow project* n'est rien d'autre qu'un répertoire contenant le code et les ressources nécessaires (données, fichiers de configuration...) nécessaire à l'exécution de votre projet. Il est résumé par un ficher `MLproject` qui résume les différentes commandes pour exécuter une pipeline ainsi que les dépendances nécessaires. De manière général un projet MLflow a la structure suivante : 

```
Projet_ML/
├── artifacts/
│   ├── model.bin
│   └── train_text.txt
├── code/
│   ├── main.py
│   └── preprocessing.py
├── MLmodel
├── conda.yaml
├── python_env.yaml
├── python_model.pkl
└── requirements.txt
```


## Les modèles MLflow

En plus de *packager* son projet, MLflow permet de également de *packager* son modèle **quelque que soit** la librairie de machine learning sous-jacente utilisée (parmi celles [compatibles avec MLflow](https://mlflow.org/docs/latest/models.html#built-in-model-flavors), aka toutes les librairies que vous utilisez !). Ainsi, deux modèles entraînés avec des librairies différentes, disons PyTorch et Keras, peuvent être ainsi déployé et requêté de la même manière grâce à cette surcouche rajoutée par MLflow.

![](/mlflow-models.png){fig-align="center"}

::: {.callout-tip}
## Tip

Il est également possible de *packager* son propre modèle personnalisé ! Pour cela vous pouvez suivre le tutoriel présent dans la [documentation](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html?highlight=custom%20models).
:::

## Le serveur de suivi (*tracking server*)

MLflow fourni un serveur de suivi qui contient à la fois une interface graphique ergonomique ainsi qu'une API permettant de *logger* différents paramètres, métriques, fichiers, etc durant l’entraînement de votre modèle de machine learning.
Le *tracking server* est très utile pour comparer les différentes expérimentations que vous avez effectué, pour les stocker et également pour être capable de les reproduire. En effet, chaque *run* sauvegarde la source des données utilisées mais également le commit sur lequel le *run* est basé. 

![](/tracking-server.png){fig-align="center"}


## L'entrepôt de modèles (*model registry*)

Une fois que l'on a effectué différentes expérimentations et pu sélectionner les modèles qui nous satisfont il est temps de passer à l'étape suivante du cycle de vie d'un modèle. En effet, le modèle choisi doit pouvoir ensuite passer dans un environnement de production ou de pré-production. Or, connaître l'état d'un modèle dans son cycle de vie nécessite une très bonne organisation et n'est pas si aisé. MLflow a développé une fonctionnalité qui permet justement de simplifier cette gestion des versions des modèles grâce à son [Model Registry](https://mlflow.org/docs/latest/model-registry.html). Cet entrepôt permet d'ajouter des tags et des alias à nos modèles pour définir leur position dans leur cycle de vie et ainsi pouvoir les récupérer de manière efficace. 

De manière générale un modèle de machine learning passe par 4 stades qu'il est nécessaire de pouvoir connaître en tout temps :

1. **Expérimental**
2. **En évaluation**
3. **En production**
4. **Archivé**

![](/mlflow-model-registry.png){fig-align="center"}

## MLflow en résumé

MLflow est donc un projet open-source qui fournit une plateforme pour suivre le cycle de vie d'une modèle de machine learning de bout en bout. Ce n'est pas le seul outil disponible et il n'est peut être pas le plus adapté à certains de vos projets précis. En revanche, il présente selon nous plusieurs avantages en premier desquels sa très simple prise en main et sa faculté à répondre aux besoins de l'approche MLOps. Il faut garder en tête que cet environnement est encore très récent et de nouveaux projets open-source émergent chaque jour, il est nécessaire de faire une veille sur les dernières évolutions. 

Pour résumer, MLFlow permet :

- de simplifier le suivi de l’entraînement des modèles de machine learning grâce à son API et son *tracking server*
- d'intégrer les principaux framework de machine learning de manière simple
- d'intégrer son propre framework si besoin
- de standardiser son script d'entraînement et donc pouvoir l'industrialiser, pour réaliser un *fine-tuning* des hyper-paramètres par exemple
- de *packager* ses modèles, de sorte à pouvoir les requêter de manière simple et harmonisée entre les différents frameworks
- stocker ses modèles de manières pertinentes en leur affectant des tags et favorisant le suivi de leur cycle de vie

# Spécificités liées à la mise en production de modèles de ML

## 1️⃣ Entraînements des modèles


La première étape d'un projet de machine learning corresponds à tout ce que l'on effectue jusqu'à l'entraînement des premiers modèles. Cette étape est un - fastidieux - processus itératif qui ne suit pas un développement linéaire : les méthodes de récupération des données peuvent être changeantes, le preprocessing peut varier de même que la selection des features pour le modèle (*feature engineering*), les algorithmes testés peuvent être nombreux... Garder une trace de tous les essais effectués apparaît indispensable afin de savoir ce qui a fonctionné ou non. Cette étape barbante est rendue très simple grâce au *Tracking Server* de MLFlow. 
Lors de l'exécution d'un *run* MLFlow enregistre tout un tas de métadonnées qui permet de retrouver toutes les informations relatives à ce *run* : la date, le hash du commit, les paramètres du modèles, le dataset utilisé, les métriques spécifiées etc. Cela permet, non seulement de comparer les différents essais réalisés mais aussi d'être capable de reproduire une *run* passé.

De manière générale cette phase exploratoire est réalisée par le data-scientist dans des notebooks. Ces notebooks sont effet parfaitement adaptés pour cette étape puisqu'ils permettent une grande flexibilité et sont particulièrement commode pour effectuer des tests. En revanche, lorsque l'on souhaite aller plus loin et que l'on vise une mise en production de son projet, les notebooks ne sont plus adaptés et cela pour diverses raisons : 

- la collaboration est grandement limitée à cause d'une compatibilité très faible avec les outils de contrôle de version standard (git).
- l'automatisation de pipeline est beaucoup plus compliqué et peu lisible. Il existe cependant des packages qui permettent d'automatiser des *pipelines* de notebooks commpe [Elyra](https://github.com/elyra-ai/elyra) par exemple, mais ce n'est clairement pas l'approche que nous vous recommandons.
- Les *workflows* sont souvent moins clairs et peu reproductibles avec souvent une organisation peu optimale (toutes les fonctions définies dans le même fichiers affectant la lisibilité du code par exemple)
- les notebooks offrent généralement une modularité insuffisante lorsque l'on veut travailler avec des composants de machine learning complexes

Toutes ces raisons nous amènent à vous conseiller de réduire au maximum votre utilisation de notebook et de restreindre leur utilisation à la phase exploratoire ou à la diffusion de résultats/rapports. Passer le plus tôt possible à des script `.py` vous permettra de réduire le coût de la mise en production. Pour reprendre ce qui a déjà été évoqué dans le chapitre [Architecture des projets](/chapters/projects-architecture.qmd), nous vous invitons à favoriser une structure modulaire de sorte à pouvoir industrialiser votre projet.

Une autre spécificité pouvant impacter la mise en production concerne la manière dont l'entraînement est réalisé. Il existe pour cela 2 écoles qui ont chacune leurs avantages et désavantages : le *batch training* et l'*online training*. 

### Batch training

Le *batch training* est la manière usuelle d'entraîner un modèle de machine learning. Cette méthode consiste à entraîner son modèle sur un jeu de données fixe d'une seule traite. Le modèle est entraîné sur l'intégralité des données disponibles et les prédictions sont réalisées sur de nouvelles données. Cela signifie que le modèle n'est pas mis à jour une fois entraîné et qu'il est nécessaire de le ré-entraîner si l'on souhaite mettre à jour les poids du modèle. Cette méthode est particulièrement simple à mettre en place puisqu'il suffit d'entraîner le modèle une fois, le déployer puis le ré-entraîner ultérieurement. Cette simplicité a forcément une contrepartie : votre modèle est statique et il est nécessaire de le ré-entraîner fréquemment pour prendre en compte les nouvelles données. Un exemple fréquemment utilisé est la détection de spam, si un nouveau type de spam apparaît votre modèle entraîné en batch ne sera pas en capacité de le détecter à moins de le ré-entraîner en entier. De plus, cette méthode peut rapidement nécessiter d'avoir en mémoire une grande quantité de données en fonction de la taille de votre jeu de données ce qui peut être contraignant en fonction de votre infrastructure mais également allonger sensiblement le temps de l'entraînement. 

### Online training

L'*online training* est un peu l'opposé du *batch training* puisqu'il est réalisé de manière incrémentale. L'entraînement consiste à envoyer de petit lot de données séquentiellement afin que l'algorithme mette à jour ses poids à chaque nouveau lot de données. Cela permet alors au modèle de bien repérer les différents changements dans les nouvelles données. Attention cependant à la spécification du *learning rate* pour éviter que le modèle oublie tout ce qu'il a appris sur les anciennes données. L'un des grand avantages de cette méthode est que l'entraînement continue alors même que le modèle est en production puisque celui-ci est computationnellement moins coûteux. Elle est également particulièrement adapté dans les cas où les données en entrée varient fréquemment (i.e. prédiction d'un cours de bourse). L'*online training* est cependant beaucoup plus compliqué à mettre en oeuvre dans un contexte de production et les frameworks classiques de machine learning ne sont pas compatible (Scikit learn, PyTorch, TensorFlow, Keras...).

### Distributed training

ind

- Mettre à l'échelle l’entraînement grâce à des orchestrateurs

## 2️⃣ Servir un modèle ML à des utilisateur

- Questions essentielles ? Comment rendre dispo son modèles ?  
Il faut savoir que MLflow permet également de déployer directement un modèle MLflow. Cependant, nous avons décider de ne pas passer cette étape sous le tapis et de la détailler

- Pourquoi exposer un modèle via une API Rest / FastAPI

- Déployer sur Kubernetes

- Batch vs real-time prediction 


## 3️⃣ Observabilité en temps réel d'un modèle de ML

- Différence monitoring informaticien vs monitoring data-scientist business unit
- Data Drifts / concept drift
- Quelles métriques monitorer
- Systeme d'alerting 
- dashboard


## 4️⃣ Réentraînement d'un modèle ML

- Amélioration continue Réentraînement : périodique vs continu
- Feedback loops expérimentation/déploiement/monitoring

## 5️⃣ Défis organisationnels du MLOps

Pas ouf : 

Dans la plupart des organisations, les équipes data transverses ou intégrées dans différents départements métier sont relativement jeunes et manquent de ressources qualifiées pour gérer le déploiement et le maintien en condition opérationnelle de systèmes ML complexes. En effet, ces équipes se composent généralement principalement de data scientists qui se concentrent sur le développement des modèles de machine learning, mais n’ont pas les compétences nécessaires pour gérer le déploiement et la maintenance d’applications complètes. De plus, les équipes data évoluent encore trop souvent en silo, sans communiquer avec les différentes équipes techniques avec lesquelles elles devraient interagir pour mettre en production leurs modèles.














Batch learning and online learning are two different types of machine learning techniques that are used to train models on data.

Batch learning, also known as offline learning, involves training a model on a fixed dataset, or a batch of data, all at once. The model is trained on the entire dataset, and then used to make predictions on new data. This means that batch learning requires a complete dataset before training can begin, and the model cannot be updated once it has been trained without retraining the entire model. Batch learning is commonly used in situations where the dataset is relatively small and can be processed quickly.

On the other hand, online learning, also known as incremental learning or streaming learning, involves training a model on new data as it arrives, one observation at a time. The model is updated each time a new observation is received, allowing it to adapt to changes in the data over time. Online learning is commonly used in situations where the data is too large to be processed all at once, or where the data is constantly changing, such as in stock market data or social media data.

The key difference between batch learning and online learning is that batch learning requires a fixed dataset, while online learning can adapt to new data as it arrives. Batch learning is typically faster and requires less computational resources than online learning, but may not be as flexible in handling changing or large datasets. Online learning, on the other hand, can be more flexible and adaptable, but may require more resources and be slower to process data. Both techniques have their advantages and disadvantages, and the choice between them depends on the specific problem being solved and the characteristics of the data.