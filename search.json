[
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "",
    "text": "L’objectif de cette mise en application est d’illustrer les différentes étapes qui séparent la phase de développement d’un projet de celle de la mise en production. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.\nNous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d’un projet de data science. On a un notebook un peu monolithique, qui réalise les étapes classiques d’un pipeline de machine learning : - import de données - statistiques descriptives et visualisations - feature engineering - entraînement d’un modèle - évaluation du modèle\nL’objectif est d’améliorer le projet de manière incrémentale jusqu’à pouvoir le mettre en production, en le valorisant sous une forme adaptée.\n{{% box status=“warning” title=“Warning” icon=“fa fa-exclamation-triangle” %}} Il est important de bien lire les consignes et d’y aller progressivement. Certaines étapes peuvent être rapides, d’autres plus fastidieuses ; certaines être assez guidées, d’autres vous laisser plus de liberté. Si vous n’effectuez pas une étape, vous risquez de ne pas pouvoir passer à l’étape suivante qui en dépend.\nBien que l’exercice soit applicable sur toute configuration bien faite, nous recommandons de privilégier l’utilisation du SSP Cloud, où tous les outils nécessaires sont pré-installés et pré-configurés. {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#etape-0-forker-le-dépôt-dexemple-et-créer-une-branche-de-travail",
    "href": "chapters/application.html#etape-0-forker-le-dépôt-dexemple-et-créer-une-branche-de-travail",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 0: forker le dépôt d’exemple et créer une branche de travail",
    "text": "Etape 0: forker le dépôt d’exemple et créer une branche de travail\n\nOuvrir un service vscode sur le SSP Cloud. Vous pouvez aller dans la page My Services et cliquer sur New service. Sinon, vous pouvez lancer le service en cliquant directement ici.\nGénérer un jeton d’accès (token) sur GitHub afin de permettre l’authentification en ligne de commande à votre compte. La procédure est décrite ici. Garder le jeton généré de côté.\nForker le dépôt Github  https://github.com/avouacr/ensae-reproductibilite-projet\nClôner votre dépôt Github  en utilisant le terminal depuis Visual Studio (Terminal > New Terminal) :\n\n$ git clone https://<TOKEN>@github.com/avouacr/ensae-reproductibilite-projet.git\noù <TOKEN> est à remplacer par le jeton que vous avez généré précédemment.\n\nSe placer avec le terminal dans le dossier en question :\n\n$ cd ensae-reproductibilite-projet\n\nCréez une branche nettoyage :\n\n$ git checkout -b nettoyage\nSwitched to a new branch 'nettoyage'"
  },
  {
    "objectID": "chapters/application.html#etape-1-sassurer-que-le-notebook-sexécute-correctement",
    "href": "chapters/application.html#etape-1-sassurer-que-le-notebook-sexécute-correctement",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 1 : s’assurer que le notebook s’exécute correctement",
    "text": "Etape 1 : s’assurer que le notebook s’exécute correctement\nLa première étape est simple, mais souvent oubliée : vérifier que le code fonctionne correctement.\n\nOuvrir dans VSCode le notebook titanic.ipynb, et choisir comme kernel basesspcloud\nExécuter le notebook en entier pour vérifier s’il fonctionne\nCorriger l’erreur qui empêche la bonne exécution.\n\nIl est maintenant temps de commit les changements effectués avec Git :\n$ git add titanic.ipynb\n$ git commit -m \"Corrige l'erreur qui empêchait l'exécution\"\n$ git push\nEssayez de commit vos changements à chaque étape de l’exercice, c’est une bonne habitude à prendre."
  },
  {
    "objectID": "chapters/application.html#etape-2-modularisation---mise-en-fonctions-et-mise-en-module",
    "href": "chapters/application.html#etape-2-modularisation---mise-en-fonctions-et-mise-en-module",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 2 : Modularisation - mise en fonctions et mise en module",
    "text": "Etape 2 : Modularisation - mise en fonctions et mise en module\nNous allons mettre en fonctions les parties importantes de l’analyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook. En reformattant le code présent dans le notebook :\n\ncréer une fonction qui importe les données d’entraînement (train.csv) et de test (test.csv) et renvoie des DataFrames pandas\ncréer une (ou plusieurs) fonction(s) pour réaliser les étapes de feature engineering\ncréer une fonction qui réalise le split train/test de validation\ncréer une fonction qui entraîne et évalue un classifieur RandomForest, et qui prend en paramètre le nombre d’arbres (n_estimators). La fonction doit imprimer à la fin la performance obtenue et la matrice de confusion.\nmettre ces fonctions dans un module functions.py\nimporter les fonctions via le module dans le notebook et vérifier que l’on retrouve bien les différents résultats en utilisant les fonctions.\n\n{{% box status=“warning” title=“Warning” icon=“fa fa-exclamation-triangle” %}} Attention à bien spécifier les dépendances (packages à importer) dans le module pour que les fonctions puissent faire leur travail indépendamment du notebook ! {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#etape-3-utiliser-un-main-script",
    "href": "chapters/application.html#etape-3-utiliser-un-main-script",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 3 : utiliser un main script",
    "text": "Etape 3 : utiliser un main script\nFini le temps de l’expérimentation : on va maintenant essayer de se passer complètement du notebook. Pour cela, on va utiliser un main script, c’est à dire un script qui reproduit l’analyse en important et en exécutant les différentes fonctions dans l’ordre attendu.\n\ncréer un script main.py (convention de nommage pour les main scripts en Python)\nimporter les fonctions nécessaires à partir du module functions.py. Ne pas faire d’ import *, ce n’est pas une bonne pratique ! Appeler les fonctions une par une en les séparant par des virgules\nprogrammer leur exécution dans l’ordre attendu dans le script\nvérifier que tout fonctionne bien en exécutant le main script à partir de l’exécutable Python :\n\n$ python main.py\nSi tout a correctement fonctionné, la performance du RandomForest et la matrice de confusion devraient s’afficher dans la console."
  },
  {
    "objectID": "chapters/application.html#etape-4-appliquer-les-standards-de-qualité-de-code",
    "href": "chapters/application.html#etape-4-appliquer-les-standards-de-qualité-de-code",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 4 : appliquer les standards de qualité de code",
    "text": "Etape 4 : appliquer les standards de qualité de code\nOn va maintenant améliorer la qualité de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le linter classique PyLint.\nPour appliquer le linter à un script .py, la syntaxe à entrer dans le terminal est la suivante :\n$ pylint mon_script.py\nLe linter renvoie alors une série d’irrégularités, en précisant à chaque fois la ligne de l’erreur et le message d’erreur associé (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualité du code à l’aune des standards communautaires (PEP8 et PEP257).\n\nappliquer une première fois le linter, respectivement aux scripts functions.py et main.py. Noter les notes obtenues.\nà partir des codes d’erreur, modifier le code pour résoudre les différents problèmes un par un\nviser une note minimale de 9/10 pour main.py et 6/10 pour functions.py.\n\n{{% box status=“tip” title=“Note” icon=“fa fa-hint” %}} N’hésitez pas à taper un code d’erreur sur un moteur de recherche pour obtenir plus d’informations si jamais le message n’est pas clair ! {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#etape-5-adopter-une-architecture-standardisée-de-projet",
    "href": "chapters/application.html#etape-5-adopter-une-architecture-standardisée-de-projet",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 5 : adopter une architecture standardisée de projet",
    "text": "Etape 5 : adopter une architecture standardisée de projet\nOn va maintenant modifier l’architecture de notre projet pour la rendre plus standardisée. Pour cela, on va utiliser le package cookiecutter qui génère des templates de projet. En particulier, on va choisir le template datascience développé par la communauté pour s’inspirer de sa structure.\n{{% box status=“tip” title=“Note” icon=“fa fa-hint” %}} L’idée de cookiecutter est de proposer des templates que l’on utilise pour initialiser un projet, afin de bâtir à l’avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante :\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\nIci, on a déjà un projet, on va donc faire les choses dans l’autre sens : on va s’inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires. {{% /box %}}\n\nanalyser et comprendre la structure de projet proposée par le template\ncréer les dossiers qui vous semblent pertinents pour contenir les différents éléments de notre projet selon le modèle\nvous aller devoir séparer le module functions.py en différents modules afin de pouvoir entrer dans la structure suggérée dans le dossier src (le dossier destiné à contenir le code source de votre package)\nvous devriez arriver à une structure semblable à celle-ci :\n\nensae-reproductibilite-projet\n├── data\n│   └── raw\n│       ├── test.csv\n│       └── train.csv\n├── main.py\n├── notebooks\n│   └── titanic.ipynb\n├── README.md\n└── src\n    ├── data\n    │   ├── import_data.py\n    │   └── train_test_split.py\n    ├── features\n    │   └── build_features.py\n    └── models\n        └── train_evaluate.py\n{{% box status=“tip” title=“Note” icon=“fa fa-hint” %}} Il est normal d’avoir des dossiers __pycache__ qui traînent : ils se créent automatiquement à l’exécution d’un script en Python. On verra comment les supprimer définitivement à l’étape 8. {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#conda-export",
    "href": "chapters/application.html#conda-export",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 6 : fixer l’environnement d’exécution",
    "text": "Etape 6 : fixer l’environnement d’exécution\nAfin de favoriser la portabilité du projet, il est d’usage de “fixer l’environnement”, c’est à dire d’indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version. Il est conventionnellement localisé à la racine du projet.\nSur le VSCode du SSP Cloud, on se situe dans un environnement conda. La commande pour exporter un environnement conda est la suivante :\n$ conda env export > environment.yml\nVous devriez à présent avoir un fichier environement.yml à la racine de votre projet, qui contient les dépendances et leurs versions.\n{{% box status=“tip” title=“Note” icon=“fa fa-hint” %}} En réalité, on aun peu triché : on a exporté l’environnement de base du VSCode SSP Cloud, qui contient beaucoup plus de packages que ceux utilisés par notre projet. On verra dans la Partie 2 de l’application comment fixer proprement les dépendances de notre projet. {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 7 : stocker les données de manière externe",
    "text": "Etape 7 : stocker les données de manière externe\n{{% box status=“warning” title=“Warning” icon=“fa fa-exclamation-triangle” %}} Cette étape n’est pas facile. Vous devrez suivre la documentation du SSP Cloud pour la réaliser. Une aide-mémoire est également disponible dans le cours de Python pour les data-scientists {{% /box %}}\nComme on l’a vu dans le cours, les données ne sont pas censées être versionnées sur un projet Git. L’idéal pour éviter cela tout en maintenant la reproductibilité est d’utiliser une solution de stockage externe. On va utiliser pour cela MinIO, la solution de stockage de type S3 offerte par le SSP Cloud.\n\ncréer un dossier ensae-reproductibilite dans votre bucket personnel via l’interface utilisateur\nmodifier votre fonction d’import des données pour qu’elle récupère les données à partir de MinIO. Elle devra prendre en paramètres le nom du bucket et le dossier dans lequel sont contenues les données sur MinIO.\nmodifier le main script pour appeler la fonction avec les paramètres propres à votre compte\nsupprimer les fichiers .csv du dossier data de votre projet, on n’en a plus besoin vu qu’on les importe de l’extérieur\nvérifier le bon fonctionnement de votre application"
  },
  {
    "objectID": "chapters/application.html#etape-8-nettoyer-le-dépôt-git",
    "href": "chapters/application.html#etape-8-nettoyer-le-dépôt-git",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 8 : nettoyer le dépôt Git",
    "text": "Etape 8 : nettoyer le dépôt Git\nDes dossiers parasites __pycache__ se sont glissés dans notre projet. Ils se créent automatiquement à l’exécution d’un script en Python, afin de rendre plus rapide les exécutions ultérieures. Ils n’ont cependant pas de raison d’être versionnés, vu que ce sont des fichiers locaux (spécifiques à un environnement d’exécution donné).\n\nsupprimer les différents dossiers __pycache__ du projet\najouter le fichier .gitignore adapté à Python à la racine du projet\najouter le dossier data/ au .gitignore pour éviter tout ajout involontaire de données au dépôt Git\n\n{{% box status=“tip” title=“Note” icon=“fa fa-hint” %}} En pratique, mieux vaut adopter l’habitude de toujours mettre un .gitignore, pertinent selon le langage du projet, dès le début du projet. GitHub offre cette option à l’initialisation d’un projet. Le site gitignore.io propose des modèles selon le langage que vous utilisez qui peuvent être utiles. {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#etape-9-ouvrir-une-pull-request-sur-le-dépôt-du-projet",
    "href": "chapters/application.html#etape-9-ouvrir-une-pull-request-sur-le-dépôt-du-projet",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 9 : ouvrir une pull request sur le dépôt du projet",
    "text": "Etape 9 : ouvrir une pull request sur le dépôt du projet\nEnfin terminé ! Enfin presque… On s’est donné beaucoup de mal à nettoyer ce dépôt et le mettre aux standards, autant valoriser ce travail. On va pour cela faire une pull request sur le dépôt du projet initial, c’est à dire proposer à l’auteur d’intégrer tous les changements que vous avez effectué en committant à chaque étape.\nSuivre la procédure décrite dans la documentation GitHub pour créer une pull request à partir de votre fork. Pour la branche upstream (le dépôt cible), on va choisir master. Par contre, pour la branche locale (celle sur votre dépôt), on va choisir la branche nettoyage.\nSi tout s’est bien passé, vous devriez à présent voir votre pull request sur le dépôt cible (ici). Bravo, vous venez de faire votre première contribution à l’open source !\n{{% box status=“warning” title=“Warning” icon=“fa fa-exclamation-triangle” %}} Faire une pull request via la branche master d’un fork est très mal vu. En effet, il faut souvent faire des contorsionnements pour réussir à faire coïncider deux histoires qui n’ont pas de raison de coïncider. On s’évite beaucoup de problèmes en prenant l’habitude de toujours faire ses pull requests à partir d’une autre branche que master. {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#configyaml",
    "href": "chapters/application.html#configyaml",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 1: créer un répertoire de variables servant d’input",
    "text": "Etape 1: créer un répertoire de variables servant d’input\n\nEnjeu\nLors de l’étape 7, nous avons amélioré la qualité du script en séparant stockage et code. Cependant, peut-être avez-vous remarqué que nous avons introduit un nom de bucket personnel dans le script (voir le fichier main.py). Il s’agit typiquement du genre de petit vice caché d’un script qui peut générer une erreur: vous n’avez pas accès au bucket en question donc si vous essayez de faire tourner ce script en l’état, vous allez rencontrer une erreur.\nUne bonne pratique pour gérer ce type de configuration est d’utiliser un fichier YAML qui stocke de manière hiérarchisée les variables globales 1.\nEn l’occurrence, nous n’avons besoin que de deux éléments pour pouvoir dé-personnaliser ce script :\n\nle nom du bucket\nl’emplacement dans le bucket\n\n\n\nApplication\nDans VSCode, créer un fichier nommé config.yaml et le localiser à la racine de votre dépôt. Voici, une proposition de hiérarchisation de l’information que vous devez adapter à votre nom d’utilisateur :\ninput:\n  bucket: \"lgaliana\"\n  path: \"ensae-reproductibilite\"\nDans main.py, importer ce fichier et remplacer la ligne précédemment évoquée par les valeurs du fichier. Tester en faisant tourner main.py"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 2 : créer un environnement conda à partir du fichier environment.yml",
    "text": "Etape 2 : créer un environnement conda à partir du fichier environment.yml\nL’environnement conda créé avec conda env export (étape 6) contient énormément de dépendances, dont de nombreuses qui ne nous sont pas nécessaires (il en serait de même avec pip freeze). Nous n’avons en effet besoin que des packages présents dans la section import de nos scripts et les dépendances nécessaires pour que ces packages soient fonctionnels.\nVous allez chercher à obtenir un environment.yml beaucoup plus parcimonieux que celui généré par conda env export\n\n{{% panel name=“Approche générale :koala:” %}}\nLe tableau récapitulatif présent dans la partie portabilité peut être utile dans cette partie. L’idée est de partir from scratch et figer l’environnement qui permet d’avoir une appli fonctionnelle.\n\nCréer un environnement vide avec Python 3.10 \nActiver cet environnement\nInstaller en ligne de commande avec pip les packages nécessaires pour faire tourner votre code\n\n\n\nFaire un pip freeze > requirements.txt ou conda env export > environment.yml (privilégier la deuxième option)\nRetirer la section prefix (si elle est présente) et changer la section name en monenv\n\n{{% /panel %}}\n{{% panel name=“Approche fainéante :sloth:” %}}\nNous allons générer une version plus minimaliste grâce à l’utilitaire pipreqs\n\nInstaller pipreqs en pip install\nEn ligne de commande, depuis la racine du projet, faire pipreqs\nOuvrir le requirements.txt automatiquement généré. Il est beaucoup plus minimal que celui que vous obtiendriez avec pip freeze ou l’environment.yml obtenu à l’étape 6.\nRemplacer toute la section dependencies du environment.yml par le contenu du requirements.txt (:warning: ne pas oublier l’indentation et le tiret en début de ligne)\n:warning: Modifier le tiret à scikit learn. Il ne faut pas un underscore mais un tiret\nAjouter la version de python (par exemple python=3.10.0) au début de la section dependencies\nRetirer la section prefix du fichier environment.yml (si elle est présente) et changer le contenu de la section name en monenv\nCréer l’environnement (voir le tableau récapitulatif dans la partie portabilité)\n\n\n{{% /panel %}}\n{{% /panelset %}}\nMaintenant, il reste à tester si tout fonctionne bien dans notre environnement plus minimaliste:\n\nActiver l’environnement\nTester votre script en ligne de commande\nFaire un commit quand vous êtes contents"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 3: conteneuriser avec Docker ",
    "text": "Etape 3: conteneuriser avec Docker \n\nPréliminaire\n\nSe rendre sur l’environnement bac à sable Play with Docker\nDans le terminal Linux, cloner votre dépôt Github \nCréer via la ligne de commande un fichier Dockerfile. Il y a plusieurs manières de procéder, en voici un exemple:\n\necho \"#Dockerfile pour reproduire mon super travail\" > Dockerfile\n\nOuvrir ce fichier via l’éditeur proposé par l’environnement bac à sable.\n\n\n\nCréation d’un premier Dockerfile\n\n:one: Comme couche de départ, partir d’une image légère comme ubuntu:20.04\n:two: Dans une deuxième couche, faire un apt get -y update et installer wget qui va être nécessaire pour télécharger Miniconda depuis la ligne de commande\n:three: Dans la troisième couche, nous allons installer Miniconda :\n\nTélécharger la dernière version de Miniconda avec wget depuis l’url de téléchargement direct https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nInstaller Miniconda dans le chemin /home/coder/local/bin/conda\nEffacer le fichier d’installation pour libérer de la place sur l’image\n\n:four: En quatrième couche, on va installer mamba pour accélérer l’installation des packages dans notre environnement.\n:five: En cinquième couche, nous allons créer l’environnement conda:\n\nUtiliser COPY pour que Docker soit en mesure d’utiliser le fichier environment.yml (sinon Docker renverra une erreur)\nCréer l’environnement vide monenv (présentant uniquement Python 3.10) avec la commande conda adéquate\nMettre à jour l’environnement en utilisant environment.yml avec mamba\n\n:six: Utiliser ENV pour ajouter l’environnement monenv au PATH et utiliser le fix suivant:\n\nRUN echo \"export PATH=$PATH\" >> /home/coder/.bashrc  # Temporary fix while PATH gets overwritten by code-server\n\n:seven: Exposer sur le port 5000\n:eight: En dernière étape, utiliser CMD pour reproduire le comportement de python main.py\n\n{{% box status=“hint” title=“Hint: mamba” icon=“fa fa-lightbulb” %}} mamba est une alternative à conda pour installer des packages dans un environnement Miniconda/Anaconda. mamba n’est pas obligatoire, conda peut suffire. Cependant, mamba est beaucoup plus rapide que conda pour installer des packages à installer ; il s’agit donc d’un utilitaire très pratique. {{% /box %}}\n\n{{% panel name=“Indications supplémentaires” %}}\nCliquer sur les onglets ci-dessus :point_up_2: pour bénéficier d’indications supplémentaires, pour vous aider. Cependant, essayez de ne pas les consulter immédiatement: n’hésitez pas à tâtonner.\n{{% /panel %}}\n{{% panel name=“Installation de Miniconda” %}}\n# INSTALL MINICONDA -------------------------------\nARG CONDA_DIR=/home/coder/local/bin/conda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nRUN bash Miniconda3-latest-Linux-x86_64.sh -b -p $CONDA_DIR\nRUN rm -f Miniconda3-latest-Linux-x86_64.sh\n{{% /panel %}}\n{{% panel name=“Installation de mamba” %}}\nENV PATH=\"/home/coder/local/bin/conda/bin:${PATH}\"\nRUN conda install mamba -n base -c conda-forge\n{{% /panel %}}\n{{% panel name=“Création de l’environnement” %}}\nCOPY environment.yml .\nRUN conda create -n monenv python=3.10\nRUN mamba env update -n monenv -f environment.yml\n{{% /panel %}}\n\n\n\nConstruire l’image\nMaintenant, nous avons défini notre recette. Il nous reste à faire notre plat et à le goûter\n\nUtiliser docker build pour créer une image avec le tag my-python-app\nVérifier les images dont vous disposez. Vous devriez avoir un résultat proche de celui-ci\n\n\nREPOSITORY      TAG       IMAGE ID       CREATED         SIZE\nmy-python-app   latest    c0dfa42d8520   6 minutes ago   2.23GB\nubuntu          20.04     825d55fb6340   6 days ago      72.8MB\n\n\nTester l’image: découverte du cache\nIl ne reste plus qu’à goûter la recette et voir si le plat est bon.\nUtiliser docker run avec l’option it pour pouvoir appeler l’image depuis son tag\n\n:warning: :bomb: :fire: Docker ne sait pas où trouver le fichier main.py. D’ailleurs, il ne connait pas d’autres fichiers de notre application qui sont nécessaires pour faire tourner le code: config.yaml et le dossier src\n\nAvant l’étape EXPOSE utiliser plusieurs ADD et/ou COPY pour que l’application dispose de tous les éléments minimaux pour être en mesure de fonctionner\nRefaire tourner docker run \n\n{{% box status=“tip” title=“Note” icon=“fa fa-hint” %}} Ici, le cache permet d’économiser beaucoup de temps. Par besoin de refaire tourner toutes les étapes, Docker agit de manière intelligente en faisant tourner uniquement les nouvelles étapes. {{% /box %}}\n\n\nCorriger une faille de reproductibilité\nVous devriez rencontrer une erreur liée à la variable d’environnement AWS_ENDPOINT_URL. C’est normal, elle est inconnue de cet environnement minimaliste. D’ailleurs, Docker n’a aucune raison de connaître votre espace de stockage sur le S3 du SSP-Cloud si vous ne lui dites pas. Donc cet environnement ne sait pas comment accéder aux fichiers présents dans votre minio.\nVous allez régler ce problème avec les étapes suivantes, :\n\n:one: Naviguer dans l’interface du SSP-Cloud pour retrouver les liens d’accès direct de vos fichiers\n:two: Dans VSCode, les mettre dans config.yaml (faire de nouvelles clés)\n:three: Dans VSCode, modifier la fonction d’import pour s’adapter à ce changement.\n:four: Faire un commit et pusher les fichiers\n:five: Dans l’environnement bac à sable, faire un pull pour récupérer ces modifications\n:six: Tester à nouveau le build (là encore le cache est bien pratique !)\n\n\n:tada: A ce stade, la matrice de confusion doit fonctionner. Vous avez créé votre première application reproductible !"
  },
  {
    "objectID": "chapters/application.html#etape-préliminaire",
    "href": "chapters/application.html#etape-préliminaire",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape préliminaire",
    "text": "Etape préliminaire\nPour ne pas risquer de tout casser sur notre branche master, nous allons nous placer sur une branche nommée dev:\n\nsi dans l’étape suivante vous appliquez la méthode la plus simple, vous allez pouvoir la créer depuis l’interface de Github ;\nsi vous utilisez l’autre méthode, vous allez devoir la créer en local ( via la commande git checkout -b dev)"
  },
  {
    "objectID": "chapters/application.html#etape-1-mise-en-place-de-tests-automatisés",
    "href": "chapters/application.html#etape-1-mise-en-place-de-tests-automatisés",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 1: mise en place de tests automatisés",
    "text": "Etape 1: mise en place de tests automatisés\nAvant d’essayer de mettre en oeuvre la création de notre image Docker de manière automatisée, nous allons présenter la logique de l’intégration continue en généralisant les évaluations de qualité du code avec le linter\n\n{{% panel name=“Utilisation d’un template Github :cat:” %}}\nMethode la plus simple: utilisation d’un template Github\nSi vous cliquez sur l’onglet Actions de votre dépôt, Github vous propose des workflows standardisés reliés à Python. Choisir l’option Python Package using Anaconda.\nwarning: Nous n’allons modifier que deux éléments de ce fichier.\n:one: La dernière étape (Test with pytest) ne nous est pas nécessaire car nous n’avons pas de tests unitaires Nous allons donc remplacer celle-ci par l’utilisation de pylint pour avoir une note de qualité du package.\n\nUtiliser pylint à cette étape pour noter les scripts ;\nVous pouvez fixer un score minimal à 5 (option --fail-under=5)\n\n:two: Mettre entre guillements la version de Python pour que celle-ci soit reconnue.\n:three: Enfin, finaliser la création de ce script:\n\nEn cliquant sur le bouton Start Commit, choisir la méthode Create a new branch for this commit and start a pull request en nommant la branche dev\nCréer la Pull Request en lui donnant un nom signifiant\n\n{{% /panel %}}\n{{% panel name=“Méthode manuelle” %}}\n:warning: On est plutôt sur une méthode de galérien. Il vaut mieux privilégier l’autre approche\nOn va éditer depuis VisualStudio nos fichiers.\n\nCréer une branche dev en ligne de commande\nCréer un dossier .github/workflows via la ligne de commande ou l’explorateur de fichier \nCréer un fichier .github/workflows/quality.yml.\n\nNous allons construire, par étape, une version simplifiée du Dockerfile présent dans ce post et dans celui-ci\n:one: D’abord, définissons des paramètres pour indiquer à Github quand faire tourner notre script:\n\nCommencez par nommer votre workflow par exemple Python Linting avec la clé name\nNous allons faire tourner ce workflow dans la branche master et dans la branche actuelle (dev). Ici, nous laissons de côté les autres éléments (par exemple le fait de faire tourner à chaque pull request). La clé on est dédiée à cet usage\n\n:two: Ensuite, défnissons le contexte d’exécution des tâches (jobs) de notre script dans les options de la partie build:\n\nUtilisons une machine ubuntu-latest. Nous verrons plus tard comment améliorer cela.\n\n:three: Nous allons ensuite mélanger des étapes pré-définies (des actions du marketplace) et des instructions que nous faisons :\n\nLe runner Github doit récupérer le contenu de notre dépôt, pour cela utiliser l’action checkout. Par rapport à l’exemple, il convient d’ajouter, pour le moment, un paramètre ref avec le nom de la branche (par exemple dev)\nOn installe ensuite Python avec l’action setup-python Pas besoin d’installer Python, on va utiliser l’option conda-incubator/setup-miniconda@v2\nPour installer Python et l’environnement conda, on va plutôt utiliser l’astuce de ce blog avec l’option conda-incubator/setup-miniconda@v2\nOn utilise ensuite flake8 et pylint (option --fail-under=5) pour effectuer des diagnostics de qualité\n\nIl ne reste plus qu’à faire un commit et espérer que cela fonctionne. Cela devrait donner le fichier suivant :\nname: Python Linting\non:\n  push:\n    branches: [master, dev]\njobs:\n  build:\n    runs-on: ubuntu-latest    \n    steps:\n      - uses: actions/checkout@v3\n        with:\n          ref: \"dev\"\n      - uses: conda-incubator/setup-miniconda@v2\n        with:\n          activate-environment: monenv\n          environment-file: environment.yml\n          python-version: '3.10'\n          auto-activate-base: false\n      - shell: bash -l {0}\n        run: |\n          conda info\n          conda list\n      - name: Lint with flake8\n        run: |\n          pip install flake8\n          flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics\n          flake8 src --count --max-complexity=10 --max-line-length=79 --statistics\n      - name: Lint with Pylint\n        run: |\n          pip install pylint\n          pylint src\n{{% /panel %}}\n\nMaintenant, nous pouvons observer que l’onglet Actions s’est enrichi. Chaque commit va entraîner une action pour tester nos scripts.\nSi la note est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi détecter, en développant son projet, les moments où on dégrade la qualité du script afin de la rétablir immédiatemment.\n{{% box status=“hint” title=“Un linter sous forme de hook pre-commit” icon=“fa fa-lightbulb” %}}\nGit offre une fonctionalité intéressante lorsqu’on est puriste: les hooks. Il s’agit de règles qui doivent être satisfaites pour que le fichier puisse être committé. Cela assurera que chaque commit remplisse des critères de qualité afin d’éviter le problème de la procrastination.\nLa documentation de pylint offre des explications supplémentaires.\n{{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#etape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#etape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 2: Automatisation de la livraison de l’image Docker",
    "text": "Etape 2: Automatisation de la livraison de l’image Docker\nMaintenant, nous allons automatiser la mise à disposition de notre image sur DockerHub. Cela facilitera sa réutilisation mais aussi des valorisations ultérieures.\nLà encore, nous allons utiliser une série d’actions pré-configurées.\n:one: Pour que Github puisse s’authentifier auprès de DockerHub, il va falloir d’abord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace sécurisé associé à votre dépôt Github. Cette démarche sera là même ultérieurement lorsque nous connecterons notre dépôt à un autre service tiers, à savoir Netlify:\n\nSe rendre sur https://hub.docker.com/ et créer un compte.\nAller dans les paramètres (https://hub.docker.com/settings/general) et cliquer, à gauche, sur Security\nCréer un jeton personnel d’accès, ne fermez pas l’onglet en question, vous ne pouvez voir sa valeur qu’une fois.\nDans votre dépôt Github, cliquer sur l’onglet Settings et cliquer, à gauche, sur Actions. Sur la page qui s’affiche, cliquer sur New repository secret\nDonner le nom DOCKERHUB_TOKEN à ce jeton et copier la valeur. Valider\nCréer un deuxième secret nommé DOCKERHUB_USERNAME ayant comme valeur le nom d’utilisateur que vous avez créé sur Dockerhub\n\n:two: A ce stade, nous avons donné les moyens à Github de s’authentifier avec notre identité sur Dockerhub. Il nous reste à mettre en oeuvre l’action en s’inspirant de https://github.com/docker/build-push-action/#usage. On ne va modifier que trois éléments dans ce fichier. Effectuer les actions suivantes:\n\nCréer depuis VSCode un fichier .github/workflows/docker.yml et coller le contenu du template dedans ;\nChanger le nom en un titre plus signifiant (par exemple “Production de l’image Docker”)\nAjouter master et dev à la liste des branches sur lesquelles tourne le pipeline ;\nChanger le tag à la fin pour mettre <username>/ensae-repro-docker:latest où username est le nom d’utilisateur sur DockerHub;\nFaire un commit et un push de ces fichiers\n\n:four: Comme on est fier de notre travail, on va afficher ça avec un badge sur le README. Pour cela, on se rend dans l’onglet Actions et on clique sur un des scripts en train de tourner.\n\nEn haut à droite, on clique sur ...\nSélectionner Create status badge\nRécupérer le code Markdown proposé\nCopier dans le README depuis VSCode\nFaire de même pour l’autre workflow\n\n:five: Maintenant, il nous reste à tester notre application dans l’espace bac à sable:\n\nSe rendre sur l’environnement bac à sable\nCréer un fichier Dockerfile ne contenant que l’import et le déploiement de l’appli:\n\nFROM <username>/ensae-repro-docker:latest\n\nEXPOSE 5000\nCMD [\"python\", \"main.py\"]\n\nComme précédemment, faire un build\nTester l’image avec run\n\n:tada: La matrice de confusion doit s’afficher ! Vous avez grandement facilité la réutilisation de votre image."
  },
  {
    "objectID": "chapters/application.html#etape-3-création-dun-rapport-automatique",
    "href": "chapters/application.html#etape-3-création-dun-rapport-automatique",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 3: création d’un rapport automatique",
    "text": "Etape 3: création d’un rapport automatique\nMaintenant, nous allons créer et déployer un site web pour valoriser notre travail. Cela va impliquer trois étapes:\n\nTester en local le logiciel quarto et créer un rapport minimal qui sera compilé par quarto ;\nEnrichir l’image docker avec le logiciel quarto ;\nCompiler le document en utilisant cette image sur les serveurs de Github ;\nDéployer ce rapport minimal pour le rendre disponible à tous sur le web.\n\nLe but est de proposer un rapport minimal qui illustre la performance du modèle est la feature importance. Pour ce dernier élément, le rapport qui sera proposé utilise shap qui est une librairie dédiée à l’interprétabilité des modèles de machine learning\n\n1. Rapport minimal en local\n:one: La première étape consiste à installer quarto sur notre machine Linux sur laquelle tourne VSCode:\n\nDans un terminal, installer quarto avec les commandes suivantes:\n\nQUARTO_VERSION=\"0.9.287\"\nwget \"https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb\"\nsudo apt install \"./quarto-${QUARTO_VERSION}-linux-amd64.deb\"\n\nS’assurer qu’on travaille bien depuis l’environnement conda monenv. Sinon l’activer\n\n:two: Il va être nécessaire d’enrichir l’environnement conda. Certaines dépendances sont nécessaires pour que quarto fonctionne bien avec Python (jupyter, nbclient…) alors que d’autres ne sont nécessaires que parce qu’ils sont utilisés dans le document (seaborn, shap…). Changer la section dependencies avec la liste suivante:\ndependencies:\n  - python=3.10.0\n  - ipykernel==6.13.0\n  - jupyter==1.0.0\n  - matplotlib==3.5.1\n  - nbconvert==6.5.0\n  - nbclient==0.6.0\n  - nbformat==5.3.0\n  - pandas==1.4.1\n  - PyYAML==6.0\n  - s3fs==2022.2.0\n  - scikit-learn==1.0.2\n  - seaborn==0.11.2\n  - shap==0.40.0\n:three: Créer un fichier nommé report.qmd\n\n\n---\ntitle: \"Comprendre les facteurs de survie sur le Titanic\"\nsubtitle: \"Un rapport innovant\"\nformat:\n  html:\n    self-contained: true\n  ipynb: default\njupyter: python3\n---\n\n\n\nVoici un rapport présentant quelques intuitions issues d'un modèle \n_random forest_ sur le jeu de données `Titanic` entraîné et \ndéployé de manière automatique. \n\nIl est possible de télécharger cette page sous format `Jupyter Notebook` <a href=\"report.ipynb\" download>ici</a>\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nimport main\nX_train = main.X_train\ny_train = main.y_train\ntraining_data = main.training_data\nrdmf = RandomForestClassifier(n_estimators=20)\nrdmf.fit(X_train, y_train)\n```\n\n# Feature importance\n\nLa @fig-feature-importance représente l'importance des variables :\n\n```python\nfeature_imp = pd.Series(rdmf.feature_importances_, index=training_data.iloc[:,1:].columns).sort_values(ascending=False)\n```\n\n```python\n#| label: fig-feature-importance\n#| fig-cap: \"Feature importance\"\nplt.figure(figsize=(10,6))\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.tight_layout()\nplt.show()\n```\n\nCelle-ci peut également être obtenue grâce à la librairie\n`shap`:\n\n```python\n#| echo : true\nimport shap\nshap_values = shap.TreeExplainer(rdmf).shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\", feature_names = training_data.iloc[:,1:].columns)\n```\n\nOn peut également utiliser cette librairie pour\ninterpréter la prédiction de notre modèle:\n\n\n```python\n# explain all the predictions in the test set\nexplainer = shap.TreeExplainer(rdmf)\n# Calculate Shap values\nchoosen_instance = main.X_test[15]\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, feature_names = training_data.iloc[:,1:].columns)\n```\n\n# Qualité prédictive du modèle\n\nLa matrice de confusion est présentée sur la\n@fig-confusion\n\n```python\n#| label: fig-confusion\n#| fig-cap: \"Matrice de confusion\"\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(main.y_test, rdmf.predict(main.X_test))\nplt.figure(figsize=(8,5))\nsns.heatmap(conf_matrix, annot=True)\nplt.title('Confusion Matrix')\nplt.tight_layout()\n```\n\nOu, sous forme de tableau:\n\n\n```python\npd.DataFrame(conf_matrix, columns=['Predicted','Observed'], index = ['Predicted','Observed']).to_html()\n```\n:four: On va tenter de compiler ce document\n\nLe compiler en local avec la commande quarto render report.qmd\nVous devriez rencontrer l’erreur suivante:\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [1], in <cell line: 6>()\n      4 from sklearn.ensemble import RandomForestClassifier\n      5 import main\n----> 6 X_train = main.X_train\n      7 y_train = main.y_train\n      8 training_data = main.training_data\n\nAttributeError: module 'main' has no attribute 'X_train'\nAttributeError: module 'main' has no attribute 'X_train'\n\nRefactoriser main.py pour que toutes les opérations, à l’exception du print de la matrice de confusion ne soient plus dans la section __main__ afin qu’ils soient systématiquement exécutés.\nTenter à nouveau quarto render report.qmd\nDeux fichiers ont été générés:\n\nun Notebook que vous pouvez ouvrir et dont vous pouvez exécuter des cellules\nun fichier HTML que vous pouvez télécharger et ouvrir\n\n\n:five: On a déjà un résultat assez esthétique en ce qui concerne la page HTML. Cependant, on peut se dire que certains paramètres par défaut, comme l’affichage des blocs de code, ne conviennent pas au public ciblé. De même, certains paramètres de style, comme l’affichage des tableaux peuvent ne pas convenir à notre charte graphique. On va remédier à cela en deux étapes:\n\nenrichir le header d’options globales contrôlant le comportement de quarto\ncréer un fichier CSS pour avoir de beaux tableaux\n\n:six: Changer la section format du header avec les options suivantes:\nformat:\n  html:\n    echo: false\n    code-fold: true\n    self-contained: true\n    code-summary: \"Show the code\"\n    warning: false\n    message: false\n    theme:\n      - cosmo\n      - css/custom.scss\n  ipynb: default\n:seven: Créer le fichier css/custom.scss avec le contenu suivant:\n/*-- scss:rules --*/\n\ntable {\n    border-collapse: collapse;\n    margin: 25px 0;\n    font-size: 0.9em;\n    font-family: sans-serif;\n    min-width: 400px;\n    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);  \n}\n\nthead tr {\n    background-color: #516db0;\n    color: #ffffff;\n    text-align: center;\n}\n\nth, td {\n    padding: 12px 15px;\n}\n\ntbody tr {\n    border-bottom: 1px solid #dddddd;\n}\n\ntbody tr:nth-of-type(even) {\n    background-color: #f3f3f3;\n}\n\ntbody tr:last-of-type {\n    border-bottom: 2px solid #516db0;\n}\n\ntbody tr.active-row {\n    font-weight: bold;\n    color: #009879;\n}\n:eight: Compiler à nouveau et observer le changement d’esthétique du HTML\n:nine: Commit des nouveaux fichier report.qmd, custom.scss et des fichiers déjà existants.\n{{% box status=“hint” title=“Un linter sous forme de hook pre-commit” icon=“fa fa-lightbulb” %}}\nOn ne commit pas les output, ici le notebook et le fichier html. Les mettre sur le dépôt Github n’est pas la bonne manière de les mettre à disposition. On va le voir, on va utiliser l’approche CI/CD pour cela.\nIdéalement, on ajoute au .gitignore les fichiers concernés, ici report.ipynb et report.html\n{{% /box %}}\n\n\n3. Enrichir l’image Docker\nOn va vouloir mettre à jour notre image pour automatiser, à terme, la production de nos livrables (le notebook et la page web).\nPour cela, il est nécessaire que notre image intègre le logiciel quarto.\n:one: A partir du script précédent d’installation de quarto, enrichir l’image Docker2\n\n\n\n4. Automatisation avec Github Actions\n:one: Créer un nouveau fichier .github/workflows.report.yml\nSi les dépendances et l’image ont bien été enrichis, cette étape est quasi directe avec\n\n{{% panel name=“Version autonome :car:” %}}\n\nDonner comme nom Deploy as website\nEffectuer cette action à chaque push sur les branches main, master et dev\nLe job doit tourner sur une machine ubuntu\nCependant, il convient d’utiliser comme container votre image Docker\nLes steps:\n\nRécupérer le contenu du dossier avec checkout\nFaire un quarto render\nRécupérer le notebook sous forme d’artefact\n\n\n{{% /panel %}}\n{{% panel name=“Version guidée :map:” %}}\nname: Deploy as website\n\non:\n  push:\n    branches:\n      - main\n      - master\n      - dev\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    container: linogaliana/ensae-repro-docker:latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Render site\n        run: quarto render report.qmd\n      - uses: actions/upload-artifact@v1\n        with:\n          name: Report\n          path: report.ipynb\n{{% /panel %}}\n\nSi vous êtes fier de vous, vous pouvez ajouter le badge de ce workflow sur le README :sunglasses:\nCette étape nous a permis d’automatiser la construction de nos livrables. Mais la mise à disposition de ce livrable est encore assez manuelle: il faut aller chercher à la main la dernière version du notebook pour la partager.\nOn va améliorer cela en déployant automatiquement un site web présentant en page d’accueil notre rapport et permettant le téléchargement du notebook."
  },
  {
    "objectID": "chapters/application.html#etape-4-déploiement-de-ce-rapport-automatique-sur-le-web",
    "href": "chapters/application.html#etape-4-déploiement-de-ce-rapport-automatique-sur-le-web",
    "title": "Appliquer les concepts étudiés à un projet de data science",
    "section": "Etape 4: Déploiement de ce rapport automatique sur le web",
    "text": "Etape 4: Déploiement de ce rapport automatique sur le web\n:one: Dans un premier temps, nous allons connecter notre dépôt Github au service tiers Netlify\n\nAller sur https://www.netlify.com/ et faire Sign up (utiliser son compte Github)\nDans la page d’accueil de votre profil, vous pouvez cliquer sur Add new site > Import an existing project\nCliquer sur Github. S’il y a des autorisations à donner, les accorder. Rechercher votre projet dans la liste de vos projets Github\nCliquer sur le nom du projet et laisser les paramètres par défaut (nous allons modifier par la suite)\nCliquer sur Deploy site\n\n:two: A ce stade, votre déploiement devrait échouer. C’est normal, vous essayez de déployer depuis master qui ne comporte pas de html. Mais le rapport n’est pas non plus présent dans la branche dev. En fait, aucune branche ne comporte le rapport: celui-ci est généré dans votre pipeline mais n’est jamais présent dans le dépôt car il s’agit d’un output. On va désactiver le déploiement automatique pour privilégier un déploiement depuis Github Actions:\n\nAller dans Site Settings puis, à gauche, cliquer sur Build and Deploy\nDans la section Build settings, cliquer sur Stop builds et valider\n\nOn vient de désactiver le déploiement automatique par défaut. On va faire communiquer notre dépôt Github et Netlify par le biais de l’intégration continue.\n:three: Pour cela, il faut créer un jeton Netlify pour que les serveurs de Github, lorsqu’ils disposent d’un rapport, puissent l’envoyer à Netlify pour la mise sur le web. Il va être nécessaire de créer deux variables d’environnement pour connecter Github et Netlify: l’identifiant du site et le token\n\nPour le token :\n\nCréer un jeton en cliquant, en haut à droite, sur l’icone de votre profil. Aller dans User settings. A gauche, cliquer sur Applications et créer un jeton personnel d’accès avec un nom signifiant (par exemple PAT_ENSAE_reproductibilite)\nMettre de côté (conseil : garder l’onglet ouvert)\n\nPour l’identifiant du site:\n\ncliquer sur Site Settings dans les onglets en haut\nGarder l’onglet ouvert pour copier la valeur quand nécessaire\n\nIl est maintenant nécessaire d’aller dans le dépôt Github et de créer les secrets (Settings > Secrets > Actions):\n\nCréer le secret NETLIFY_AUTH_TOKEN en collant la valeur du jeton d’authentification Netlify\nCréer le secret NETLIFY_SITE_ID en collant l’identifiant du site\n\n\n:four: Nous avons effectué toutes les configurations nécessaires. On va maintenant mettre à jour l’intégration continue afin de mettre à disposition sur le web notre rapport. On va utiliser l’interface en ligne de commande (CLI) de Netlify. Celle-ci attend que le site web se trouve dans un dossier public et que la page d’accueil soit nommée index.html:\n\n{{% panel name=“Vision d’ensemble” %}}\n\nune installation de npm\nune étape de déploiement via la CLI de netlify\n\n- name: Install npm\n  uses: actions/setup-node@v2\n  with:\n    node-version: '14'\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n{{% /panel %}}\n{{% panel name=“Détails npm” %}}\n\n\nname: Install npm uses: actions/setup-node@v2 with: node-version: ‘14’\n\n\nnpm est le gestionnaire de paquet de JS. Il est nécessaire de le configurer, ce qui est fait automatiquement grâce à l’action actions/setup-node@v2\n{{% /panel %}}\n{{% panel name=“Détails Netlify CLI” %}}\n\nOn rappelle à Github Actions nos paramètres d’authentification sous forme de variables d’environnement. Cela permet de les garder secrètes\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo’s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install –unsafe-perm=true netlify-cli -g netlify init netlify deploy –prod –dir=“public” –message “Deploy master”\n\n\n\nOn déplace les rapports de la racine vers le dossier public\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo’s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install –unsafe-perm=true netlify-cli -g netlify init netlify deploy –prod –dir=“public” –message “Deploy master”\n\n\n\nOn installe et initialise Netlify\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo’s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install –unsafe-perm=true netlify-cli -g netlify init netlify deploy –prod –dir=“public” –message “Deploy master”\n\n\n\nOn déploie sur l’url par défaut (-- prod) depuis le dossier public\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo’s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install –unsafe-perm=true netlify-cli -g netlify init netlify deploy –prod –dir=“public” –message “Deploy master”\n\n\n{{% /panel %}}\n\nAu bout de quelques minutes, le rapport est disponible en ligne sur l’URL Netlify (par exemple https://spiffy-florentine-c913b9.netlify.app)"
  },
  {
    "objectID": "chapters/code-quality.html",
    "href": "chapters/code-quality.html",
    "title": "Améliorer la qualité de son code",
    "section": "",
    "text": "The code is read much more often than it is written.\n\nGuido Van Rossum\n\n\nLorsque l’on s’initie à la pratique de la data science, il est assez naturel de voir le code d’une manière très fonctionnelle : je veux réaliser une tâche donnée — par exemple un algorithme de classification — et je vais donc assembler dans un notebook des bouts de code, souvent trouvés sur internet, jusqu’à obtenir un projet qui réalise la tâche voulue. La structure du projet importe assez peu, tant qu’elle permet d’importer correctement les données nécessaires à la tâche en question. Si cette approche flexible et minimaliste fonctionne très bien lors de la phase d’apprentissage, il est malgré tout indispensable de s’en détacher à progressivement à mesure que l’on progresse et que l’on peut être amené à réaliser des projets plus professionnels ou bien à intégrer des projets collaboratifs.\nEn particulier, il est important de proposer, parmi les multiples manières de résoudre un problème informatique, une solution qui soit intelligible par d’autres personnes parlant le langage. Le code est en effet lu bien plus souvent qu’il n’est écrit, c’est donc avant tout un outil de communication. De même, la maintenance d’un code demande généralement beaucoup plus de moyens que sa phase de développement initial, il est donc important de penser en amont la qualité de son code et la structure de son projet de sorte à le rendre au maximum maintenable dans le temps. Afin de faciliter ces réflexions, des tentatives plus ou moins institutionnalisées de définir des conventions ont émergé. Ces conventions dépendent naturellement du langage utilisé, mais les principes sous-jacents s’appliquent de manière universelle à tout projet basé sur du code.\n\n\n\nPython est un langage très lisible. Avec un peu d’effort sur le nom des objets, sur la gestion des dépendances et sur la structure du programme, on peut très bien comprendre un script sans avoir besoin de l’exécuter. C’est l’une des principales forces du langage Python qui permet ainsi une acquisition rapide des bases. R, dans sa version de base, est un langage un peu plus verbeux que Python. Cependant, les packages les plus utiles pour l’analyse de données (notamment dplyr ou data.table) offrent une grammaire un peu plus transparente.\nLa communauté Python a abouti à un certain nombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard dans l’écosystème Python. Les deux normes les plus connues sont la norme PEP8 (code) et la norme PEP257 (documentation).\nDans l’univers R, la formalisation a été moins organisée. Ce langage est plus permissif que Python sur certains aspects1. Néanmoins, des standards ont émergé, à travers un certain nombre de style guides dont les plus connus sont le tidyverse style guide et le google style guide2 (voir ce post qui pointe vers un certain nombre de ressources sur le sujet).\nCes conventions ne sont pas immuables: les langages et leurs usages évoluent, ce qui nécessite de mettre à jour les conventions. Cependant, adopter dans la mesure du possible certains des réflexes préconisés par ces conventions devrait améliorer la capacité à être compris par la communauté, augmenter les chances de bénéficier d’apport de celle-ci pour adapter le code mais aussi réduire la difficulté à faire évoluer un code. Il existe beaucoup de philosophies différentes sur le style de codage et, en fait, le plus important est la cohérence : si on choisit une convention, par exemple snake case plutôt que camel case, le mieux est de s’y tenir.\nLes conventions vont au-delà de la syntaxe. Un certain nombre de standards d’organisation d’un projet ont émergé. La structuration d’un projet permet d’immédiatement identifier les éléments de code et les éléments annexes (par exemple les dépendances à gérer, la documentation, etc.). Un certain nombre d’assistants au développement de projets orientés données (des packages d’helpers, des extensions aux environnements de développement comme VisualStudio ou RStudio…) ont émergé pour gagner en productivité et faciliter le lancement d’un projet (voir ce post très complet sur les extensions VisualStudio). L’idée générale est de privilégier une structure de projet bien plus fiable qu’une suite sans structure de scripts ou un notebook jupyter (voir ce post de blog sur ce sujet).\n\n\n\nLes éléments exposés dans ce chapitre ne sont pas exhaustifs. Ils visent à pointer vers quelques problématiques prioritaires tout en proposant des conseils pratiques. Ils sont complémentaires du guide des bonnes pratiques utilitR qui vise à présenter de manière plus formelle quelques recommendations.\nDans la lignée de la vision des bonnes pratiques comme continuum proposée en introduction(), il n’est pas nécessairement souhaitable d’appliquer toutes les recommendations présentées dans ce chapitre à chaque projet. Nous recommandons de les voir plutôt comme des bonnes habitudes à acquérir en opérant un va-et-vient régulier entre la pratique et la théorie. Par exemple, à la lecture de ce chapitre, vous allez certainement retenir en particulier certaines règles qui tranchent avec vos pratiques actuelles. Vous pouvez alors essayer d’appliquer ces nouvelles règles pendant un certain temps puis, lorsque celles-ci seront devenues naturelles, revenir à ce guide et appliquer le processus à nouveau. En procédant ainsi de manière incrémentale, vous améliorerez progressivement la qualité de vos projets sans avoir l’impression de passer trop de temps sur des micro-détails, au détriment des objectifs globaux du projet."
  },
  {
    "objectID": "chapters/code-quality.html#principes-généraux",
    "href": "chapters/code-quality.html#principes-généraux",
    "title": "Améliorer la qualité de son code",
    "section": "Principes généraux",
    "text": "Principes généraux\nLes premières conventions à évoquer ont trait à la syntaxe du code et ont les objectifs suivants, qui seront détaillés par la suite :\n\nAméliorer la lisibilité ce qui est indispensable pour rendre la démarche intelligible par d’autres mais aussi pour soi, lorsqu’on reprend un code écrit il y a quelques temps ;\nFavoriser la concision pour réduire le risque d’erreur et rendre la démarche plus claire ;\nSuivre les règles explicites ou les conventions d’un langage pour assurer le fonctionnement et la cohérence d’un code ;\nLimiter la redondance ce qui permet de simplifier un code (paradigme du don’t repeat yourself) ;\nDocumenter un code ce qui facilite son acquisition par d’autres (à condition de ne pas aller dans l’excès de documentation).\n\n\nLisibilité\nUn code écrit avec des noms de variables et de fonctions explicites est autant, voire plus, informatif que les commentaires qui l’accompagnent. C’est pourquoi il est essentiel de respecter des conventions pour le choix des noms des objets afin d’assurer la lisibilité des programmes.\nUn certain nombre de conseils sont présents dans le Hitchhiker’s Guide to Python qui vise à faire connaître les préceptes du “Zen of Python” (PEP 20). Ce post de blog illustre quelques uns de ces principes avec des exemples. Vous pouvez retrouver ces conseils dans Python en tapant le code suivant:\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\nVoici quelques conseils complémentaires.\n\nAdopter les mêmes standards que la communauté pour les noms de package\n\n# bien\nimport numpy as np\n\n# trompeur\nimport numpy as pd\n\nFaire attention aux namespaces pour éviter les conflits entre fonctions. Cela implique de ne pas importer l’ensemble des fonctions d’un package de la manière suivante:\n\nfrom numpy import *\nfrom math import *\nDans ce cas, on va se retrouver avec des conflits potentiels entre les fonctions du package numpy et du package math qui portent le même nom (floor par exemple).\nEn complément de ces premières recommandations, il est conseillé de suivre ces deux principes lorsqu’on commence à programmer des fonctions (ce qui, comme cela est évoqué plus bas, est toujours recommandé).\n\nFaire attention au type d’objet renvoyé par Python ou R. Ces deux langages ne proposent pas de typage fort, il est donc possible qu’une fonction renvoie des objets de nature différente en fonction, par exemple, de conditions if (selon les cas une liste, un vecteur, un dataframe, etc.). Cela peut amener à des surprises lorsqu’on utilise une telle fonction dans un code. Il est recommandé d’éviter ce comportement en proposant des fonctions différentes si l’output d’une fonction est de nature différente. Ce principe de précaution (mais aussi d’information) renvoie au paradigme de la programmation défensive.\nPrivilégier la programmation orientée objet lorsqu’une fonction doit s’adapter au type d’objet en entrée (par exemple aller chercher des éléments différents pour un objet lm ou un objet glm). Cela évite les codes spaghetti :spaghetti: inutilement complexes qui sont impossibles à débugger.\n\n{{% box status=“hint” title=“Hint” icon=“fa fa-lightbulb” %}}\nPython propose une fonctionalité assez plaisante qui est le type hinting (doc officielle et tutoriel sur realpython.com). Celle-ci permet d’indiquer le type d’argument attendu par une fonction et celui qui sera renvoyé par la fonction. Par exemple, la personne ayant écrit la fonction suivante\ndef calcul_moyenne(df: pd.DataFrame, col : str = \"y\") -> pd.DataFrame:\n    return df[col].mean()\npropose d’utiliser deux types d’inputs (un DataFrame pandas et une chaine de caractère) et indique qu’elle renverra un DataFrame pandas. A noter que c’est indicatif, non contraignant. En effet, le code ci-dessus fonctionnera si on fournit en argument col une liste puisque pandas sait gérer cela à l’étape df[col].mean()\n{{% /box %}}\n\n{{% box status=“note” title=“Note: le code spaghetti :spaghetti:” icon=“fa fa-comment” %}} Le code spaghetti est un style d’écriture qui favorise l’apparition du syndrome du plat de spaghettis : un code impossible à déméler parce qu’il fait un usage excessif de conditions, d’exceptions en tous sens, de gestion des événements complexes. Il devient quasi-impossible de savoir quelles ont été les conditions à l’origine de telle ou telle erreur sans exécuter ligne à ligne (et celles-ci sont excessivement nombreuses du fait de mauvaises pratiques de programmation) le programme.\nEn fait, la programmation spaghetti qualifie tout ce qui ne permet pas de déterminer le qui, le quoi et le comment. Le code est donc plus long à mettre à jour car cela nécessite de remonter un à un le fil des renvois.\n{{% /box %}}\n\n\nConcision\nComme une démonstration mathématique, un code intelligible doit viser la concision et la simplicité. Les codes très longs sont souvent signes de répétitions et sont difficiles à débugger.\n\nPrivilégier les list comprehensions lorsque cela est possible:\n\nliste_nombres = range(10)\n\n# très mauvais\ny = []\nfor x in liste_nombres:\n  y.append(x*x)\n\n# mieux\ny = [x*x for x in liste_nombres]\n\nPrivilégier les appels à des fonctions à des blocs copier-coller en changeant un seul détail.\n\n\n\nCohérence interne\nLister les dépendances est important, tant pour des raisons techniques (que le logiciel sache où aller chercher des fonctions nécessaires pour avoir un code fonctionnel) que pour des raisons conventionnelles (que les utilisateurs comprennent les dépendances à s’installer pour être en mesure de réutiliser le code).\nPour cette raison, il est de bonne pratique de lister les dépendances de deux manières.\n:one: En début de script, l’ensemble des fonctions issues de librairies externes ou les packages à importer doivent être listés ;\nLes imports se mettent conventionnellement en début de script, qu’il s’agisse d’import de packages dans leur ensemble ou seulement de certaines fonctions:\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nDans le premier cas, on fait ensuite référence aux fonctions en les faisant précéder du nom du package :\npd.DataFrame([0,1])\nCela permet de dire à Python d’aller chercher dans le namespace pd (alias pour pandas qui est lui-même un ensemble de scripts enregistrés sur le disque) la fonction DataFrame.\n:two: Dans un fichier externe (voir la partie structure et le chapitre portabilité), les dépendances à installer sont listées.\nIl s’agit du fichier requirements.txt\nUn code reproductible doit pouvoir s’exécuter de manière linéaire. S’il provoque une erreur, il est important de pouvoir identifier l’instruction responsable pour pouvoir debugger.\nLes scripts trop longs ne sont pas une bonne pratique. Il est préférable de diviser l’ensemble des scripts exécutant une chaîne de production en “monades”“, c’est-à-dire en petites unités cohérentes. Les fonctions sont un outil privilégié pour cela (en plus de limiter la redondance, et d’être un outil privilégié pour documenter un code).\n\n\nLimiter la redondance\nUn bon principe à suivre est “don’t repeat yourself !” (DRY). Celui-ci réduit la charge de code à écrire, à comprendre et à tenir à jour.\n\nCe post donne quelques bonnes pratiques pour réduire la redondance des codes.\nSupposons qu’on dispose d’une table de données qui utilise le code −99 pour représenter les valeurs manquantes. On désire remplacer l’ensemble des −99 par des NA.\n# On fixe la racine pour être sûr de tous avoir le même dataset\nnp.random.seed(1234)\n\n# On créé un dataframe\na = np.random.randint(1, 10, size = (5,6))\ndf = np.insert(\n    a,\n    np.random.choice(len(a), size=6),\n    -99,\n)\ndf = pd.DataFrame(df.reshape((6,6)), columns=[chr(x) for x in range(97, 103)])\nUn premier jet de code pourrait prendre la forme suivante:\n# Dupliquer les données\ndf2 = df.copy()\n# Remplacer les -99 par des NA\ndf2.loc[df2['a'] == -99,'a'] = np.nan\ndf2.loc[df2['b'] == -99,'b'] = np.nan\ndf2.loc[df2['c'] == -99,'c'] = np.nan\ndf2.loc[df2['d'] == -99,'d'] = np.nan\ndf2.loc[df2['e'] == -98,'e'] = np.nan\ndf2.loc[df2['f'] == -99,'e'] = np.nan\nQuelles sont les choses qui vous dérangent dans le code ci-dessus? Indice: regardez précisément le code et le dataframe (indice: surveillez la colonne e et la colonne g).\nOn peut noter au moins deux problèmes:\n\nLe code est long et répétitif, ce qui nuit à sa lisibilité;\nLe code est très dépendant de la structure des données (nom et nombre de colonnes) et doit être adapté dès que celle-ci évolue;\nOn a introduit une erreur humaine dans le code, difficile à détecter, dans l’instruction concernant la colonne e.\n\nOn voit dans la première version de notre code qu’il y a une structure commune à toutes nos lignes de la forme .[. == -99] <- NA. Cette structure va servir de base à notre fonction, en vue de généraliser le traitement que nous voulons faire.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2['a'] = fix_missing(df['a'])\ndf2['b'] = fix_missing(df['b'])\ndf2['c'] = fix_missing(df['c'])\ndf2['d'] = fix_missing(df['d'])\ndf2['e'] = fix_missing(df['e'])\ndf2['f'] = fix_missing(df['f'])\nCette seconde version du code est meilleure que la première version, car on a réglé le problème d’erreur humaine (il n’est plus possible de taper -98 au lieu de -99). Mais le code reste long et répétitif, et n’élimine pas encore toute possibilité d’erreur, car il est toujours possible de se tromper dans le nom des variables.\nLa prochaine étape est ainsi d’éliminer ce risque d’erreur en combinant deux fonctions (ce qu’on appelle combinaison de fonctions). La première fix_missing() sert à régler le problème sur un vecteur. La seconde généralisera ce procédé à toutes les colonnes. Comme R est un langage vectoriel, c’est une approche fréquente de construire des fonctions sur des vecteurs et les appliquer ensuite à plusieurs colonnes.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\ndf2 = df.copy()\ndf2 = df2.apply(fix_missing)\nCette troisième version du code a plusieurs avantages sur les deux autres versions:\n\nElle est plus concise et plus lisible;\nSi on a un changement de code pour les valeurs manquantes, il suffit de le mettre à un seul endroit;\nElle fonctionne quels que soient le nombre de colonnes et le nom des colonnes;\nOn ne peut pas traiter une colonne différemment des autres par erreur.\n\nDe plus, le code est facilement généralisable. Par exemple, à partir de la même structure, écrire le code qui permet de ne traiter que les colonnes a,b et e.\ndf2 = df.copy()\ndf2[['a','b','e']] = df2[['a','b','e']].apply(fix_missing)\n\n\nDocumentation\nUn code sans aucun commentaire est très difficile à s’approprier (y compris pour la personne qui l’a rédigé et qui y revient quelques semaines plus tard). Cependant, un code présentant trop de commentaires est également illisible et reflète généralement un défaut de conception du code qui n’est pas assez explicite.\nLa documentation vise à présenter la démarche générale, éventuellement à travers des exemples, mais aussi à expliciter certains éléments du code (une opération qui n’est pas évidente, des arguments de fonction, etc.). La documentation se mélange donc aux instructions visant à être exécutées mais s’en distingue. Ces principes sont hérités du paradigme de la “programmation lettrée” (Literate programming) dont l’un des avocats était Donald Knuth.\n\n“Je crois que le temps est venu pour une amélioration significative de la documentation des programmes, et que le meilleur moyen d’y arriver est de considérer les programmes comme des œuvres littéraires. D’où mon titre, « programmation lettrée .\nNous devons changer notre attitude traditionnelle envers la construction des programmes : au lieu de considérer que notre tâche principale est de dire à un ordinateur ce qu’il doit faire, appliquons-nous plutôt à expliquer à des êtres humains ce que nous voulons que l’ordinateur fasse.\nCelui qui pratique la programmation lettrée peut être vu comme un essayiste, qui s’attache principalement à exposer son sujet dans un style visant à l’excellence. Tel un auteur, il choisit , avec soin, le dictionnaire à la main, les noms de ses variables et en explique la signification pour chacune d’elles. Il cherche donc à obtenir un programme compréhensible parce que ses concepts sont présentés dans le meilleur ordre possible. Pour cela, il utilise un mélange de méthodes formelles et informelles qui se complètent”\nDonald Knuth, Literate Programming (source)\n\nCela peut amener à distinguer deux types de documentation:\n\nUne documentation générale de type Jupyter Notebook ou R Markdown qui présente certes du code exécuté mais dont l’objet principal est de présenter une démarche ou des résultats\nUne documentation de la démarche plus proche du code dont l’un des exemples sont les docstrings Python ou la documentation Roxygen\n\n\nPEP 8, PEP 257"
  },
  {
    "objectID": "chapters/code-quality.html#standards-communautaires-de-code",
    "href": "chapters/code-quality.html#standards-communautaires-de-code",
    "title": "Améliorer la qualité de son code",
    "section": "Standards communautaires de code",
    "text": "Standards communautaires de code\nPour parler le même langage, un certain nombre de conventions ont émergé dans les communautés R et Python. L’objectif ici n’est pas de lister ces conventions (une partie d’entre elles ayant déjà été évoquées) mais pointer rapidement vers les principales conventions. Dans le domaine, Python a crée un système un peu plus formel que R mais globalement, la démarche est la même.\n\nPEP 8, PEP 257"
  },
  {
    "objectID": "chapters/code-quality.html#outils-et-méthodes-pour-améliorer-un-code",
    "href": "chapters/code-quality.html#outils-et-méthodes-pour-améliorer-un-code",
    "title": "Améliorer la qualité de son code",
    "section": "Outils et méthodes pour améliorer un code",
    "text": "Outils et méthodes pour améliorer un code\n\nHelpers\nPython ou R étant l’outil de travail principal de milliers de data-scientists, un certain nombre d’outils ont vu le jour pour réduire le temps nécessaire pour créer un projet ou disposer d’un code fonctionnel. Ces outils permettent un gros gain de productivité, réduisent le temps passé à effectuer des tâches rébarbatives et améliorent la qualité d’un projet en offrant des diagnostics voire des correctifs à des codes perfectibles.\n\n\nAnalyse de code\n\nlinters\nformatters\n\nLes linters sont des outils qui permettent d’évaluer la qualité du code et son risque de provoquer une erreur (explicite ou silencieuse). Voici quelques exemples de problèmes que peuvent rencontrer les linters:\n\nles variables sont utilisées mais n’existent pas (erreur)\nles variables inutilisées (inutiles)\nla mauvaise organisation du code (risque d’erreur)\nle non respect des bonnes pratiques d’écriture de code\nles erreurs de syntaxe (par exemple les coquilles)\n\nLa plupart des logiciels de développement embarquent des fonctionalités de diagnostic (voire de suggestion de correctif). Il faut parfois les paramétrer dans les options (ils sont désactivés pour ne pas effrayer l’utilisateur avec des croix rouges partout)\n\n\nRelecture par un tiers / pair-programming"
  },
  {
    "objectID": "chapters/deployment.html",
    "href": "chapters/deployment.html",
    "title": "Déployer et valoriser son projet de data science",
    "section": "",
    "text": "Dans les chapitres précédents, nous avons exploré la manière dont une structure de projet et de code adéquate facilite la réutilisation d’un projet. Cependant, le code est rarement, en soi, le produit final mais un moyen. Le code peut servir à mettre en oeuvre une application, à effectuer des traitements sur une base de données pour un papier ou un rapport, etc. La fréquence de ré-utilisation du code peut elle-même être variable: certains projets vont être utilisés quotidiennement alors que d’autres ne le seront qu’à des échéances diverses.\nComme tout produit, un projet a un cycle de vie. Pour faire simple, on peut séparer celui-ci en trois phases:\n\nla phase de développement correspond à l’écriture du code et à des exploitations exploratoires ;\nla phase de mise en production correspond à l’adaptation du prototype à des contraintes nécessaires pour qu’un projet produise un output à la demande ;\nla phase de maintenance correspond à la situation où on ne met plus en oeuvre de nouvelles fonctionalités mais où on s’assure qu’un projet informatique continue de produire les output désirés malgré l’évolution du contexte.\n\nEn pratique, la distinction entre ces moments d’un projet peut être floue. Par exemple, grâce à Git, on peut ainsi mettre en oeuvre de nouvelles fonctionalités (protypage correspond à la phase de développement) parallèles à celles déjà existantes (phase de maintenance). Néanmoins, ces différences conceptuelles sont intéressantes pour appréhender les contraintes différentes de ces phases d’un projet.\n\n\nLes bonnes pratiques mises en oeuvre jusqu’à présent avaient pour objectif de faciliter la compréhension et la réutilisation d’un projet. Elles sont donc particulièrement appropriées pour réduire le coût en temps de la mise en production et de la maintenance d’un projet. Ces coûts pourraient amener un projet, même utile, à être abandonné.\n\n\n\nDe même, l’emphase du chapitre précédent sur la portabilité vise à faciliter la mise en production. En effet, en créant un environnement normalisé qui créé des conditions simples pour reproduire certains output, on évite un hiatus entre le protypage et la mise en production."
  },
  {
    "objectID": "chapters/deployment.html#pipelines-de-données",
    "href": "chapters/deployment.html#pipelines-de-données",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Pipelines de données",
    "text": "Pipelines de données\nUne chaine de production implique plusieurs étapes qui peuvent éventuellement nécessiter plusieurs langages. Ces étapes peuvent être vues comme des transformations à la chaine d’un ou plusieurs inputs afin de produire un ou plusieurs output.\nLa représentation de ces étapes peut être faite à l’aide des diagrammes acycliques dirigés (DAG):\n\nUn workflow complet sera ainsi reproductible si on peut, en ayant accès aux inputs et à l’ensemble des règles de transformation reproduire exactement les outputs. Si les inputs ou le code change, on peut être en mesure de mettre à jour les outputs, si possible sans faire retourner les parties du projet non concernés.\nUne première manière de développer est l’approche manuelle, qui est une tâche digne de Sisyphe:\n\nEcriture du code\nExécution du code jusqu’à sa fin\nDécouverte d’une erreur ou mise à jour du code ou des données\nRelance le code dans son ensemble\n\nPour éviter ce cycle interminable, on est tenté d’écrire des bases intermédiaires et de ne faire tourner qu’une partie du code. Cette approche, si elle a l’avantage de faire gagner du temps, est néanmoins dangereuse car on peut facilement oublier de mettre à jour une base intermédiaire qui a changé ou au contraire refaire tourner une partie du code qui n’a pas été mise à jour.\nIl existe des méthodes plus fiables pour éviter ces gestes manuels. Celles-ci sont inspirées de GNU Make et consistent à créer le chemin de dépendance de la chaine de production (lister l’environnement, les inputs et les outputs à produire), à déterminer les chemins affectés par un changement de code ou de données pour ne faire tourner à nouveau que les étapes nécessaires.\nLes implémentations en Python et R sont nombreuses. Parmi celles-ci, on peut mettre en valeur\n\nsnakemake pour Python\ntargets pour R"
  },
  {
    "objectID": "chapters/deployment.html#orchestration",
    "href": "chapters/deployment.html#orchestration",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Orchestration",
    "text": "Orchestration\nCertains outils vont plus loin:\n\nargo\nmlflow\nairflow"
  },
  {
    "objectID": "chapters/deployment.html#définition",
    "href": "chapters/deployment.html#définition",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Définition",
    "text": "Définition\n\nL’intégration continue est un ensemble de pratiques utilisées en génie logiciel consistant à vérifier à chaque modification de code source que le résultat des modifications ne produit pas de régression dans l’application développée. […] Elle permet d’automatiser l’exécution des suites de tests et de voir l’évolution du développement du logiciel.\n\n\nLa livraison continue est une approche d’ingénierie logicielle dans laquelle les équipes produisent des logiciels dans des cycles courts, ce qui permet de le mettre à disposition à n’importe quel moment. Le but est de construire, tester et diffuser un logiciel plus rapidement."
  },
  {
    "objectID": "chapters/deployment.html#avantages",
    "href": "chapters/deployment.html#avantages",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Avantages",
    "text": "Avantages\nL’approche CI/CD garantit une automatisation et une surveillance continues tout au long du cycle de vie d’un projet.\nCela présente de nombreux avantages:\n\non peut anticiper les contraintes de la mise en production grâce à des environnements normalisés partant d’image docker standardisées\non peut tester les changements apportés à un livrable par un nouveau prototype\non peut déterminer très rapidement l’introduction de bugs dans un projet"
  },
  {
    "objectID": "chapters/deployment.html#mise-en-oeuvre",
    "href": "chapters/deployment.html#mise-en-oeuvre",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Mise en oeuvre",
    "text": "Mise en oeuvre\nL’idée de l’approche CI/CD est ainsi d’associer chaque changement de code (commit) à l’exécution de scripts automatisés. Bien que mis en oeuvre de manière différente, Gitlab et Github proposent tous les deux ce type de fonctionalités.\nLes actions Github sont un ensemble de règles qui se suivent au format YAML. Cela permet de définir différentes étapes du processus avec, pour chaque étape, des éléments de configuration.\nVoici, par exemple, l’action qui sert de modèle à modifier dans l’exercice d’application\nname: Python Package using Conda\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n    strategy:\n      max-parallel: 5\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: 3.10\n    - name: Add conda to system path\n      run: |\n        # $CONDA is an environment variable pointing to the root of the miniconda directory\n        echo $CONDA/bin >> $GITHUB_PATH\n    - name: Install dependencies\n      run: |\n        conda env update --file environment.yml --name base\n    - name: Lint with flake8\n      run: |\n        conda install flake8\n        # stop the build if there are Python syntax errors or undefined names\n        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n    - name: Test with pytest\n      run: |\n        conda install pytest\n        pytest"
  },
  {
    "objectID": "chapters/deployment.html#quest-ce-quun-site-web",
    "href": "chapters/deployment.html#quest-ce-quun-site-web",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Qu’est-ce qu’un site web ?",
    "text": "Qu’est-ce qu’un site web ?\nPour simplifier, on peut voir un site web comme la combinaison de trois éléments:\n\nune arborescence de fichiers HTML qui présentent le contenu du site dans un balisage lourd\ndes fichiers CSS qui gèrent la mise en forme1\ndes fonctions javascript\n\nIl existe énormément d’outils aujourd’hui qui permettent, sans connaissance en HTML, CSS ou JS, de créer un site web. Dans le domaine de la data-science, le format Markdown (fichiers .md) s’est imposé.\nMarkdown est un système d’édition doté d’une syntaxe simplifiée souvent utilisé pour faire de la documentation de projet. Le format est utilisé sur de nombreux sites internet, notamment Gitlab et Stackoverflow. L’extension de ce type de fichier est .md. Markdown présente plusieurs avantages:\n\nil est facile d’inclure des blocs de code informatique et des équations mathématiques dans un document Markdown ;\nle formatage de blocs de texte ou de code est simple et très bien fait (et beaucoup plus léger qu’en LaTeX par exemple) ;\nil existe des outils de conversion de Markdown en HTML très bien faits.\n\nPlus d’éléments sur la logique de Markdown et ses intérêts sont disponibles dans le chapitre R Markdown de la documentation utilitR"
  },
  {
    "objectID": "chapters/deployment.html#comment-faire-un-site-web",
    "href": "chapters/deployment.html#comment-faire-un-site-web",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Comment faire un site web ?",
    "text": "Comment faire un site web ?\nIl existe historiquement plusieurs approches dans l’écosystème de la data-science, selon le langage utilisé et l’output désiré.\nSi on part de fichiers qui présentent des blocs de code qui ne nécessitent pas d’être exécutés, l’écosystème le plus riche est Hugo. Celui-ci permet de générer des sites web à l’architecture complexe à partir d’une arborescence de .md. C’est l’approche adoptée pour ce site web\nCependant, il est souvent nécessaire d’exécuter des bouts de code pour tester les exemples présentés, ou générer des tableaux ou sorties graphiques. Pour cela, historiquement, il existe deux paradigmes:\n\nJupyterBook qui vise à générer des sites web et des notebooks à partir de fichiers markdown. Le notebook n’est donc pas le produit de départ (cf. XXXX) mais un livrable.\nR Markdown: l’écosystème le plus riche avec de nombreux modèles de documents customisables (documents HTML ou articles PDF, sites web de documentation avec bookdown, blogs avec blogdown, dashboards, etc.) Le principe de R Markdown est d’offrir une surcouche à Markdown pour que les blocs de code soient exécutés afin de créer un output reproductible.\n\nR Markdown est un système d’une grande richesse. Initialement pensé pour les utilisateurs de R, ce paradigme permet maintenant de créer des documents executant d’autres langages, notamment du Python (le cours de python de 2e année de l’ENSAE est testé et construit grâce à R Markdown).\nQuarto est le petit nouveau dans cet écosystème, amené à devenir un outil standard des data-scientists comme peuvent l’être, aujourd’hui, les Notebooks Jupyter. Successeur de R Markdown, il vise à améliorer l’aspect universel des documents produits en n’obligeant plus à utiliser R pour compiler le document qui ne le nécessitent pas. C’est un outil particulièrement adapté aux utilisateurs de Python\n\nUn document quarto hérite des principes de base d’un document R Markdown, notamment la structure. Il comporte ainsi deux parties principales :\n\nL’en-tête (YAML header) qui gère les éléments de style et les paramètres globaux;\nLe contenu qui gère le fond et permet d’alterner librement texte et code :\n\nLes blocs de texte brut mis en forme selon la syntaxe markdown\nLes blocs de code sont présentés dans des chunks identifiés par un langage (ici python) qui seront exécutés de manière linéaire et produiront l’output désiré (ici une figure matplotlib)."
  },
  {
    "objectID": "chapters/deployment.html#comment-mettre-à-disposition-un-site-web",
    "href": "chapters/deployment.html#comment-mettre-à-disposition-un-site-web",
    "title": "Déployer et valoriser son projet de data science",
    "section": "Comment mettre à disposition un site web ?",
    "text": "Comment mettre à disposition un site web ?\nMettre à jour manuellement un site web après l’avoir compilé est une tâche pénible et source d’erreur dans un projet actif de data-science. L’automatisation issue de l’approche CI/CD permet un gain de confort aussi dans ce domaine.\nSupposons qu’on ait mis en oeuvre une routine pour automatiser la construction d’un site web à partir de fichiers Markdown. Comment les mettre à disposition ?\nIl existe plusieurs manières de déployer automatiquement un site web :\n\nGitlab pages ;\nGithub pages ;\nNetlify.\n\nCes trois services sont gratuits. Ils consistent à mettre à disposition un DNS sur lequel des fichiers HTML peuvent être mis à disposition automatiquement pour assurer que chaque commit\nL’exercice d’application supposant l’utilisation de Github, il présentera Netlify qui est plus pratique que Github Pages2. Le résultat de cette chaine de production d’un site web reproductible est accessible à l’url https://spiffy-florentine-c913b9.netlify.app/"
  },
  {
    "objectID": "chapters/git.html",
    "href": "chapters/git.html",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "",
    "text": "Le développement rapide de la data science au cours de ces dernières années s’est accompagnée d’une complexification substantielle des projets. Par ailleurs, les projets sont de plus en plus collaboratifs, que ce soit dans le cadre d’équipes dans un contexte professionnel ou bien pour des contributions à des projets open-source. Naturellement, ces évolutions doivent nous amener à modifier nos manières de travailler pour gérer cette complexité croissante et continuer à produire de la valeur à partir des projets de data science.\nPourtant, tout data scientist s’est parfois demandé :\n\nquelle était la bonne version d’un programme\nqui était l’auteur d’un bout de code en particulier\nsi un changement était important ou juste un essai\ncomment fusionner des programmes\netc.\n\nEt il n’est pas rare de perdre le fil des versions de son projet lorsque l’on garde trace de celles-ci de façon manuelle.\nExemple de contrôle de version fait “à la main”\n\nPourtant, il existe un outil informatique puissant afin de répondre à tous ces besoins : la gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer l’historique des modifications d’un ensemble de fichiers\nrevenir à des versions précédentes d’un ou plusieurs fichiers\nrechercher les modifications qui ont pu créer des erreurs\ntravailler simultanément sur un même fichier sans risque de perte\npartager ses modifications et récupérer celles des autres\nproposer des modifications, les discuter, sans pour autant modifier la dernière version existante\nidentifier les auteurs et la date des modifications\n\nEn outre, ces outils fonctionnent avec tous les langages informatiques car ils reposent sur la comparaison des lignes et des caractères des programmes, indépendamment du langage. En bref, c’est la bonne manière pour partager des codes et travailler à plusieurs sur un projet de data science. En réalité, il ne serait pas exagéré de dire que l’utilisation du contrôle de version est la bonne pratique la plus fondamentale de tout projet faisant intervenir du code, et qu’elle conditionne largement toutes les autres.\n\n\n\nPlusieurs logiciels de contrôle de version existent sur le marché. En principe, le logiciel Git, développé initialement pour fournir une solution décentralisée et open-source dans le cadre du développement du noyau Linux, est devenu largement hégémonique. Aussi, toutes les application de ce cours s’effectueront à l’aide du logiciel Git.\n\n\n\nTravailler de manière collaborative avec Git implique de synchroniser son répertoire local avec une copie distante, située sur un serveur hébergeant des projets Git. Ce serveur peut être un serveur interne à une organisation, ou bien être fourni par un hébergeur externe. Les deux alternatives les plus populaires en la matière sont GitHub et GitLab. Dans ce cours, nous utiliserons GitHub, qui est devenu au fil des années la référence pour l’hébergement des projets open-source. En pratique, les deux services sont relativement semblables, et tous les concepts présentés se retrouvent sous une forme similaire sur les deux plateformes."
  },
  {
    "objectID": "chapters/git.html#principes-et-commandes-usuelles",
    "href": "chapters/git.html#principes-et-commandes-usuelles",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Principes et commandes usuelles",
    "text": "Principes et commandes usuelles\nLe graphique suivant illustre les principes fondamentaux de Git.\nGit tout-en-un (Source) \nLorsqu’on utilise Git, il est important de bien distinguer ce qui se passe en local (sur son poste, sur le serveur sur lequel on travaille…) de ce qui se passe en remote, i.e. en intéragissant avec un serveur distant. Comme le montre le graphique, l’essentiel du contrôle de version se passe en réalité en local.\nEn théorie, sur un projet individuel, il est même possible de réaliser l’ensemble du contrôle de version en mode hors-ligne. Pour cela, il suffit d’indiquer à Git le projet (dossier) que l’on souhaite versionner en utilisant la commande git init. Cette commande a pour effet de créer un dossier .git à la racine du projet, dans lequel Git va stocker tout l’historique du projet (commits, branches, etc.) et permettre de naviguer entre les versions. A cause du . qui préfixe son nom, ce dossier est généralement caché par défaut, ce qui n’est pas problématique dans la mesure où il n’y a jamais besoin de le parcourir ou de le modifier à la main en pratique. Retenez simplement que c’est la présence de ce dossier .git qui fait qu’un dossier est considéré comme un projet Git, et donc que vous pouvez utilisez les commandes usuelles de Git dans ce dossier à l’aide d’un terminal : - git status : affiche les modifications du projet par rapport à la version précédente ; - git add chemin_du_fichier : ajoute un fichier nouveau ou modifié à la zone de staging de Git en vue d’un commit ; - git add -A : ajoute tous les fichiers nouveaux ou modifiés à la zone de staging ; - git commit -m \"message de commit\" : crée un commit, i.e. une photographie des modifications (ajouts, modifications, suppressions) apportées au projet depuis la dernière version, et lui assigne un message décrivant ces changements. Les commits sont l’unité de base de l’historique du projet construit par Git.\nEn pratique, travailler uniquement en local n’est pas très intéressant. Pour pouvoir travailler de manière collaborative, on va vouloir synchroniser les différentes copies locales du projet à un répertoire centralisé, qui maintient de fait la “source de vérité” (single source of truth). Même sur un projet individuel, il fait sens de synchroniser son répertoire local à une copie distante pour assurer l’intégrité du code de son projet en cas de problème matériel.\nEn général, on va donc initialiser le projet dans l’autre sens : - créer un nouveau projet sur GitHub - générer un jeton d’accès (personal access token) - cloner le projet en local via la méthode HTTPS : git clone https://github.com/<username>/<project_name>.git\nLe projet cloné est un projet Git — il contient le dossier .git — synchronisé par défaut avec le répertoire distant. On peut le vérifier avec la commande remote de Git :\n$ git remote -v\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (fetch)\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (push)\nLe projet local est bien lié au répertoire distant sur GitHub, auquel Git donne par défaut le nom origin. Ce lien permet d’utiliser les commandes de synchronisation usuelles : - git pull : récupérer les changements (fetch) sur le remote et les appliquer au projet local - git push : envoyer les changements locaux sur le remote"
  },
  {
    "objectID": "chapters/git.html#implémentations",
    "href": "chapters/git.html#implémentations",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Implémentations",
    "text": "Implémentations\nGit est un logiciel, qui peut être téléchargé sur le site officiel pour différents systèmes d’exploitation. Il existe cependant différentes manières d’utiliser Git : - le client en ligne de commande : c’est l’implémentation standard, et donc la plus complète. C’est celle qu’on utilisera dans ce cours. Le client Git est installé par défaut sur les différents services du SSP Cloud (VSCode, RStudio, Jupyter, etc.) et peut donc être utilisé via n’importe quel terminal. La documentation du SSP Cloud détaille la procédure ; - des interfaces graphiques : elles facilitent la prise en main de Git via des guides visuels, mais ne permettent pas de réaliser toutes les opérations permises par Git - l’interface native de RStudio pour les utilisateurs de R : très complète et stable. La formation au travail collaboratif avec Git et RStudio présente son utilisation de manière détaillée ; - le plugin Jupyter-git pour les utilisateurs de Python : elle implémente les principales features de Git, mais s’avère assez instable à l’usage."
  },
  {
    "objectID": "chapters/git.html#bonnes-pratiques",
    "href": "chapters/git.html#bonnes-pratiques",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\nLe contrôle de version est une bonne pratique de développement en soi… mais son utilisation admet elle même des bonnes pratiques qui, lorsqu’elles sont appliquées, permettent d’en tirer le plus grand profit.\n\nQue versionne-t-on ?\n\nUniquement des fichiers texte\nA chaque commit, Git calcule les différences entre les versions successives du projet, afin de ne pas avoir à stocker une image complète de ce dernier à chaque fois. C’est ce qui permet aux projets Git de rester très légers par défaut, et donc aux différentes opérations impliquant le remote (clone, push, pull..) d’être très rapides.\nLa contrepartie de cette légèreté est une contrainte sur les types d’objets que l’on doit versionner. Les différences sont calculables uniquement sur des fichiers de type texte : codes source, fichiers texte, fichiers de configuration non-sensibles… Voici donc une liste non-exhaustive des extensions de fichier que l’on retrouve fréquemment dans un dépôt Git d’un projet R ou Python : .py, .R, .Rmd, .txt, .json, .xml, .yaml, .toml, et bien d’autres.\nEn revanche tous les fichiers binaires — pour faire simple, tous les fichiers qui ne peuvent pas être ouverts dans un éditeur de texte basique sans produire une suite inintelligible de caractères — n’ont généralement pas destination à se retrouver sur un dépôt Git. Du fait de leur formatage (binaire), Git ne peut pas calculer les différences entre versions pour ces fichiers et c’est donc le fichier entier qui est sauvegardé dans l’historique à chaque changement, ce qui peut très rapidement faire croître la taille du dépôt. Pour éviter de versionner ces fichiers par erreur, on va les ajouter au fichier .gitignore (cf. supra).\n\n\nPas de données\nComme expliqué en introduction, le fil rouge de ce cours sur les bonnes pratiques est l’importance de bien séparer code, données et environnement d’exécution afin de favoriser la reproductibilité des projets de data science. Ce principe doit s’appliquer également à l’usage du contrôle de version, et ce pour différentes raisons.\nA priori, inclure ces données dans un dépôt Git peut sembler une bonne idée en termes de reproductibilité. En machine learning par exemple, on est souvent amené à réaliser de nombreuses expérimentations à partir d’un même modèle appliqué à différentes transformations des données initiales, transformations que l’on pourrait versionner. En pratique, il est généralement préférable de versionner le code qui permet de générer ces transformations et donc les expérimentations associées, dans la mesure où le suivi des versions des datasets peut s’avérer rapidement complexe. Pour de plus gros projets, des alternatives spécifiques existent : c’est le champ du MLOps, domaine en constante expansion qui vise à rendre les pipelines de machine learning plus reproductibles.\nEnfin, la structure même de Git n’est techniquement pas faite pour le stockage de données. Si des petits datasets dans un format texte ne poseront pas de problème, des données volumineuses (à partir de plusieurs Mo) vont faire croître la taille du dépôt et donc ralentir significativement les opérations de synchronisation avec le remote.\n\n\nPas d’informations locales\nLà encore en vertu du principe de séparation données / code/ environnement, les données locales, i.e. spécifiques à l’environnement de travail sur lequel le code a été exécuté, n’ont pas vocation à être versionnées. Par exemple, des fichiers de configuration spécifiques à un poste de travail, des chemins d’accès spécifiques à un ordinateur donné, etc. Cela demande une plus grande rigueur lors de la construction du projet, mais garantit par là même une meilleure reproductiblité pour les futurs utilisateurs du projet.\n\n\nPas d’outputs\nLes outputs d’un projet (graphiques, publications, modèle entraîné…) n’ont pas vocation à être versionné, en vertu des différents arguments présentés ci-dessus : - il ne s’agit généralement pas de fichiers de type texte ; - le code source du projet doit dans tous les cas permettre des les regénérer à l’identique.\n\n\nUtiliser un .gitignore\nOn a listé précédemment un large éventail de fichiers qui n’ont, par nature, pas vocation à être versionné. Bien entendu, faire attention à ne pas ajouter ces différents fichiers au moment de chaque git add serait assez pénible. Git simplifie largement cette procédure en nous donnant la possibilité de remplir un fichier .gitignore, situé à la racine du projet, qui spécifie l’ensemble des fichiers et types de fichiers que l’on ne souhaite pas versionner dans le cadre du projet courant.\nDe manière générale, il y a pour chaque langage des fichiers que l’on ne souhaitera jamais versionner. Pour en tenir compte, une première bonne pratique est de choisir le .gitignore associé au langage du projet lors de la création du dépôt sur GitHub. Ce faisant, le projet est initialité avec un gitignore déjà existant et pré-rempli de chemins et de types de fichiers qui ne sont pas à versionner. Regardons un extrait du gitignore Python pour comprendre sa structure et son fonctionnement.\npip-log.txt\n__pycache__/\n*.log\nChaque ligne du gitignore spécifie un élément à ignorer du contrôle de version, élément qui peut être un ficher/dossier ou bien une règle concernant un ensemble de fichiers/dossiers. Sauf si spécifié explicitement, les chemins sont relatifs à la racine du projet. L’extrait du gitignore Python illustre les différentes possibilités :\n\nligne 1 : ignore le fichier pip-log.txt ;\nligne 2 : ignore le dossier __pycache__/ ;\nligne 3 : ignore tous les fichiers dont l’extension est .log.\n\nDe nombreuses autres possiblités existent, et sont détaillées par exemple dans la documentation de Git.\n\n\n\nMessages des commits\nLe commit est l’unité de temps de Git, et donc fondamentalement ce qui permet de remonter dans l’historique d’un projet. Afin de pouvoir bénéficier à plein de cet avantage de Git, il est capital d’accompagner ses commits de messages pertinents, en se plaçant dans la perspective que l’on peut être amené plusieurs semaines ou mois plus tard à vouloir retrouver du code dans l’historique de son projet. Les quelques secondes prises à chaque commit pour réfléchir à une description pertinente du bloc de modifications que l’on apporte au projet peuvent donc faire gagner un temps précieux à la longue.\nDe nombreuses conventions existent pour rédiger des messages de commit pertinents. Nous rappelons ici les plus importantes :\n\ncontenu : le message doit détailler le pourquoi plutôt que le comment des modifications. Par exemple, plutôt que “Ajoute le fichier test.py”, on préférera écrire “Ajout d’une série de tests unitaires” ;\nstyle : le message doit être à l’impératif et former une phrase (sans point à la fin) ;\nlongueur : le message du commit doit être court (< 72 caractères). S’il n’est pas possible de trouver un message de cette taille qui résume le commit, c’est généralement un signe que le commit regroupe trop de changements (cf. point suivant). Le fait de devoir mettre des + ou des & / et dans un message de commit pour séparer les changements est également un bon indicateur d’un commit trop gros.\n\n\n\nFréquence des commits\nDe manière générale, il est conseillé de réaliser des commits réguliers lorsque l’on travaille sur un projet. Une règle simple que l’on peut par exemple appliquer est la suivante : dès lors qu’un ensemble de modifications forment un tout cohérent et peuvent être résumées par un message simple, il est temps d’en faire un commit. Cette approche a de nombreux avantages :\n\nsi l’on fait suivre chaque commit d’un push — ce qui est conseillé en pratique — on s’assure de disposer régulièreemnt d’une copie de ses travaux, ce qui limite le risque de perte ;\nil est plus facile de revenir en arrière en cas d’erreur si les commits portent sur des changements ciblés et cohérents ;\nle processus de review d’une pull request est facilité, car les différents blocs de modification sont plus clairement séparés ;\ndans une approche d’intégration continue — concept que l’on verra en détail dans le chapitre sur la [mise en production]() — faire des commits et des PR régulièrement permet de déployer de manière continue les changements en production, et donc d’obtenir les feedbacks des utilisateurs et d’adapter plus rapidement si besoin."
  },
  {
    "objectID": "chapters/git.html#branches",
    "href": "chapters/git.html#branches",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Branches",
    "text": "Branches\n\nConcept\nLa possibilité de créer des branches est l’une des fonctionnalités majeures de Git. La création d’une branche au sein d’un projet permet de diverger de la ligne principale de développement (généralement appelée master, terme tendant à disparaître au profit de celui de main) sans impacter cette ligne. Cela permet de séparer le nouveau développement et de faire cohabiter plusieurs versions, pouvant évoluer séparément et pouvant être facilement rassemblées si nécessaire.\nPour comprendre comment fonctionnent les branches, il nous faut revenir un peu plus en détail sur la manière dont Git stocke l’historique du projet. Comme nous l’avons vu, l’unité temporelle de Git est le commit, qui correspond à une photographie à un instant donné de l’état du projet (snapshot). Chaque commit est uniquement identifié par un hash, une longue suite de caractères. La commande git log, qui liste les différents commits d’un projet, permet d’afficher ce hash ainsi que diverses métadonnées (auteur, date, message) associées au commit.\n$ git log\ncommit e58b004d3b68bdf28093fe6ad6036b5d13216e55 (HEAD -> master, origin/master, origin/HEAD)\nAuthor: Lino Galiana <xxx@xxx.fr>\nDate:   Tue Mar 22 14:34:04 2022 +0100\n\n    ajoute code équivalent python\n\n...\nUne branche est simplement un pointeur vers un commit. Dans l’exemple précédent, on a imprimé les informations du dernier commit en date. La branche principale (master) pointe vers ce commit. Si l’on faisait un nouveau commit, le pointeur se décalerait et la branche master pointerait à présent sur le nouveau commit.\n\n\nBranches locales\nDans ce contexte, créer une nouvelle branche (en local) revient simplement à créer un nouveau pointeur vers un commit donné. Supposons que l’on crée une branche testing à partir du dernier commit.\n$ git branch testing  # Crée une nouvelle branche\n$ git branch  # Liste les branches existantes\n* master  # La branche sur laquelle on se situe\n  testing  # La nouvelle branche créée\nLa figure suivante illustre l’effet de cette création sur l’historique Git.\n\nDésormais, deux branches (master et testing) pointent vers le même commit. Si l’on effectue à présent des commits sur la branche testing, on va diverger de la branche principale, ce qui permet de développer une nouvelle fonctionnalité sans risquer d’impacter master.\nPour savoir sur quelle branche on se situe à instant donné — et donc sur quelle branche on va commiter — Git utilise un pointeur spécial, appelé HEAD, qui pointe vers la branche courante. On comprend à présent mieux la signification de HEAD -> master dans l’output de la commande git log vu précédemment. Cet élément spécifie la situation locale actuelle et signifie : on se situe actuellement sur la branche master, qui pointe sur le commit e58b004. Pour changer de branche, i.e. déplacer le HEAD, on utilise la commande git checkout. Par exemple, pour passer de la branche master sur laquelle on est par défaut à la branche testing :\n$ git checkout testing  # Changement de branche\nSwitched to branch 'testing'\nOn se situe désormais sur la branche testing, sur laquelle on peut laisser libre cours à sa créativité sans risquer d’impacer la branche principale du projet. Mais que se passe-t-il si, pendant que l’on développe sur testing, un autre membre du projet commit sur master ? On dit que les historiques ont divergé. La figure suivante illustre à quoi ressemble à présent l’historique du projet (et suppose que l’on est repassé sur master).\n\nCette divergence n’est pas problématique en soi : il est normal que les différentes parties et expérimentations d’un projet avancent à différents rythmes. La difficulté est de savoir comment réconcillier les différents changements si l’on décide que la branche testing doit être intégrée dans master. Deux situations peuvent survenir : - les modifications opérées en parallèle sur les deux branches ne concernent pas les mêmes fichiers ou les mêmes parties des fichiers. Dans ce cas, Git est capable de fusionner (merge) les changements automatiquement et tout se passe sans encombre ; - dans le cas contraire, survient un merge conflict : les branches ont divergé de telle sorte qu’il n’est pas possible pour Git de fusionner les changements automatiquement. Il faut alors résoudre les conflits manuellement.\nLa résolution des conflits est une étape souvent douloureuse lors de l’apprentissage de Git. Aussi, nous conseillons dans la mesure du possible de ne pas fusionner des branches manuellement en local avec Git — c’est d’ailleurs pour cette raison que nous n’avons pas détaillé les commandes pour le faire. Dans les sections suivantes, nous verrons comment une bonne organisation préalable du travail en équipe, combinée aux outils collaboratifs fournis par GitHub, permet de rendre le processus de fusion des branches largement indolore.\n\n\nBranches remote\nRappellons que toutes les opérations que nous avons effectuées sur les branches dans cette section se sont passés en local, le répertoire distant est resté totalement inchangé. Pour pouvoir collaborer sur une branche ou bien en faire une pull request (cf. supra), il faut pousser la branche sur le répertoire distant. La commande est simple : git push origin <branche>.\n$ git push origin testing\nCounting objects: 24, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (15/15), done.\nWriting objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.\nTotal 24 (delta 2), reused 0 (delta 0)\nTo https://github.com/linogaliana/ensae-reproductibilite-website\n * [new branch]      testing -> testing"
  },
  {
    "objectID": "chapters/git.html#workflow-collaboratif",
    "href": "chapters/git.html#workflow-collaboratif",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Workflow collaboratif",
    "text": "Workflow collaboratif\nComme on l’a vu précédemment, si le modèle des branches de Git semble idéal pour gérer le travail collaboratif et asynchrone, il peut également s’avérer rapidement complexe à manipuler en l’absence d’une bonne organisation du travail en équipe. De nombreux modèles (“workflows”) existent en la matière, avec des complexités plus ou moins grandes selon la nature du projet. Nous conseillons d’adopter dans la plupart des cas un modèle très simple : le GitHub Flow.\nLe GitHub Flow est une méthode d’organisation minimaliste du travail collaboratif, qui est progressivement devenue la norme dans les projets open-source. Elle est résumée par la figure suivante, dont nous détaillons par la suite les différentes étapes.\n\n\nDéfinition des rôles des contributeurs\nDans tout projet collaboratif, une première étape essentielle est de bien délimiter les rôles des différents contributeurs. Les différents participants au projet ont en effet généralement des rôles différents dans l’organisation, des niveaux différents de pratique de Git, etc. Il est important de refléter ces différents rôles dans l’organisation du travail collaboratif.\nSur les différents hébergeurs de projets Git, cela prend la forme de rôles que l’on attribue aux différents membres du porjet. Les mainteneurs sont les seuls à pouvoir écrire directement sur master. Les contributeurs sont quant à eux tenus de développer sur des branches. Cela permet de protéger la branche principale, qui doit rester une version propre et jamais compromise du projet.\nNotons que la possibilité de donner des rôles sur les projets GitHub n’est possible que dans le cadre d’organisations (payantes), donc dans un contexte professionnel ou de projets open-source d’une certaine ampleur. Pour des petits projets, il est nécessaire de s’astreindre à une certaine rigueur individuelle pour respecter cette organisation.\n\n\nDéveloppement sur des branches de court-terme\nLes contributeurs développent uniquement sur des branches. Il est d’usage de créer une branche par fonctionnalité, en lui donnant un nom reflétant la fonctionnalité en cours de développement (ex : ajout-tests-unitaires). Les différents contributeurs à la fonctionnalité en cours de développement font des commits sur la branche, en prenant bien soin de pull régulièrement les éventuels changements pour ne pas risquer de conflits de version. Pour la même raison, il est préférable de faire des branches dites de court-terme, c’est à dire propres à une petite fonctionnalité, quite à diviser une fonctionnalité en séries d’implémentations. Cela permet de limiter les éventuels conflits à gérer lors de la fusion finale de la branche avec master.\n\n\nPull Request\nUne fois la série de modifications terminée, vient le temps de rassembler les différents travaux, par l’intermédiaire de la fusion entre la branche et master. Il faut alors “demander” de fusionner (pull request) sur GitHub. Cela ouvre une page liée à la pull request, qui rappelle les différents changements apportés et leurs auteurs, et permet d’entamer une discussion à propos de ces changements.\n\n\nProcessus de review\nLes différents membres du projet peuvent donc analyser et commenter les changements, poser des questions, suggérer des modifications, apporter d’autres contributions, etc. Il est par exemple possible de mentionner un membre de l’équipe par l’intermédiaire de @personne. Il est également possible de procéder à une code review, par exemple par un développeur plus expérimenté.\n\n\nRésolution des éventuels conflits\nEn adoptant cette manière de travailler, master ne sera modifiée que via des pull requests. Il ne devrait donc jamais y avoir le moindre conflit à régler sur master, les seuls conflits possibles se passent sur les branches. Par exemple, dans le cas où une autre pull request aurait été fusionnée sur master depuis l’ouverture de la pull request en question.\nDans le cas d’un conflit à gérer, le conflit doit être résolu dans la branche et pas dans master. Voici la marche à suivre :\n\nappliquez le conseil de survie : faites une copie de sauvegarde de votre clone\ndans votre clone, placez vous sur la branche en question : git checkout nom-de-la-branche\nfusionnez master dans la branche : git merge master\nrésolvez les éventuels conflits dans les fichiers concernés\nfinalisez le commit de merge et poussez-le sur la branche remote, ce qui le fera apparaître dans la pull request\n\n\n\nFusion de la branche\nSi tout est en ordre, la branche peut être fusionnée. Seuls les mainteneurs, qui ont les droits sur master, peuvent fusionner la pull request. En termes d’historique du projet, deux choix sont possibles : - “Create a merge commit” : tous les commits réalisés sur la branche apparaîtront dans l’historique du projet ; - “Squash and merge” : les différents commits réalisés sur la branche seront rassemblés en un commit unique. Cette option est généralement préférable lorsqu’on utilise des branches de court-terme : elles permettent de garder l’historique plus lisible."
  },
  {
    "objectID": "chapters/git.html#utiliser-les-issues",
    "href": "chapters/git.html#utiliser-les-issues",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Utiliser les issues",
    "text": "Utiliser les issues\nLa manière la plus simple de contribuer à un projet open-source est d’ouvrir une issue. Sur GitHub, cela se fait sur la page du projet, sous l’onglet Issue (cf. documentation officielle). Les issues peuvent avoir différentes nature : - suggestion d’amélioration (sans code) - notification de bug - rapports d’expérience - etc.\nLes issues sont une manière très peu couteuse de contributer à un projet, mais leur importance est capitale, dans la mesure où il est impossible pour les développeurs d’un projet de penser en amont à toutes les utilisations possibles et donc tous les bugs possibles d’une application."
  },
  {
    "objectID": "chapters/git.html#proposer-une-pull-request",
    "href": "chapters/git.html#proposer-une-pull-request",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Proposer une pull request",
    "text": "Proposer une pull request\nUne autre manière, plus ambitieuse, de contribuer à l’open source est de proposer des pull requests. Concrètement, l’idée est de proposer une amélioration ou bien de résoudre un bug sous forme de code, que les mainteneurs du projet peuvent ensuite décider d’intégrer au code existant.\nLa procédure pour proposer une pull request à un projet sur lequel on n’a aucun droit est très similaire à celle décrite ci-dessus dans le cas normal. La principale différence est que, du fait de l’absence de droits, il est impossible de pousser une branche locale sur le répertoire du projet. On va donc devoir créer au préalable un fork, i.e. une copie du projet que l’on crée dans son espace personnel sur GitHub. C’est sur cette copie que l’on va appliquer la procédure décrite précédemment, en prenant bien soin de travailler sur une branche et non sur master. Une fois les modifications pertinentes effectuées sur la branche du fork, GitHub propose de créer une pull request sur le dépôt original. Cette pull request sera alors visible des mainteneurs du projet, qui pourront l’évaluer et décider d’adopter (ou non) les changements proposés."
  },
  {
    "objectID": "chapters/git.html#respecter-les-règles-de-contribution",
    "href": "chapters/git.html#respecter-les-règles-de-contribution",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Respecter les règles de contribution",
    "text": "Respecter les règles de contribution\nVouloir contribuer à un projet open-source est très louable, mais ne peut pas pour autant se faire n’importe comment. Un projet est constitué de personnes, qui ont développé ensemble une manière de travailler, des standards de bonnes pratiques, etc. Pour s’assurer que sa contribution ne reste pas lettre morte, il est indispensable de s’imprégner un minimum de la culture du projet.\nPour faciliter les contributions, les projets open-source spécifient bien souvent la manière dont les utilisateurs peuvent contribuer ainsi que le format attendu. En général, ces règles de contribution sont spécifiées dans un fichier CONTRIBUTING.md situé à la racine du projet GitHub, ou a défaut dans le README du projet. Il est essentiel de bien lire ce document s’il existe afin de s’assurer de proposer des contributions pertinentes."
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours s’adresse aux praticiens de la data science, entendue ici au sens large comme la combinaison de techniques issues des mathématiques, de la statistique et de l’informatique pour produire de la connaissance utile à partir de données. Cela inclut donc tout autant les data scientists et statisticiens travaillant dans le privé ou dans des administrations que les chercheurs dont les travaux font intervenir des traitements computationnels à partir de données.\nIl part du constat que les formations académiques dans ce domaine adoptent souvent une orientation essentiellement technique, visant une compréhension fine des modèles manipulés, mais ne discutent que rarement des problèmes pratiques qui forment le quotidien du data scientist dans un contexte professionnel. Ce cours vise à combler ce manque en proposant des pistes de solution à diverses questions que peuvent se poser les data scientists lorsqu’ils transitionnent du contexte de la formation initiale à des projets réels : - comment travailler de manière collaborative sur un projet ? - comment partager du code et s’assurer que celui-ci va tourner sans erreur sur un autre environnement d’exécution ? - comment passer d’un environnement de développement — par exemple, des notebooks — à un environnement de production — comme un serveur de production ou bien un cluster ? - comment déployer un modèle de data science, et rendre celui-ci accessible à des utilisateurs afin de le valoriser ? - comment automatiser les différentes étapes de son projet afin de simplifier sa maintenance ?\nAfin de proposer des réponses à ces interrogations, ce cours présente un ensemble de bonnes pratiques et d’outils issus de différents domaines de l’informatique, comme le développement logiciel, l’infrastructure, l’administration de serveurs, le déploiement applicatif, etc.. L’objectif n’est bien entendu pas de développer une expertise dans chacun de ces domaines, dans la mesure où ces compétences font l’objet de métiers à part entière, que sont les développeurs, les data architects, les sysadmin, ou encore les data engineers.\nEn revanche, face à la taille croissante des projets de data science et donc des équipes qui les portent, le data scientist tend à se retrouver à l’interface de ces différentes professions, avec lesquelles il doit communiquer de manière efficiente pour mener ces projets à bien. Ce cours vise à fournir, plus que des connaissances techniques pointues, les élements de langage nécessaires pour pouvoir jouer ce rôle d’interface en communiquant à la fois avec les équipes métiers et les équipes techniques qui entourent un projet de data science."
  },
  {
    "objectID": "chapters/introduction.html#origine",
    "href": "chapters/introduction.html#origine",
    "title": "Introduction",
    "section": "Origine",
    "text": "Origine\nLa notion de “bonnes pratiques” qui nous intéresse dans ce cours trouve son origine au sein de la communauté des développeurs logiciels. Elle constitue une réponse à plusieurs constats : - le code est beaucoup plus souvent lu qu’il n’est écrit ; - la maintenance d’un code demande souvent (beaucoup) plus de ressources que son développement initial ; - la personne qui maintient une base de code a de fortes chances de ne pas être celle qui l’a écrite.\nFace à ces constats, un ensemble de règles informelles ont été conventionnellement acceptées par la communauté des développeurs comme produisant des logiciels plus fiables, évolutifs et maintenables dans le temps. Récemment, dans le contexte d’une évolution des logiciels vers des applications web vivant dans le cloud, un certain nombre de ces bonnes pratiques ont été formalisées dans un manifeste : la 12-factor app."
  },
  {
    "objectID": "chapters/introduction.html#pourquoi-sintéresser-aux-bonnes-pratiques",
    "href": "chapters/introduction.html#pourquoi-sintéresser-aux-bonnes-pratiques",
    "title": "Introduction",
    "section": "Pourquoi s’intéresser aux bonnes pratiques ?",
    "text": "Pourquoi s’intéresser aux bonnes pratiques ?\nTout cela est bien intéressant, mais en quoi est-ce pertinent pour le data scientist, dont le rôle n’est pas de développer des applications mais de donner du sens aux données ? Du fait du développement rapide de la data science et conséquemment de la croissance de la taille moyenne des projets, l’activité du data scientist tend à se rapprocher par certains aspects de celle du développeur :\n\nles projets sur lesquels travaille le data scientist sont intenses en code ;\nil doit travailler de manière collaborative au sein de projets de grande envergure ;\nil est de plus en plus amené à travailler à partir de données massives, ce qui nécessite de travailler sur des infrastructures big data informatiquement complexes ;\nil est amené à interagir avec des profils informatiques pour déployer ses modèles et les rendre accessible à des utilisateurs.\n\nAussi, il fait sens pour le data scientist moderne de s’intéresser aux bonnes pratiques en vigueur dans la communauté des développeurs. Bien entendu, celles-ci doivent être adaptées aux spécificités des projets basés sur des données. Faisons à présent un tour d’horizon des bonnes pratiques et des outils que nous verrons tout au long ce cours."
  },
  {
    "objectID": "chapters/introduction.html#contenu-du-cours",
    "href": "chapters/introduction.html#contenu-du-cours",
    "title": "Introduction",
    "section": "Contenu du cours",
    "text": "Contenu du cours\n\nVoir le code comme un outil de communication\nLa première bonne pratique à adopter est de considérer le code comme un outil de communication, et non simplement de manière fonctionnelle. Un code ne sert pas seulement à réaliser une tâche donnée, il a vocation à être diffusé, réutilisé, maintenu, que ce soit dans le contexte d’une équipe dans une organisation ou bien en open-source. Pour favoriser cette communication du code, des conventions ont été developpées en matière de qualité du code et de structuration des projets, qu’il est utile d’appliquer dans ses projets. Nous présentons ces conventions dans les chapitres Qualité du Code et Architecture des Projets.\nIl est pour les mêmes raisons indispensable d’appliquer les principes du contrôle de version, qui permettent une documentation en continu des projets, ce qui accroît fortement leur réutilisabilité et leur maintenabilité dans le temps. Nous présentons pour cela l’utilisation du logiciel Git dans le chapitre Versionner son code et travailler collaborativement avec Git.\n\n\nTravailler de manière collaborative\nLe data scientist, quel que soit son contexte de travail, est amené à travailler dans le cadre de projets en équipe. Cela implique de définir une organisation du travail ainsi que d’utiliser des outils permettant de collaborer sur un projet de manière efficace et sécurisée. Nous présentons une manière moderne de travailler collaborativement avec Git et GitHub dans le chapitre Versionner son code et travailler collaborativement avec Git.\n\n\nMaximiser la reproductibilité\nLe troisième pillier des bonnes pratiques discutées dans ce cours est la reproductibilité. Un projet est dit reproductible lorsque, avec le même code et les mêmes données, il est possible de reproduire les résultats obtenus. Notons bien que le problème de la reproductibilité est différent de celui de la réplicabilité. La réplicabilité est un concept scientifique, qui signifie qu’un même procédé expérimental donne des résultats analogues sur des jeux de données différents. La reproductibilité est un concept technique : elle ne signifie pas que le protocole expérimental est scientifiquement correct, mais qu’il a été spécifié et diffusé d’une manière qui permet à tous de reproduire les résultats obtenus.\nLa notion de reproductibilité est le fil rouge de ce cours : toutes les notions vues dans les différents chapitres y contribuent. Le fait de produire du code et des projets qui respectent les conventions communautaires, comme le fait d’utiliser le contrôle de version, contribuent à rendre le code plus lisible et documenté, et donc reproductible.\nIl faut néanmoins aller plus loin pour atteindre une véritable reproductibilité, et réfléchir à la notion d’environnement d’exécution. Un code n’est pas un objet autonome, il est toujours exécuté sur un environnement (ordinateur personnel, serveur, etc.), et ces environnements peuvent être très différents (système d’exploitation, librairies installées, contraintes de sécurité, etc.). C’est pourquoi il faut réfléchir à la portabilité de son code, i.e. sa capacité à s’exécuter de manière attendue sur différents environnements. Le chapitre Portabilité présente une série d’outils qui permettent d’accroître la portabilité d’un projet.\n\n\nFaciliter la mise en production\nPour qu’un projet de data science crée in fine de la valeur, il faut qu’il soit déployé sous une forme valorisable de sorte à toucher son public. Cela implique deux choses : - trouver le format de diffusion adapté, i.e. qui valorise au mieux les résultas obtenus auprès des utilisateurs potentiels ; - faire transitionner le projet de l’environnement dans lequel il a été développé vers une infrastructure de production, i.e. permettant un déploiement robuste de l’output du projet.\nDans le chapitre Déployer et valoriser son projet de data science, nous proposons des pistes permettant de répondre à ces deux besoins. Nous présentons un certain nombre de formats standards (API, application, rapport automatisé, site internet) qui permettent à un projet de data science d’être valorisé, ainsi que les outils modernes qui permettent de les produire. Nous détaillons ensuite les concepts essentiels du déploiement sur une infrastructure de production, et illustrons ces derniers par des exemples de déploiements dans un environnement cloud moderne.\nC’est en quelque sorte la récompense de l’application des bonnes pratiques : dès lors que l’on s’est donné la peine de produire un code et un projet appliquant des standards de qualité, que l’on a bien versionné son code, et que l’on a pris des mesures pour le rendre portable, le déploiement du projet dans un environnement de production s’en trouve largement facilité.\n\n\nOutils supplémentaires\nPlusieurs outils présentés tout au long de ce cours, tels que les logiciels Git et Docker, impliquent l’utilisation du terminal ainsi que des connaissances de base du fonctionnement d’un système Linux. Dans le chapitre Démystifier le terminal Linux pour gagner en autonomie, nous présentons les connaissances essentielles des systèmes Linux qu’un data scientist doit posséder pour pouvoir être autonome dans ses déploiements et dans l’application des bonnes pratiques de développement.\nLa reproductibilité étant une quête sans fin, nous concluons ce cours par un chapitre nommé Des ressources pour aller plus loin dans l’industrialisation de son projet. Comme son nom l’indique, il vise à pointer vers un certain nombre de ressources qui permettent d’améliorer encore et toujours ses pratiques et de s’intéresser à des sujets qui dépassent le cadre de ce cours, comme la sécurité ou encore les spécificités liées au déploiement et à la maintenance de modèles de machine learning."
  },
  {
    "objectID": "chapters/introduction.html#un-continuum-de-bonnes-pratiques",
    "href": "chapters/introduction.html#un-continuum-de-bonnes-pratiques",
    "title": "Introduction",
    "section": "Un continuum de bonnes pratiques",
    "text": "Un continuum de bonnes pratiques\nLa notion de bonnes pratiques ne doit pas être vue de manière binaire : il n’y a pas d’un côté les projets qui les appliquent et de l’autre ceux qui ne les appliquent pas. Les bonnes pratiques ont un coût, qu’il ne faut pas négliger — même si leur application évite aussi des coûts futurs, notamment en terme de maintenance. Il faut donc plutôt voir les bonnes pratiques comme un spectre, sur lequel on vient positionner son projet en fonction de différents critères.\n\nComment fixer le bon seuil ?\nLa détermination du seuil pertinent doit résulter d’un arbitrage entre différents critères liés au projet : - ambitions : le projet est-il amené à évoluer, prendre de l’ampleur ? Est-il destiné à devenir collaboratif, que ce soit dans le cadre d’une équipe en organisation ou bien en open-source ? Les outputs du projet ont-ils vocation à être diffusés au grand public ? - ressources : quels sont les moyens humain du projet ? Pour un projet open-source, existe-t-il une communauté potentiel de contributeurs ? - contraintes : le projet a-t-il une échéance proche ? Des exigences de qualité ont-elles été fixées ? Est-il destiné à la mise en production ? Existe-t-il des enjeux de sécurité forts ?\nIl n’est donc pas question pour nous de suggérer que tout projet de data science doit respecter toutes les bonnes pratiques présentées dans ce cours.\n\n\nUn socle minimal pour les projets de data science ?\nCela étant dit, nous sommes convaincus qu’il est important pour tout data scientist de réfléchir à ces questions pour améliorer ces pratiques au fil du temps. En particulier, nous pensons qu’il est possible de définir un socle, i.e. un ensemble minimal de bonnes pratiques qui apportent plus d’avantages qu’elles ne coûtent à implémenter. Notre suggestion pour un tel socle est la suivante :\n\ncontrôler la qualité de son code en utilisant des outils dédiés (cf. chapitre Qualité du Code) ;\nadopter une structure de projet standardisée en utilisant des templates prêts à l’emploi (cf. chapitre Architecture des Projets) ;\nutiliser Git pour versionner le code de ses projets, qu’ils soient individuels ou collectifs (cf. chapitre Versionner son code et travailler collaborativement avec Git) ;\ncontrôler les dépendances de son projet en développant dans des environnements virtuels (cf. chapitre Portabilité)."
  },
  {
    "objectID": "chapters/introduction.html#approche-pédagogique",
    "href": "chapters/introduction.html#approche-pédagogique",
    "title": "Introduction",
    "section": "Approche pédagogique",
    "text": "Approche pédagogique\nLe parti pris de ce cours est que seule la pratique, et en particulier la confrontation à des problèmes issus de projets réels, permet d’acquérir efficacement des concepts informatiques. Aussi, une large part du cours consistera en l’application des notions étudiées à des cas concrets. Chaque chapitre se concluera pas des applications touchant à des sujets réalistes de data science.\nPour l’évaluation générale du cours, l’idée sera de partir d’un projet personnel, idéalement terminé, et de lui appliquer un maximum de bonnes pratiques présentées dans ce cours."
  },
  {
    "objectID": "chapters/introduction.html#langages",
    "href": "chapters/introduction.html#langages",
    "title": "Introduction",
    "section": "Langages",
    "text": "Langages\nLes principes présentés dans ce cours sont pour la plupart agnostiques du langage de programmation utilisé. Ce choix n’est pas qu’éditorial, c’est selon nous un aspect fondamental du sujet des bonnes pratiques. Trop souvent, des différences de langage entre les phases de développement (ex : R, Python) et de mise en production (ex : Java) érigent des murs artificiels qui réduisent fortement la capacité à valoriser des projets de data science. A l’inverse, plus les différentes équipes qui forment le cycle de vie d’un projet s’accordent pour appliquer le même ensemble de bonnes pratiques, plus ces équipes développent un langage commun, et plus les déploiements en sont facilités. Un exemple parlant est l’utilisation de la conteneurisation : si le data scientist met à disposition une image Docker comme output de sa phase de développement et que le data engineer s’occupe de déployer cette image sur une infrastructure dédiée, le contenu même de l’application en termes de langage importe finalement assez peu. Cet exemple, certes simpliste, illustre malgré tout l’enjeu des bonnes pratiques en matière de communication au sein d’un projet.\nLes exemples présentés dans ce cours seront pour l’essentiel en Python et en R. La principale raison et que ces langages sont enseignés dans la majorité des cursus de data science. Encore une fois, il est tout à fait possible d’appliquer les mêmes principes avec d’autres langages, et nous encourageons les étudiants à s’essayer à cet exercice formateur."
  },
  {
    "objectID": "chapters/introduction.html#environnement-dexécution",
    "href": "chapters/introduction.html#environnement-dexécution",
    "title": "Introduction",
    "section": "Environnement d’exécution",
    "text": "Environnement d’exécution\nA l’instar du langage, les principes appliqués dans ce cours sont agnostiques à l’infrastructure utilisée pour faire tourner les exemples proposés. Il est donc à la fois possible et souhaitable d’appliquer les bonnes pratiques aussi bien à un projet individuel développé sur un ordinateur personnel qu’à un projet collaboratif visant à être déployé sur une infrastructure de production dédiée.\nCependant, nous choisissons comme environnement de référence tout au long de ce cours le SSP Cloud, une plateforme de services pour la data science développée à l’Insee et accessible aux élèves des écoles statistiques. Les raisons de ce choix sont multiples :\n\nl’environnement de développement est normalisé : les serveurs du SSP Cloud ont une configuration homogène — notamment, ils se basent sur une même distribution Linux (Debian) — ce qui garantit la reproductibilité des exemples présentés tout au long du cours ;\nvia un cluster Kubernetes sous-jacent, le SSP Cloud met à disposition une infrastructure robuste permettant le déploiement automatisé d’applications potentiellement intensives en données, ce qui permet de simuler un véritable environnement de production ;\nle SSP Cloud est construit selon les standards les plus récents des infrastructures data science, et permet donc d’acquérir les bonnes pratiques de manière organique :\n\nles services sont lancés via des conteneurs, configurés par des images Docker. Cela permet de garantir une forte reproductibilité des déploiements, au prix d’une phase de développement un peu plus coûteuse ;\nle SSP Cloud est basé sur une approche dite cloud native : il est construit sur un ensemble modulaire de briques logicielles, qui permettent d’appliquer une séparation nette du code, des données, de la configuration et de l’environnement d’exécution, principe majeur des bonnes pratiques qui reviendra tout au long de ce cours."
  },
  {
    "objectID": "chapters/linux-101.html",
    "href": "chapters/linux-101.html",
    "title": "Démystifier le terminal Linux pour gagner en autonomie",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des systèmes d’exploitation. Mais comme il a la réputation d’être austère et complexe, on utilise plutôt des interfaces graphiques pour effectuer nos opérations informatiques quotidiennes.\nPourtant, avoir des notions quant à l’utilisation d’un terminal est une vraie source d’autonomie, dans la mesure où celui-ci permet de gérer bien plus finement les commandes que l’on réalise. Pour le data scientist qui s’intéresse aux bonnes pratiques et à la mise en production, sa maîtrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont généralement limitées par rapport à l’utilisation du programme en ligne de commande. C’est par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de réaliser toutes les opérations permises par le logiciel ;\nmettre un projet de data science en production nécessite d’utiliser un serveur, qui le rend disponible en permanence à son public potentiel. Or là où Windows domine le monde des ordinateurs personnels, une large majorité des serveurs et des infrastructures cloud fonctionnent sous Linux.\nplus généralement, une utilisation régulière du terminal est source d’une meilleure compréhension du fonctionnement d’un système de fichiers et de l’exécution des processus sur un ordinateur. Ces connaissances s’avèrent très utiles dans la pratique quotidienne du data scientist, qui nécessite de plus en plus de développer dans différents environnements d’exécution.\n\nDans le cadre de ce cours, on s’intéressera donc particulièrement au terminal Linux.\n\n\n\nDifférents environnements de travail peuvent être utilisés pour apprendre à se servir d’un terminal Linux :\n\nle SSP Cloud. Dans la mesure où les exemples de mise en production du cours seront illustrées sur cet environnement, nous recommandons de l’utiliser dès à présent pour se familiariser. Le terminal est accessible à partir de différents services (RStudio, Jupyter, etc.), mais nous recommandons d’utiliser le terminal d’un service VSCode, dans la mesure où se servir d’un IDE pour organiser notre code est en soi déjà une bonne pratique ;\nKatacoda, un bac à sable dans un système Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (émulation minimaliste d’un terminal Linux), qui est installée par défaut avec Git.\n\n\n\n\nLançons un terminal pour présenter son fonctionnement basique. On prend pour exemple le terminal d’un service VSCode lancé via le SSP Cloud (Application Menu tout en haut à gauche de VSCode -> Terminal -> New Terminal). Voici à quoi ressemble le terminal en question.\n\nDécrivons d’abord les différentes inscriptions qui arrivent à l’initialisation : - (base) : cette inscription n’est pas directement liée au terminal, elle provient du fait que l’on utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en détail dans le chapitre sur la portabilité ; - coder@vscode-824991-64744dd6d8-zbgv5 : le nom de l’utilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l’on verra là encore dans le chapitre sur la portabilité - ~/work : le chemin du répertoire courant, i.e. à partir duquel va être lancée toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\nPour éviter la lourdeur des images et permettre de copier/coller facilement les commandes, on représentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l’exemple suivant. Les lignes commençant par un $ sont celles avec lesquelles une commande est lancée, et les lignes sans $ représentent le résultat d’une commande. Attention à ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement à différencier celles-ci des résultats.\n$ echo \"une petite illustration\"\nune petite illustration\n\n\n\nLe terme filesystem (système de fichiers) désigne la manière dont sont organisés les fichiers au sein d’un système d’exploitation. Cette structure est hiérarchique, en forme d’arbre : - elle part d’un répertoire racine (le dossier qui contient tous les autres) ; - contient des dossiers ; - les dossiers peuvent contenir à leur tout des dossiers (sous-dossiers) ou des fichiers.\nIntéressons nous à la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations : - la racine (root) sur Linux s’appelle /, là où elle s’appelle C:\\ par défaut sur Windows ; - le répertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un rôle essentiellement technique. Il est tout de même utile d’en décrire les principaux : - /bin : contient les binaires, i.e. les programmes exécutables ; - /etc : contient les fichiers de configuration ; - /home : contient l’ensemble des dossiers et fichiers personnels des différents utilisateurs. Chaque utilisateur a un répertoire dit “HOME” qui a pour chemin /home/<username> Ce répertoire est souvent représenté par le symbole ~. C’était notamment le cas dans l’illustration du terminal VSCode ci-dessus, ce qui signifie qu’on se trouvait formellement dans le répertoire /home/coder/work, coder étant l’utilisateur par défaut du service VSCode sur le SSP Cloud.\nChaque dossier ou fichier est représenté par un chemin d’accès, qui correspond simplement à sa position dans le filesystem. Il existe deux moyens de spécifier un chemin : - en utilisant un chemin absolu, c’est à dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconnaît donc un chemin absolu par le fait qu’il commence forcément par /. - en utilisant un chemin relatif, c’est à dire en indiquant le chemin du dossier ou fichier relativement au répertoire courant.\nComme tout ce qui touche de près ou de loin au terminal, la seule manière de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d’appliquer ces concepts à des cas pratiques.\n\n\n\nLe rôle d’un terminal est de lancer des commandes. Ces commandes peuvent être classées en trois grandes catégories : - navigation au sein du filesystem - manipulations de fichiers (créer, lire, modifier des dossiers/fichiers) - lancement de programmes\n\n\nLorsque l’on lance un programme à partir du terminal, celui-ci a pour référence le répertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l’on exécute un script Python en se trouvant dans un certain répertoire, tous les chemins des fichiers utilisés dans le script seront relatifs au répertoire courant d’exécution — à moins d’utiliser uniquement des chemins absolus, ce qui n’est pas une bonne pratique en termes de reproductibilité puisque cela lie votre projet à la structure de votre filesystem particulier.\nAinsi, la très grande majorité des opérations que l’on est amené à réaliser dans un terminal consiste simplement à se déplacer au sein du filesystem. Les commandes principales pour naviguer et se repérer dans le filesystem sont présentées dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez pénible de manipuler des chemins absolus, qui peuvent facilement être très longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient à se déplacer à partir du répertoire courant. Pour se faire, voici quelques utilisations très fréquentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d’un niveau dans l’arborescence (dossier parent)\n\n\ncd ~\nrevenir dans le répertoire HOME de l’utilisateur courant\n\n\n\nLa première commande est l’occasion de revenir sur une convention d’écriture importante pour les chemins relatifs : - . représente le répertoire courant. Ainsi, cd . revient à changer de répertoire courant… pour le répertoire courant, ce qui bien sûr ne change rien. Mais le . est très utile pour la copie de fichiers (cf. section suivante) ou encore lorsque l’on doit passer des paramètres à un programme (cf. section Lancement de programmes) ; - .. représente le répertoire parent du répertoire courant.\nCes différentes commandes constituent la très grande majorité des usages dans un terminal. Il est essentiel de les pratiquer jusqu’à ce qu’elles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d’autres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\ndéplacer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncréer (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncréer un fichier vide\n\n\n\nDans la mesure où il est généralement possible de réaliser toutes ces opérations à l’aide d’interfaces graphiques (notamment, l’explorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se déplacer dans le filesystem. Nous vous recommandons malgré tout de les pratiquer également, et ce pour plusieurs raisons : - effectuer un maximum d’opérations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ; - en devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ; - lorsque l’on est amené à manipuler un terminal pour interagir avec un serveur, il n’y a souvent pas la moindre interface graphique, auquel cas il n’y a pas d’autre choix que d’opérer uniquement à partir du terminal.\n\n\n\nLe rôle du terminal est de lancer des programmes. Lancer un programme se fait à partir d’un fichier dit exécutable, qui peut être de deux formes : - un binaire, i.e. un programme dont le code n’est pas lisible par l’humain ; - un script, i.e. un fichier texte contenant une série d’instructions à exécuter. Le langage du terminal Linux est le shell, et les scripts associés ont pour extension .sh.\nDans les deux cas, la syntaxe de lancement d’une commande est : le nom de l’exécutable, suivi d’éventuels paramètres, séparés par des espaces. Par exemple, la commande python monscript.py exécute le binaire python et lui passe comme unique argument le nom d’un script .py (contenu dans le répertoire courant), qui va donc être exécuté via Python. De la même manière, toutes les commandes vues précédemment pour se déplacer dans le filesystem ou manipuler des fichiers sont des exécutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier à copier et le chemin d’arrivée.\nDans les exemples de commandes précédents, les paramètres étaient passés en mode positionnel : l’exécutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n’est pas toujours fixé à l’avance, du fait de la présence de paramètres optionnels. Ainsi, la plupart des exécutables permettent le passage d’arguments optionnels, qui modifient le comportement de l’exécutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier à un autre endroit du filesystem, mais peut-on copier un dossier et l’ensemble de son contenu avec ? Nativement non, mais l’ajout d’un paramètre le permet : cp -R dossierdepart dossierarrivee permet de copier récursivement le dossier et tout son contenu. Notons que les flags ont très souvent un équivalent en toute lettre, qui s’écrit quant à lui avec deux tirers. Par exemple, la commande précédente peut s’écrire de manière équivalente cp --recursive dossierdepart dossierarrivee. Il est fréquent de voir les deux syntaxes en pratique, parfois même mélangées au sein d’une même commande.\n\n\n\n\nComme tout langage de programmation, le langage shell permet d’assigner et d’utiliser des variables dans des commandes. Pour afficher le contenu d’une variable, on utilise la commande echo, qui est l’équivalent de la fonction print en Python ou en R.\n$ MY_VAR=\"toto\"\n$ echo $MY_VAR\ntoto\nQuelques remarques importantes : - la syntaxe pour la création de variable est précise : aucun espace d’un côté comme de l’autre du = ; - en Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu écrire MY_VAR=toto pour le même résultat. Par contre, si l’on veut assigner à une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d’erreur ; - pour accéder à la valeur d’une variable, on la préfixe d’un $.\nNotre objectif avec ce tutoriel n’est pas de savoir coder en shell, on ne va donc pas s’attarder sur les propriétés des variables. En revanche, introduire ce concept était nécessaire pour en présenter un autre, essentiel quant à lui dans la pratique quotidienne du data scientist : les variables d’environnement. Pour faire une analogie — un peu simpliste — avec les langages de programmation, ce sont des sortes de variables “globales”, dans la mesure où elles vont être accessibles à tous les programmes lancés à partir d’un terminal, et vont modifier leur comportement.\nLa liste des variables d’environnement peut être affichée à l’aide de la commande env. Il y a généralement un grand nombre de variables d’environnement prééxistantes ; en voici un échantillon obtenu à partir du terminal du service VSCode.\n$ env\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la variété des utilisations des variables d’environnements : - la variable $SHELL précise l’exécutable utilisé pour lancer le terminal ; - la variable $HOME donne l’emplacement du répertoire utilisateur. En fait, le symbole ~ que l’on a rencontré plus haut référence cette même variable ; - la variable LANG spécifie la locale, un concept qui permet de définir la langue et l’encodage utilisés par défaut par Linux ; - la variable CONDA_PYTHON_EXE existe uniquement parce que l’on a installé conda comme système de gestion de packages Python. C’est l’existance de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous intéresse.\nUne variable d’environnement essentielle, et que l’on est fréquemment amené à modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concaténation de chemins absolus, séparés par :, qui spécifie les dossiers dans lesquels Linux va chercher les exécutables lorsque l’on lance une commande, ainsi que l’ordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n$ echo $PATH\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL’ordre de recherche est de gauche à droite. C’est donc parce que le dossier /home/coder/local/bin/conda/bin est situé en premier que l’interpréteur Python qui sera choisi lorsque l’on lance un script Python est celui issu de Conda, et non celui contenu par défaut dans /usr/bin par exemple.\nL’existence et la configuration adéquate des variables d’environnement est essentielle pour le bon fonctionnement de nombreux outils très utilisés en data science, comme Git ou encore Spark par exemple. Il est donc nécessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d’un serveur en cas de bug lié à une variable d’environnement manquante ou mal configurée.\n\n\n\nLa sécurité est un enjeu central en Linux, qui permet une gestion très fine des permissions sur les différents fichiers et programmes.\nUne différence majeure par rapport à d’autres systèmes d’exploitation, notamment Windows, est qu’aucun utilisateur n’a par défaut les droits complets d’administrateur (root). Il n’est donc pas possible nativement d’accéder au parties sensibles du système, ou bien de lancer certains types de programme. Par exemple, si l’on essaie de lister les fichiers du dossier /root, on obtient une erreur.\n$ ls /root\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines opérations telles que l’installation de binaires ou de packages nécessitent cependant des droits administrateurs. Dans ce cas, il est d’usage d’utiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l’exécution de la commande.\n$ sudo ls /root\nLe dossier /root étant vide, la commande ls renvoie une chaîne de caractères vide, mais nous n’avons plus de problème de permission. Notons qu’une bonne pratique de sécurité, en particulier dans les scripts shell que l’on peut être amenés à écrire ou exécuter, est de limiter l’utilisation de cette commande aux cas où elle s’avère nécessaire.\nUne autre subtilité concerne justement l’exécution de scripts shell. Par défaut, qu’il soit créé par l’utilisateur ou téléchargé d’internet, un script n’est pas exécutable.\n$ touch test.sh # Créer le script test.sh (vide)\n$ ./test.sh # Exécuter le script test.sh\nbash: ./test.sh: Permission denied\nC’est bien entendu une mesure de sécurité pour éviter l’exécution automatique de scripts potentiellement malveillants. Pour pouvoir exécuter un tel script, il faut attribuer des droits d’exécution au fichier avec la commande chmod. Il devient alors possible d’exécuter le script classiquement.\n$ chmod +x test.sh # Donner des droits d'exécution au script test.sh\n$ ./test.sh # Exécuter le script test.sh\n\n# Le script étant vide, il ne se passe rien\n\n\n\nMaintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell précédemment évoqués. A l’instar d’un script Python, un script shell permet d’automatiser une série de commandes lancées dans un terminal. Le but de ce tutoriel n’est pas de savoir écrire des scripts shell complexes, travail généralement dévolu aux les data engineers ou les sysadmin (administrateurs système), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces compétences sont essentielles lorsque l’on se préoccupe de mise en production. A titre d’exemple, comme nous le verrons dans le chapitre sur la portabilité, il est fréquent d’utiliser un script shell comme entrypoint d’une image docker, afin de spécifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement à l’aide d’un script simple. Considérons les commandes suivantes, que l’on met dans un fichier monscript.sh dans le répertoire courant.\n#!/bin/bash\nSECTION=$1\nCHAPTER=$2\nFORMATION_DIR=/home/coder/work/formation\nmkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\ntouch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\nAnalysons la structure de ce script : - la première ligne est classique, elle se nomme le shebang : elle indique au système quel interpréteur utiliser pour exécuter ce script. Dans notre cas, et de manière générale, on utilise bash (Bourne-Again SHell, l’implémentation moderne du shell) ; - les lignes 2 et 3 assignent à des variables les arguments passés au script dans la commande. Par défaut, ceux-ci sont assignés à des variables n où n est la position de l’argument, en commençant à 1 ; - la ligne 4 assigne un chemin à une variable - la ligne 5 crée le chemin complet, défini à partir des variables créées précédemment. Le paramètre -p est important : il précise à mkdir d’agir de manière récursive, c’est à dire de créer les dossiers intermédiaires qui n’existent pas encore ; - la ligne 6 crée un fichier texte vide dans le dossier créé avec la commande précédente.\nExécutons maintenant ce script, en prenant soin de lui donner les permission adéquates au préalable.\n$ chmod +x monscript.sh\n$ bash monscript.sh section2 chapitre3\n$ ls formation/section1/chapitre2/\ntext.txt\nOpération réussie : le dossier a bien été créé et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash très bien réalisée.\n\n\n\nUne différence fondamentale entre Linux et Windows tient à la manière dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier exécutable en .exe) sur le site du logiciel, et on l’exécute. En Linux, on passe généralement par un gestionnaire de packages qui va chercher les logiciels sur un répertoire centralisé, à la manière de pip en Python par exemple.\nPourquoi cette différence ? Une raison importante est que, contrairement à Windows, il existe une multitude de distributions différentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent différemment et peuvent avoir différentes versions. En utilisant le package manager (gestionnaire de paquets) propre à la distribution en question, on s’assure de télécharger le logiciel adapté à sa distribution. Dans ce cours, on fait le choix d’utiliser une distribution Debian et son gestionnaire de paquets associé apt. Debian est en effet un choix populaire pour les servers de part sa stabilité et sa simplicité, et sera également familière aux utilisateurs d’Ubuntu, distribution très populaire pour les ordinateurs personnels et qui est basée sur Debian.\nL’utilisation d’apt est très simple. La seule difficulté est de savoir le nom du paquet que l’on souhaite installer, ce qui nécessite en général d’utiliser un moteur de recherche. L’installation de paquets est également un cas où il faut utiliser sudo, puisque cela implique souvent l’accès à des répertoires protégés.\n$ sudo apt install tree\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nDésinstaller un package est également simple : c’est l’opération inverse. Par sécurité, le terminal vous demande si vous êtes sûr de votre choix en vous demandant de tapper la lettre y ou la lettre n. On peut passer automatiquement cette étape en ajoutant le paramètre -y\n$ sudo apt remove -y tree\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d’installer un package, il est toujours préférable de mettre à jour la base des packages, pour s’assurer qu’on obtiendra bien la dernière version.\n$ sudo apt update\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date.\n\n\n\nOn l’a dit et redit : devenir à l’aise avec le terminal Linux est essentiel et demande de la pratique. Il existe néanmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa première est l’autocomplétion. Dès lors que vous écrivez une commande contenant un nom d’exécutable, un chemin sur le filesystem, ou autre, n’hésitez pas à utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorité des cas, cela va vous faire gagner un temps précieux.\nUne seconde astuce, qui n’en est pas vraiment une, est de lire la documentation d’une commande lorsque l’on n’est pas sûr de sa syntaxe ou des paramètres admissibles. Via le terminal, la documentation d’une commande peut être affichée en exécutant man suivie de la commande en question, par exemple : man cp. Comme il n’est pas toujours très pratique de lire de longs textes dans un petit terminal, on peut également chercher la documentation d’une commande sur le site man7."
  },
  {
    "objectID": "chapters/portability.html",
    "href": "chapters/portability.html",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "",
    "text": "Dans les chapitres précédents, nous avons vu un ensemble de bonnes pratiques qui permettent de considérablement améliorer la qualité d’un projet : rendre le code plus lisible, adopter une structure du projet normalisée et évolutive, et versionner proprement son code sur un dépôt GitHub.\nUne fois ces bonnes pratiques appliquées à notre projet, ce dernier apparaît largement partageable. Du moins en théorie, car la pratique est souvent plus compliquée : il y a fort à parier que si vous essayez d’exécuter votre projet sur un autre environnement d’exécution (un autre ordinateur, un serveur, etc.), les choses ne se passent pas du tout comme attendu. Cela signifique qu’en l’état, le projet n’est pas portable : il n’est pas possible, sans modifications coûteuses, de l’exécuter dans un environnement différent de celui dans lequel il a été développé.\nLa principale raison est qu’un code ne vit pas dans une bulle isolée, il contient en général de nombreuses adhérences, plus ou moins visibles, au langage et à l’environnement dans lesquels il a été développé :\n\ndes dépendances dans le langage du projet ;\ndes dépendances dans d’autres langages (ex : NumPy est écrit en C et nécessite donc un compilateur C) ;\ndes librairies systèmes nécessaires pour installer certains packages (par exemple, : les librairies de cartographie dynamique comme Leaflet ou Folium nécessitent la librairie système GDAL), qui ne seront pas les mêmes selon le système d’exploitation utilisé.\n\nSi le premier problème peut être géré relativement facilement en adoptant une structure de projet et en spécifiant bien les différentes dépendances utilisées, les deux autres nécessitent en général des outils plus avancés.\nCes outils vont nous permettre de normaliser l’environnement afin de produire un projet portable, i.e. exécutable sur une large variété d’environnements d’exécution. Cette étape est primordiale lorsque l’on se préoccupe de la mise en production d’un projet, car elle assure une transition relativement indolore entre l’environnement de développement et celui de production.\n\nImage empruntée à https://devrant.com/rants/174386/when-i-say-but-it-works-on-my-machine"
  },
  {
    "objectID": "chapters/portability.html#introduction",
    "href": "chapters/portability.html#introduction",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nPour illustrer l’importance de travailler avec des environnements virtuels, mettons-nous à la place d’un.e aspirant data scientist qui commencerait ses premiers projets. Selon toute vraisemblance, il va commencer par installer une distribution de Python — souvent, via Anaconda — sur son poste et commencer à développer, projet après projet. Dans cette approche, les différents packages qu’il va être amené à utiliser vont être installés au même endroit. Cela pose plusieurs problèmes : - conflits de version : une application A peut dépendre de la version 1 d’un package là où une application B peut dépendre de la version 2 de ce même package. Une seule application peut donc fonctionner dans cette configuration ; - version de Python fixe — on ne peut avoir qu’une seule installation par système — là où on voudrait pouvoir avoir des versions différentes selon le projet ; - reproductiblité limitée : difficile de dire quel projet repose sur tel package, dans la mesure où ceux-ci s’accumulent en un même endroit au fil des projets ; - portabilité limitée : conséquence du point précédent, il est difficile de fixer dans un fichier les dépendances spécifiques à un projet.\nLes environnements virtuels constituent une solution à ces différents problèmes."
  },
  {
    "objectID": "chapters/portability.html#fonctionnement",
    "href": "chapters/portability.html#fonctionnement",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLe concept d’environnement virtuel est techniquement très simple. On peut lui donner la définition suivante pour Python :\n\n“dossier auto-suffisant qui contient une installation de Python pour une version particulière de Python ainsi que des packages additionnels et qui est isolé des autres environnements existants.”\n\nOn peut donc simplement voir les environnements virtuels comme un moyen de faire cohabiter sur un même système différentes installations de Python avec chacune leur propre liste de packages installés et leurs versions. Développer dans des environnements virtuels vierges à chaque début de projet est une très bonne pratique pour accroître la reproductibilité des analyses."
  },
  {
    "objectID": "chapters/portability.html#implémentations",
    "href": "chapters/portability.html#implémentations",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Implémentations",
    "text": "Implémentations\nIl existe différentes implémentations des environnements virtuels en Python, dont chacune ont leurs spécificités et leur communauté d’utilisateurs :\n\nL’implémentation standard en Python est venv.\nDans le domaine de la data science, l’implémentation la plus courante est sans doute conda.\n\nEn pratique, ces implémentations sont relativement proches. La différence majeure est que conda est à la fois un package manager (comme pip) et un gestionnaire d’environnements virtuels (comme venv).\nPendant longtemps, conda en tant que package manager s’est avéré très pratique en data science, dans la mesure où il gérait non seulement les dépendances Python mais aussi dans d’autres langages — comme des dépendances C. Par ailleurs, la distribution Anaconda, qui contient à la fois Python, conda et beaucoup de packages utiles pour la data science, explique également cette popularité auprès des data scientists.\nPour toutes ces raisons, nous allons présenter l’utilisation de conda comme gestionnaire d’environnements virtuels. Les principes présentés restent néanmoins valides pour les autres implémentations"
  },
  {
    "objectID": "chapters/portability.html#conda",
    "href": "chapters/portability.html#conda",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Conda",
    "text": "Conda\n\nInstallation\nLes instructions à suivre pour installer conda sont détaillées dans la documentation officielle. conda seul étant peu utile en pratique, il est généralement installé dans le cadre de distributions. Les deux plus populaires sont : - Miniconda : une distribution minimaliste contenant conda, Python ainsi qu’un petit nombre de packages techniques très utiles ; - Anaconda : une distribution assez volumineuse contenant conda, Python, d’autres logiciels (R, Spyder, etc.) ainsi qu’un ensemble de packages utiles pour la data science (SciPy, NumPy, etc.).\nLe choix de la distribution importe assez peu en pratique, dans la mesure où nous allons de toute manière utiliser des environnements virtuels vierges pour développer nos projets.\nL’écosystème Conda\n\n\n\nEn pratique\n\nCréer un environnement\nPour commencer à utiliser conda, commençons par créer un environnement vierge, nommé dev, en spécifiant la version de Python que l’on souhaite installer pour notre projet.\n$ conda create -n dev python=3.9.7\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/coder/local/bin/conda/envs/dev\n\n  added / updated specs:\n    - python=3.9.7\n\n\nThe following packages will be downloaded:\n...\nThe following NEW packages will be INSTALLED:\n...\nProceed ([y]/n)? y\nDownloading and Extracting Packages\n...\nComme indiqué dans les logs, Conda a créé notre environnement et nous indique son emplacement sur le filesystem. En réalité, l’environnement n’est jamais vraiment vierge : Conda nous demande — et il faut répondre oui en tapant “y” — d’installer un certain nombre de packages, qui sont ceux qui viennent avec la distribution Miniconda.\nOn peut vérifier que l’environnement a bien été créé en listant les environnements installés sur le système.\nconda info --envs\n# conda environments:\n#\nbase                    * /home/coder/local/bin/conda\nbasesspcloud              /home/coder/local/bin/conda/envs/basesspcloud\ndev                       /home/coder/local/bin/conda/envs/dev\n\n\nActiver un environnement\nComme plusieurs environnements peuvent coexister sur un même système, il faut spécifier à Conda que l’on souhaite utiliser cet environnement pour la session courante du terminal.\n$ conda activate dev\nConda nous indique que l’on travaille à partir de maintenant dans l’environnement dev en indiquant son nom entre parenthèses au début de la ligne de commandes. Autrement dit, dev devient pour un temps notre environnement par défaut. Pour s’en assurer, vérifions avec la commande which l’emplacement de l’interpréteur Python qui sera utilisé si on lance une commande du type python mon-script.py.\n(dev) $ which python \n/home/coder/local/bin/conda/envs/dev/bin/python\nOn travaille bien dans l’environnement attendu : l’interpréteur qui se lance n’est pas celui du système global, mais bien celui spécifique à notre environnement virtuel.\n\n\nLister les packages installés\nUne fois l’environnement activé, on peut lister les packages installés et leur version. Cela confirme qu’un certain nombre de packages sont installés par défaut lors de la création d’un environnement virtuel.\n(dev) $ conda list\n# packages in environment at /home/coder/local/bin/conda/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \n_openmp_mutex             4.5                       1_gnu  \nca-certificates           2022.3.29            h06a4308_0  \n...\n\n\nInstaller un package\nLa syntaxe pour installer un package avec Conda est très similaire à celle de pip :\nconda install nom_du_package\nLa différence est que là où pip install va installer un package à partir du répertoire PyPI, conda install va chercher le package sur les répertoires maintenus par les développeurs de Conda1. Installons par exemple le package phare de machine learning scikit-learn.\n(dev) $ conda install scikit-learn\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/coder/local/bin/conda/envs/dev\n\n  added / updated specs:\n    - scikit-learn\n...\nLà encore, Conda nous demande d’installer d’autres packages, qui sont des dépendances de scikit-learn. Par exemple, la librairie de calcul scientifique NumPy.\nL’autre différence majeure avec pip est que Conda utilise une méthode plus avancée — et donc également plus coûteuse en temps — de résolution des dépendances. En effet, différents packages peuvent spécifier différentes versions d’un même package dont ils dépendent tous les deux, ce qui provoque un conflit de version. Conda va par défaut appliquer un algorithme qui vise à gérer au mieux ces conflits, là où pip va choisir une approche plus minimaliste2.\nIl arrive que des packages disponibles sur le répertoire PyPI ne soient pas disponible sur les canaux gérés par Conda. Dans ce cas, il est possible d’installer un package dans l’environnement via la commande pip install. Il est néanmonins toujours préférable de privilégier une installation via Conda si disponible.\n\n\nExporter les spécifications de l’environnement\nDévelopper à partir d’un environnement vierge est une bonne pratique de reproductibilité : en partant d’une base minimale, on s’assure que seuls les packages effectivement nécessaires au bon fonctionnement de notre application ont été installés au fur et à mesure du projet.\nCela rend également notre projet plus portable : on peut exporter les spécifications de l’environnement (version de Python, canaux de téléchargement des packages, packages installés et leurs versions) dans un fichier, appelé par convention environment.yml.\n(dev) $ conda env export > environment.yml\nCe fichier est mis par convention à la racine du dépôt Git du projet. Ainsi, les personnes souhaitant tester l’application peuvent recréer le même environnement Conda que celui qui a servi au développement via la commande suivante.\n$ conda env create -f environment.yml\n\n\nChanger d’environnement\nPour changer d’environnement, il suffit d’en activer un autre.\n(dev) $ conda base\n(base) $ \nPour sortir de tout environnement Conda, on utilise la commande conda deactivate :\n(base) $ conda deactivate\n$ \n\n\nSupprimer un environnement\nPour supprimer l’environnement dev, on utilise la commande conda env remove -n dev.\n\n\n\nAide-mémoire\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nconda create -n <env_name> python=<python_version>\nCréation d’un environnement nommé <env_name> dont la version de Python est <python_version>\n\n\nconda info --envs\nLister les environnements\n\n\nconda activate <env_name>\nUtiliser l’environnement <env_name> pour la session du terminal\n\n\nconda list\nLister les packages dans l’environnement actif\n\n\nconda install <pkg>\nInstaller le package <pkg> dans l’environnement actif\n\n\nconda env export > environment.yml\nExporter les spécifications de l’environnement dans un fichier environment.yml"
  },
  {
    "objectID": "chapters/portability.html#limites",
    "href": "chapters/portability.html#limites",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Limites",
    "text": "Limites\nDévelopper dans des environnements virtuels est une bonne pratique, car cela accroît la portabilité d’une application. Néanmoins, il y a plusieurs limites à leur utilisation : - les librairies système nécessaires à l’installation des packages ne sont pas gérées ; - les environnements virtuels ne permettent pas toujours de gérer des projets faisant intervenir différents langages de programmation ; - devoir installer conda, Python, et les packages nécessaires à chaque changement d’environnement peut être assez long et pénible en pratique ; - dans un environnement de production, gérer des environnements virtuels différents pour chaque projet peut s’avérer rapidement complexe pour les administrateurs système.\nLa technologie des conteneurs permet de répondre à ces différents problèmes."
  },
  {
    "objectID": "chapters/portability.html#introduction-1",
    "href": "chapters/portability.html#introduction-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nAvec les environnements virtuels, l’idée était de permettre à chaque utilisateur potentiel de notre projet d’installer sur son environnement d’exécution les packages nécessaires à la bonne exécution du projet. Néanmoins, comme on l’a vu, cette approche ne garantit pas une reproductibilité parfaite et a l’inconvénient de nécessiter beaucoup de gestion manuelle.\nChangeons de perspective : au lieu de distribuer une recette permettant à l’utilisateur de recréer l’environnement nécessaire sur sa machine, ne pourrait-on pas directement distribuer à l’utilisateur une machine contenant l’environnement pré-configuré ? Bien entendu, on ve pas configurer et envoyer des ordinateurs portables à tous les utilisateurs potentiels d’un projet. Une autre solution serait de distribuer des machines virtuelles, qui tournent sur un serveur et simulent un véritable ordinateur. Ces machines ont cependant l’inconvénient d’être assez lourdes, et complexes à répliquer et distribuer. Pour pallier ces différentes limites, on va utiliser la technologie des conteneurs.\n\nImage trouvée sur reddit"
  },
  {
    "objectID": "chapters/portability.html#fonctionnement-1",
    "href": "chapters/portability.html#fonctionnement-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nComme les machines virtuelles, les conteneurs permettent d’empaqueter complètement l’environnement (librairies systèmes, application, configuration) qui permet de faire tourner l’application. Mais à l’inverse d’une machine virtuelle, le conteneur n’inclut pas de système d’exploitation propre, il utilise celui de la machine hôte qui l’exécute. La technologie des conteneurs permet ainsi de garantir une très forte reproductibilité tout en restant suffisamment légère pour permettre une distribution et un déploiement simple aux utilisateurs.\nDifférences entre l’approche conteneurs (gauche) et l’approche machines virtuelles (droite)\n\nSource : docker.com"
  },
  {
    "objectID": "chapters/portability.html#implémentations-1",
    "href": "chapters/portability.html#implémentations-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Implémentations",
    "text": "Implémentations\nComme pour les environnements virtuels, il existe différentes implémentations de la technologie des conteneurs. En pratique, l’implémentation offerte par Docker est devenue largement prédominante, au point qu’il est devenu courant d’utiliser de manière interchangeable les termes “conteneuriser” et “Dockeriser” une application. C’est donc cette implémentation que nous allons étudier et utiliser dans ce cours."
  },
  {
    "objectID": "chapters/portability.html#docker",
    "href": "chapters/portability.html#docker",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Docker ",
    "text": "Docker \n\nInstallation\nLes instructions à suivre pour installer Docker  selon son système d’exploiration sont détaillées dans la documentation officielle. Il existe également des environnements bacs à sable en ligne comme Play with Docker.\n\n\nPrincipes\nUn conteneur Docker est mis à disposition sous la forme d’une image, c’est à dire d’un fichier binaire qui contient l’environnement nécessaire à l’exécution de l’application.\nPour construire (build) l’image, on utilise un Dockerfile, un fichier texte qui contient la recette — sous forme de commandes Linux — de construction de l’environnement. L’image va être uploadée (push) sur un dépôt (registry), public ou privé, depuis lequel les utilisateurs vont pouvoir télécharger l’image (pull). Le moteur Docker permet ensuite de lancer (run) un conteneur, c’est à dire une instance vivante de l’image.\n{{% box status=“note” title=“Note: les commandes Docker” icon=“fa fa-comment” %}} Le répertoire d’images publiques le plus connu est DockerHub. Il s’agit d’un répertoire où n’importe qui peut proposer une image Docker, associée ou non à un projet disponible sur Github  ou Gitlab . Il est possible de mettre à disposition de manière manuelle des images mais, comme nous le montrerons, il est beaucoup plus pratique d’utiliser des fonctionalités d’interaction automatique entre DockerHub et un dépôt Git. {{% /box %}}\n\n\nEn pratique\n\nApplication\nAfin de présenter l’utilisation de Docker en pratique, nous allons présenter les différentes étapes permettant de “dockeriser” une application web minimaliste construite avec le framework Python Flask3.\nLa structure de notre projet est la suivante.\n├── myflaskapp\n│   ├── Dockerfile\n│   ├── hello-world.py\n│   └── requirements.txt\nLe script hello-world.py contient le code d’une application minimaliste, qui affiche simplement “Hello, World!” sur une page web.\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef hello_world():\n    return \"<p>Hello, World!</p>\"\nPour faire tourner l’application, il nous faut donc à la fois Python et le package Flask. Ces installations doivent être spécifiées dans le Dockerfile (cf. section suivante). L’installation de Flask se fait via un fichier requirements.txt, qui contient juste la ligne suivante :\nFlask==2.1.1\n\n\nLe Dockerfile\nA là base de chaque image Docker se trouve un Dockerfile. C’est un fichier texte qui contient une série de commandes qui permettent de construire l’image. Ces fichiers peuvent être plus ou moins complexes selon l’application que l’on cherche à conteneuriser, mais leur structure est assez normalisée. Pour s’en rendre compte, analysons ligne à ligne le Dockerfile nécessaire pour construire une image Docker de notre application Flask.\nFROM ubuntu:20.04\n\nRUN apt-get update -y && \\\n    apt-get install -y python3-pip python3-dev\n    \nWORKDIR /app\n\nCOPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY . /app\n\nENV FLASK_APP=\"hello-world.py\"\nEXPOSE 5000\n\nCMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n\nFROM : spécifie l’image de base. Une image Docker hérite toujours d’une image de base. Ici, on choisit l’image Ubuntu version 20.04, tout va donc se passer comme si l’on développait sur une machine virtuelle vierge ayant pour système d’exploitation Ubuntu 20.044 ;\nRUN : lance une commande Linux. Ici, on met d’abord à jour la liste des packages téléchargeables via apt, puis on installe Python ainsi que des librairies système nécessaires au bon fonctionnement de notre application ;\nWORKDIR : spécifie le répertoire de travail de l’image. Ainsi, toutes les commandes suivantes seront exécutées depuis ce répertoire ;\nCOPY : copie un fichier local sur l’image Docker. Ici, on copie d’abord le fichier requirements.txt du projet, qui spécifie les dépendances Python de notre application, puis on les installe avec une commande RUN. La seconde instruction COPY copie le répertoire du projet sur l’image ;\nENV : crée une variable d’environnement qui sera accessible à l’application dans le conteneur. Ici, on définit une variable d’environnement attendue par Flask, qui spécifie le nom du script permettant de lancer l’application ;\nEXPOSE : informe Docker que le conteneur “écoute” sur le port 5000, qui est le port par défaut utilisé par le serveur web de Flask ;\nCMD : spécifie la commande que doit exécuter le conteneur lors de son lancement. Il s’agit d’une liste, qui contient les différentes parties de la commande sous forme de chaînes de caractères. Ici, on lance Flask, qui sait automatiquement quelle application lancer du fait de la commande ENV spécifiée précédemment.\n\n{{% box status=“hint” title=“Hint: choix des librairies système” icon=“fa fa-lightbulb” %}} Avec la première commande RUN du Dockerfile, nous installons Python mais aussi des librairies système nécessaires au bon fonctionnement de l’application. Mais comment les avons-nous trouvées ?\nPar essai et erreur. Lors de l’étape de build que l’on verra juste après, le moteur Docker va essayer de construire l’image selon les spécifications du Dockerfile, comme s’il partait d’un ordinateur vide contenant simplement Ubuntu 20.04. Si des librairies manquent, le processus de build devrait renvoyer une erreur, qui s’affichera dans les logs de l’application, affichés par défaut dans la console. Quand on a de la chance, les logs décrivent explicitement les librairies système manquantes. Mais souvent, les messages d’erreur ne sont pas très explicites, et il faut alors les copier dans un moteur de recherche bien connu pour trouver la réponse, souvent sur Stackoverflow. {{% /box %}}\n{{% box status=“hint” title=“Hint: pourquoi COPY ?” icon=“fa fa-lightbulb” %}} La recette présente dans le Dockerfile peut nécessiter l’utilisation de fichiers appartenant au dossier de travail. Pour que Docker les trouve dans son contexte, il est nécessaire d’introduire une commande COPY. C’est un petit peu comme pour la cuisine: pour utiliser un produit dans une recette, il faut le sortir du frigo (fichier local) et le mettre sur la table.\n{{% /box %}}\n{{% box status=“note” title=“Note: les commandes Docker” icon=“fa fa-comment” %}} Nous n’avons vu que les commandes Docker les plus fréquentes, il en existe beaucoup d’autres en pratique. N’hésitez pas à consulter la documentation officielle pour comprendre leur utilisation. {{% /box %}}\n\n\nConstruction d’une image Docker\nPour construire une image à partir d’un Dockerfile, il suffit d’utiliser la commande docker build. Il faut ensuite spécifier deux éléments importnats : - le build context. Il faut indiquer à Docker le chemin de notre projet, qui doit contenir le Dockerfile. En pratique, il est plus simple de se mettre dans le dossier du projet via la commande cd, puis de passer . comme build context pour indiquer à Docker de build “d’ici” ; - le tag, c’est à dire le nom de l’image. Tant que l’on utilisee Docker en local, le tag importe peu. On verra par la suite que la structure du tag a de l’importance lorsque l’on souhaite exporter ou importer une image Docker à partir d’un dépôt distant.\nRegardons ce qui se passe en pratique lorsque l’on essaie de construire notre image.\n$ docker build -t myflaskapp .\nSending build context to Docker daemon     47MB\nStep 1/8 : FROM ubuntu:20.04\n ---> 825d55fb6340\nStep 2/8 : RUN apt-get update && apt-get install -y python3-pip python3-dev\n ---> Running in 92b42d579cfa\n...\ndone.\nRemoving intermediate container 92b42d579cfa\n ---> 8826d53e3c01\nStep 3/8 : WORKDIR /app\n ---> Running in 153b32893c23\nRemoving intermediate container 153b32893c23\n ---> 7b4d22021986\nStep 4/8 : COPY requirements.txt /app/requirements.txt\n...\nSuccessfully built 125bd8da70ff\nSuccessfully tagged myflaskapp:latest\nLe moteur Docker essaie de construire notre image séquentiellement à partir des commandes spécifiées dans le Dockerfile. S’il rencontre une erreur, la procédure s’arrête, et il faut alors trouver la source du problème dans les logs et adapter le Dockerfile en conséquence. Si tout se passe bien, Docker nous indique que le build a réussi et l’image est prête à être utilisée. On peut vérifier que l’image est bien disponible à l’aide de la commande docker images.\n$ docker images\nREPOSITORY                               TAG       IMAGE ID       CREATED          SIZE\nmyflaskapp                               latest    57d2f410a631   2 hours ago      433MB\nIntéressons nous un peu plus en détail aux logs de l’étape de build. Entre les étapes, Docker affiche des suites de lettres et de chiffres un peu ésotériques, et nous parle de conteneurs intermédiaires. En fait, il faut voir une image Docker comme un empilement de couches (layers), qui sont elles-mêmes des images Docker. Quand on hérite d’une image avec l’instruction FROM, on spécifie donc à Docker la couche initiale, sur laquelle il va construire le reste de notre environnement. A chaque étape sa nouvelle couche, et à chaque couche son hash, un identifiant unique fait de lettres et de chiffres.\nCela peut ressembler à des détails techniques, mais c’est en fait extrêmement utile en pratique car cela permet à Docker de faire du caching. Lorsque l’on développe un Dockerfile, il est fréquent de devoir modifier ce dernier de nombreuses fois avant de trouver la bonne recette, et on aimerait bien ne pas avoir à rebuild l’environnement complet à chaque fois. Docker gère cela très bien : il cache chacune des couches intermédiaires. Par exemple, si l’on modifie la 5ème commande du Dockerfile, Docker va utiliser le cache pour ne pas avoir à recalculer les étapes précédentes, qui n’ont pas changé. Cela s’appelle l’“invalidation du cache” : dès lors qu’une étape du Dockerfile est modifiée, Docker va recalculer toutes les étapes suivantes, mais seulement celles-ci. Conséquence directe de cette observation : il faut toujours ordonner les étapes d’un Dockerfile de sorte à ce qui est le plus susceptible d’être souvent modifié soit à la fin du fichier, et inversement.\nPour illustrer cela, regardons ce qui se passe si l’on modifie le nom du script qui lance l’application, et donc la valeur de la variable d’environnement FLASK_APP dans le Dockerfile.\n$ docker build . -t myflaskapp\nSending build context to Docker daemon  4.096kB\nStep 1/10 : FROM ubuntu:20.04\n ---> 825d55fb6340\nStep 2/10 : ENV DEBIAN_FRONTEND=noninteractive\n ---> Using cache\n ---> ea1c7c083ac9\nStep 3/10 : RUN apt-get update -y &&     apt-get install -y python3-pip python3-dev\n ---> Using cache\n ---> 078b8ac0e1cb\nStep 4/10 : WORKDIR /app\n ---> Using cache\n ---> cd19632825b3\nStep 5/10 : COPY requirements.txt /app/requirements.txt\n ---> Using cache\n ---> 271cd1686899\nStep 6/10 : RUN pip install -r requirements.txt\n ---> Using cache\n ---> 3ea406fdf383\nStep 7/10 : COPY . /app\n ---> 3ce5bd3a9572\nStep 8/10 : ENV FLASK_APP=\"new.py\"\n ---> Running in b378d16bb605\nRemoving intermediate container b378d16bb605\n ---> e1f50490287b\nStep 9/10 : EXPOSE 5000\n ---> Running in ab53c461d3de\nRemoving intermediate container ab53c461d3de\n ---> 0b86eca40a80\nStep 10/10 : CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n ---> Running in 340eec151a51\nRemoving intermediate container 340eec151a51\n ---> 16d7a5b8db28\nSuccessfully built 16d7a5b8db28\nSuccessfully tagged myflaskapp:latest\nL’étape de build a pris quelques secondes au lieu de plusieurs minutes, et les logs montrent bien l’utilisation du cache faite par Docker : les étapes précédant le changement réutilisent les couches cachées, mais celle d’après sont recalculées.\n\n\nExécuter une image Docker\nL’étape de build a permis de créer une image Docker. Une image doit être vue comme un template : elle permet d’exécuter l’application sur n’importe quel environnement d’exécution sur lequel un moteur Docker est installé. En l’état, on a donc juste construit, mais rien lancé : notre application ne tourne pas encore. Pour cela, il faut créer un conteneur, i.e. une instance vivante de l’image qui permet d’accéder à l’application. Cela se fait via la commande docker run.\n$ docker run -d -p 8000:5000 myflaskapp:latest\n6a2ab0d82d051a3829b182ede7b9152f7b692117d63fa013e7dfe6232f1b9e81\nDétaillons la syntaxe de cette commande : - docker run tag : lance l’image dont on fournit le tag. Le tag est de la forme repository/projet:version. Ici, il n’y a pas de repository puisque tout est fait en local ; - -d : “détache” le conteneur du terminal qui le lance ; - -p : effectue un mapping entre un port de la machine qui exécute le conteneur, et le conteneur lui-même. Notre conteneur écoute sur le port 5000, et l’on veut que notre application soit exposée sur le port 8000 de notre machine.\nLorsque l’on exécute docker run, Docker nous répond simplement un hash qui identifie le conteneur que l’on a lancé. On peut vérifier qu’il tourne bien avec la commande docker ps, qui renvoie toutes les informations associées au conteneur.\n$ docker ps\nCONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n6a2ab0d82d05   myflaskapp   \"flask run --host=0.…\"   7 seconds ago   Up 6 seconds   0.0.0.0:8000->5000/tcp, :::8000->5000/tcp   vigorous_kalam\nLes conteneurs peuvent être utilisés pour réaliser des tâches très différentes. Grossièrement, on peut distinguer deux situations : - le conteneur effectue une tâche “one-shot”, c’est à dire une opération qui a vocation à s’effectuer en un certain temps, suite à quoi le conteneur peut s’arrêter ; - le conteneur exécute une application. Dans ce cas, on souhaite que le conteneur reste en vie aussi longtemps que l’on souhaite utiliser l’application en question.\nDans notre cas d’application, on se situe dans la seconde configuration puisque l’on veut exécuter une application web. Lorsque l’application tourne, elle expose sur le localhost, accessible depuis un navigateur web — en l’occurence, à l’adresse localhost:8000/. Les calculs sont effectués sur un serveur local, et le navigateur sert d’interface avec l’utilisateur — comme lorsque vous utilisez un notebook Jupyter par exemple.\nFinalement, on a pu développer et exécuter une application complète sur notre environnement local, sans avoir eu à installer quoi que ce soit sur notre machine personnelle, à part Docker.\n\n\nExporter une image Docker\nJusqu’à maintenant, toutes les commandes Docker que nous avons exécutées se sont passées en local. Ce mode de fonctionnement peut être intéressant pour la phase de développement. Mais comme on l’a vu, un des gros avantages de Docker est la facilité de redistribution des images construites, qui peuvent ensuite être utilisées par de nombreux utilisateurs pour faire tourner notre application. Pour cela, il nous faut uploader notre image sur un dépôt distant, à partir duquel les utilisateurs pourront la télécharger.\nPlusieurs possibilités existent selon le contexte de travail : une entreprise peut avoir un dépôt interne par exemple. Si le projet est open-source, on peut utiliser le DockerHub. Le workflow pour uploader une image est le suivant : - créer un compte sur le DockerHub ; - créer un projet (public) sur le DockerHub, qui va héberger les images Docker du projet ; - sur un terminal, utiliser docker login pour s’authentifier au DockerHub ; - on va modifier le tag que l’on fournit lors du build pour spécifier le chemin attendu. Dans notre cas : docker build -t compte/projet:version . ; - uploader l’image avec docker push compte/projet:version\n$ docker push avouacr/myflaskapp:1.0.0\nThe push refers to repository [docker.io/avouacr/myflaskapp]\n71db96687fe6: Pushed \n624877ac887b: Pushed \nea4ab6b86e70: Pushed \nb5120a5bc48d: Pushed \n5fa484a3c9d8: Pushed \nc5ec52c98b31: Pushed \n1.0.0: digest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9 size: 1574\n\n\nImporter une image Docker\nEn supposant que le dépôt utilisé pour uploader l’image est public, la procédure que doit suivre un utilisateur pour la télécharger se résume à utiliser la commande docker pull compte/projet:version\n$ docker pull avouacr/myflaskapp:1.0.0\n1.0.0: Pulling from avouacr/myflaskapp\ne0b25ef51634: Pull complete \nc0445e4b247e: Pull complete \n48ba4e71d1c2: Pull complete \nffd728caa80a: Pull complete \n906a95f00510: Pull complete \nd7d49b6e17ab: Pull complete \nDigest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9\nStatus: Downloaded newer image for avouacr/myflaskapp:1.0.0\ndocker.io/avouacr/myflaskapp:1.0.0\nDocker télécharge et extrait chacune des couches qui constituent l’image (ce qui peut parfois être long). L’utilisateur peut alors créer un conteneur à partir de l’image, en utilisant docker run comme illustré précédemment.\n\n\n\nAide-mémoire\nVoici une première aide-mémoire sur les principales commandes à intégrer dans un Dockerfile:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nFROM <image>:<tag>\nUtiliser comme point de départ l’image <image> ayant le tag <tag>\n\n\nRUN <instructions>\nUtiliser la suite d’instructions <instructions> dans un terminal Linux. Pour passer plusieurs commandes dans un RUN, utiliser &&. Cette suite de commande peut avoir plusieurs lignes, dans ce cas, mettre \\ en fin de ligne\n\n\nCOPY <source> <dest>\nRécupérer le fichier présent dans le système de fichier local à l’emplacement <source> pour que les instructions ultérieures puissent le trouver à l’emplacement <source>\n\n\nADD <source> <dest>\nGlobalement, même rôle que COPY\n\n\nENV MY_NAME=\"John Doe\"\nCréation d’une variable d’environnement (qui devient disponible sous l’alias $MY_NAME)\n\n\nWORKDIR <path>\nDéfinir le working directory du conteuneur Docker dans le dossier <path>\n\n\nUSER <username>\nCréation d’un utilisateur non root nommé <username>\n\n\nEXPOSE <PORT_ID>\nLorsqu’elle tournera, l’application sera disponible depuis le port <PORT_ID>\n\n\nCMD [\"executable\",\"param1\",\"param2\"]\nAu lancement de l’instance Docker la commande executable (par exemple python3) sera lancée avec les paramètres additionnels fournis\n\n\n\nUne seconde aide-mémoire pour les principales commandes Linux est disponible ci-dessous:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\ndocker build . -t <tag>\nConstruire l’image Docker à partir des fichiers dans le répertoire courant (.) en l’identifiant avec le tag <tag>\n\n\ndocker run -it <tag>\nLancer l’instance docker identifiée par <tag>\n\n\ndocker images\nLister les images disponibles sur la machine et quelques unes de leurs propriétés (tags, volume, etc.)\n\n\ndocker system prune\nFaire un peu de ménage dans ses images Docker (bien réfléchir avant de faire tourner cette commande)"
  },
  {
    "objectID": "chapters/projects-architecture.html",
    "href": "chapters/projects-architecture.html",
    "title": "Améliorer l’architecture de ses projets",
    "section": "",
    "text": "fonctions\nmodules\n\n\n\n\nComme Git est un pré-requis, tout projet présente un fichier .gitignore (il est très important, surtout quand on manipule des données qui ne devraient pas se retrouver sur Github ou Gitlab).\nLes output sont stockés dans un dossier séparé, de même que les inputs (idéalement ils ne sont même pas stockés avec le code, nous reviendrons sur la distinction code-stockage-exécution plus tard). Ne pas oublier d’ajouter les dossiers ou extensions qui vont bien dans le .gitignore pour ne pas les committer.\nIdéalement, un projet utilise de l’intégration continue (voir partie XXX) :\n\nsi vous utilisez Gitlab, les instructions sont stockées dans le fichier gitlab-ci.yml\nsi vous utilisez Github, cela se passe dans le dossier .github/workflows\n\nproject\n│   .gitignore    \n│   .gitlab-ci.yml    \n│   main.R   \n│   README.md\n│\n└───input\n│   │   source.csv\n│\n└───R\n│   │   utils.R\n│   │   figures.R\n|   |   statsdesc.R\n|   |   ...\n│   \n└───output\n    │   superfigure.png\n    │   agregat.csv\n    |   ...\n\n\n\n\n\n\n\ninput / traitement / output\ndomain-driven design (data, domain, application, presentation)\n\n\n\n\n\nPython : cookiescutter\n\n\n\n\n\nLa meilleure façon d’assurer la reproductibilité d’un projet est d’intégrer ses scripts dans une structure de package. Un gros projet de data-science va ainsi dépendre d’un ou plusieurs packages, ce qui\n\nassure une gestion cohérente des dépendances\noffre une certaine structure pour la documentation\nfacilitera sa réutilisation (les utilisateurs peuvent n’être intéressés que par une partie du projet)\npermettra des économies d’échelle (on peut réutiliser l’un des packages pour un autre projet)\nfacilite le debuggage (il est plus facile d’identifier une erreur quand elle est dans un package)\n…\n\n\n\nTO DO\n\n\n\n\n\n\n\n\n\nTO DO\n\n\n\nQuand on débute, on est souvent timide et on désire ne rendre public son code que lorsque celui-ci est propre. C’est une erreur classique:\n\nComme pour nettoyer un appartement, les petits gestes en continu sont beaucoup plus efficace qu’un grand ménage de printemps. Prendre l’habitude de mettre son code immédiatement sur Github vous amènera à adopter de bons gestes.\n\n\n\nL’objectif des conseils de ce cours est de réduire le coût de la maintenance à long terme en adoptant les structures les plus légères, automatisées et réutilisables.\nLes notebooks Jupyter sont très pratiques pour tâtonner et expérimenter. Cependant, ils présentent un certain nombre d’inconvénients à long terme qui peuvent rendre impossible à maintenir le code écrit avec dans un notebook:\n\ntous les objets (fonctions, classes et données) sont définis et disponibles dans le même fichier. Le moindre changement à une fonction nécessite de retrouver l’emplacement dans le code, écrire et faire tourner à nouveau une ou plusieurs cellules.\nquand on tâtonne, on écrit du code dans des cellules. Dans un cahier, on utiliserait la marge mais cela n’existe pas avec un notebook. On créé donc de nouvelles cellules, pas nécessairement dans l’ordre. Quand il est nécessaire de faire tourner à nouveau le notebook, cela provoque des erreurs difficile à debugger (il est nécessaire de retrouver l’ordre logique du code, ce qui n’est pas évident).\nles notebooks incitent à faire des copier-coller de cellules et modifier marginalement le code plutôt qu’à utiliser des fonctions.\nil est quasi-impossible d’avoir un versioning avec Git des notebooks qui fonctionne. Les notebooks étant, en arrière plan, de gros fichiers JSON, ils ressemblent plus à des données que des codes sources. Git ne parvient pas à identifier les blocs de code qui ont changé\npassage en production des notebooks coûteux alors qu’un script bien fait est beaucoup plus facile à passer en prod (voir suite cours)\nJupyter manque d’extensions pour mettre en oeuvre les bonnes pratiques (linters, etc.). VSCode au contraire est très bien\nRisques de révélation de données confidentielles puisque les outputs des blocs de code, par exemple les head, sont écrits en dur dans le code source.\n\nGlobalement, les notebooks sont un bon outil pour tâtonner ou pour faire communiquer. Mais pour maintenir un projet à long terme, il vaut mieux privilégier les scripts. Les recommandations de ce cours visent à rendre le plus léger possible la maintenance à long terme de projets data-science en favorisant la reprise par d’autres (ou par soi dans le futur)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mise en production de projets data science",
    "section": "",
    "text": "Cours en 3A ENSAE\nTous les supports sont sur Github"
  }
]