{
  "hash": "a129fd069fab68144083b880909809b2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Application\"\nimage: images/rocket.png\ndescription: |\n  Une application fil rouge pour illustrer l'intérêt d'appliquer graduellement les bonnes pratiques dans une optique de mise en production d'une application de data science.\norder: 7\nhref: chapters/application.html\n---\n\n<details>\n<summary>\nDérouler les _slides_ ci-dessous ou [cliquer ici](https://ensae-reproductibilite.github.io/slides/#/title-slide)\npour afficher les slides en plein écran.\n</summary>\n\n\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode yaml code-with-copy\"><code class=\"sourceCode yaml\"></code><button title=\"Copy to Clipboard\" class=\"code-copy-button\"><i class=\"bi\"></i></button></pre><iframe class=\"sourceCode yaml code-with-copy\" src=\"https://ensae-reproductibilite.github.io/slides/#/title-slide\"></iframe></div>\n\n</details>\n\nL'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.\n\nCelle-ci est un tutoriel pas à pas pour avoir un projet reproductible et disponible sous plusieurs livrables. \nToutes les étapes ne sont pas indispensables à tous les projets de _data science_. \n\nNous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science.\nOn a un _notebook_ un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :\n\n- Import de données ;\n- Statistiques descriptives et visualisations ;\n- *Feature engineering* ;\n- Entraînement d'un modèle ;\n- Evaluation du modèle.\n\n**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée.** \n\n\n<details>\n<summary>\nIllustration de notre point de départ\n</summary>\n![](/workflow1.png)\n</details>\n\n<details>\n<summary>\nIllustration de l'horizon vers lequel on se dirige\n</summary>\n![](/workflow2.png)\n</details>\n\n::: {.callout-important}\nIl est important de bien lire les consignes et d'y aller progressivement.\nCertaines étapes peuvent être rapides, d'autres plus fastidieuses ;\ncertaines être assez guidées, d'autres vous laisser plus de liberté.\nSi vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à\nl'étape suivante qui en dépend.\n\nBien que l'exercice soit applicable sur toute configuration bien faite, nous \nrecommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les \noutils nécessaires sont pré-installés et pré-configurés. Le service `VSCode`\nne sera en effet que le point d'entrée pour l'utilisation d'outils plus exigeants\nsur le plan de l'infrastructure: _Argo_, _MLFLow_, etc.\n:::\n\n\n# Partie 0 : initialisation du projet\n\n::: {.callout-tip}\n## Application préliminaire: forker le dépôt d'exemple\n\nLes premières étapes consistent à mettre en place son environnement de travail sur `Github`:\n\n- Générer un jeton d'accès (*token*) sur `GitHub` afin de permettre l'authentification en ligne de commande à votre compte.\nLa procédure est décrite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). \n__Vous ne voyez ce jeton qu'une fois, ne fermez pas la page de suite__. \n\n- Mettez de côté ce jeton en l'enregistrant dans un gestionnaire de mot de passe ou dans \nl'espace _[\"Mon compte\"](https://datalab.sspcloud.fr/account/third-party-integration)_\ndu `SSP Cloud`. \n\n- Forker le dépôt `Github` : [https://github.com/ensae-reproductibilite/application-correction](https://github.com/ensae-reproductibilite/application-correction) en faisant attention à deux choses:\n    + Renommer le dépôt en `ensae-reproductibilite-application-correction.git` ;\n    + Décocher la case _\"Copy the `main` branch only\"_ afin de copier également les _tags_ `Git` qui nous permettront de faire les _checkpoint_\n\n\n<details>\n\n<summary>\nCe que vous devriez voir sur la page de création du _fork_\n</summary>\n\n![](/fork-example.png)\n\n</details>\n\nIl est maintenant possible de ce lancer dans la création de l'environnement de travail:\n\n- Ouvrir un service `VSCode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller\ndans la page `My Services` et cliquer sur `New service`. Sinon, vous\npouvez initialiser la création du service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=false). __Modifier les options suivantes__:\n    + Dans l'onglet `Kubernetes`, sélectionner le rôle `Admin` ;\n    + Dans l'onglet `Networking`, cliquer sur \"Enable a custom service port\" et laisser la valeur par défaut 5000 pour le numéro du port\n\n- Clôner __votre__ dépôt `Github` en utilisant le\nterminal depuis `Visual Studio` (`Terminal > New Terminal`) et\nen passant directement le token dans l'URL selon cette structure:\n\n```{.bash filename=\"terminal\"}\ngit clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application-correction.git\n```\n\noù `<TOKEN>` et `<USERNAME>` sont à remplacer, respectivement, \npar le jeton que vous avez généré précédemment et votre nom d'utilisateur.\n\n- Se placer avec le terminal dans le dossier en question : \n\n```{.bash filename=\"terminal\"}\ncd ensae-reproductibilite-application-correction\n```\n\n- Se placer sur une branche de travail en faisant:\n\n```{.bash filename=\"terminal\"}\ngit checkout -b dev\n```\n\n:::\n\n\n# Partie 1 : qualité du script\n\nCette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours.\n\nElle fait intervenir les notions suivantes : \n\n- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.html)) ;\n- **Qualité du code** (voir [Qualité du code](/chapters/code-quality.html)) ;\n- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;\n- **Contrôle de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;\n- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).\n\nNous allons partir de ce _Notebook_ `Jupyter`,\nque vous pouvez prévisualiser voire tester\nen cliquant sur l'un des liens suivants:\n\n_to do bouton onyxia_\n<a href=\"https://github.com/ensae-reproductibilite/application-correction/blob/main/titanic.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n\nLe plan de la partie est le suivant :\n\n1. S'assurer que le script fonctionne ;\n2. Nettoyer le code des scories formelles avec un _linter_ et un _formatter_ ;\n3. Paramétrisation du script ;\n4. Utilisation de fonctions.\n\n\n## Étape 1 : s'assurer que le script s'exécute correctement\n\nOn va partir du fichier `notebook.py` qui reprend le contenu \ndu _notebook_[^jupytext] mais dans un script classique.\nLe travail de nettoyage en sera facilité. \n\n[^jupytext]: L'export dans un script `.py` a été fait\n        directement depuis `VSCode`. Comme\n        cela n'est pas vraiment l'objet du cours, nous passons cette étape et fournissons\n        directement le script expurgé du texte intermédiaire. Mais n'oubliez\n        pas que cette démarche, fréquente quand on a démarré sur un _notebook_ et\n        qu'on désire consolider en faisant la transition vers des \n        scripts, nécessite d'être attentif pour ne pas risquer de faire une erreur. \n\nLa première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**. \nPour cela, nous recommandons de faire un aller-retour entre le script ouvert dans `VSCode`\net un terminal pour le lancer. \n\n\n::: {.callout-tip}\n## Application 1: corriger les erreurs\n\n- Ouvrir dans `VSCode` le script `titanic.py` ;\n- Exécuter le script en ligne de commande (`python titanic.py`)[^interactivite] pour détecter les erreurs ;\n- Corriger les deux erreurs qui empêchent la bonne exécution ;\n- Vérifier le fonctionnement du script en utilisant la ligne de commande:\n\n```{.bash filename=\"terminal\"}\npython titanic.py\n```\n\nLe code devrait afficher des sorties.\n\n<details>\n<summary>\nAide sur les erreurs rencontrées\n</summary>\n\nLa première erreur rencontrée est une alerte `FileNotFoundError`,\nla seconde est liée à un _package_. \n\n</details>\n\n\nIl est maintenant temps de *commit* les changements effectués avec `Git`[^2] :\n\n\n```{.bash filename=\"terminal\"}\ngit add titanic.py\ngit commit -m \"Corrige l'erreur qui empêchait l'exécution\"\ngit push\n```\n\n<!---- \nTemps estimé: 3mn\n------>\n\n:::\n\n[^interactivite]: Il est également possible avec `VSCode` d'exécuter le script ligne à ligne\nde manière interactive ligne à ligne (<kbd>MAJ</kbd>+<kbd>ENTER</kbd>). Néanmoins, cela nécessite\nde s'assurer que le _working directory_ de votre console interactive est le bon. Celle-ci se\nlance selon les paramètres préconfigurés de `VSCode` et les votres ne sont peut-être pas les\nmêmes que les notres. Vous pouvez changer le _working directory_ dans le script\nen utilisant le _package_ `os` mais peut-être allez vous découvrir ultérieurement qu'il\ny a de meilleures pratiques...\n[^2]: Essayez de *commit* vos changements à chaque étape de l'exercice, c'est une bonne habitude à prendre.\n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli1\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n\n## Étape 2: utiliser un _linter_ puis un _formatter_\n\nOn va maintenant améliorer la qualité de notre code en appliquant les standards communautaires.\nPour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/)\net le _formatter_ [`Black`](https://github.com/psf/black).\n\n::: {.callout-important}\n[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)\nsont des _packages_ `Python` qui \ns'utilisent principalement en ligne de commande.\n\nSi vous avez une erreur qui suggère\nque votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)\nou [`Black`](https://black.readthedocs.io/en/stable/),\nn'oubliez pas d'exécuter la commande `pip install pylint` ou `pip install black`.\n:::\n\n\nLe _linter_ renvoie alors une série d'irrégularités,\nen précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation).\nIl renvoie finalement une note sur 10,\nqui estime la qualité du code à l'aune des standards communautaires évoqués\ndans la partie [Qualité du code](/chapters/code-quality.html).\n\n::: {.callout-tip}\n## Application 2: rendre lisible le script\n\n- Diagnostiquer et évaluer la qualité de `titanic.py` avec [`PyLint`](https://pylint.readthedocs.io/en/latest/). Regarder la note obtenue.\n- Utiliser `black titanic.py --diff --color` pour observer les changements de forme que va induire l'utilisation du _formatter_ [`Black`](https://black.readthedocs.io/en/stable/). Cette étape n'applique pas les modifications, elle ne fait que vous les montrer.\n- Appliquer le _formatter_ [`Black`](https://black.readthedocs.io/en/stable/)\n- Réutiliser [`PyLint`](https://pylint.readthedocs.io/en/latest/) pour diagnostiquer l'amélioration de la qualité du script et le travail qui reste à faire. \n- Comme la majorité du travail restant est à consacrer aux imports:\n    - Mettre tous les _imports_ ensemble en début de script\n    - Retirer les _imports_ redondants en s'aidant des diagnostics de votre éditeur\n    - Réordonner les _imports_ si [`PyLint`](https://pylint.readthedocs.io/en/latest/) vous indique de le faire\n    - Corriger les dernières fautes formelles suggérées par [`PyLint`](https://pylint.readthedocs.io/en/latest/)\n- Délimiter des parties dans votre code pour rendre sa structure plus lisible \n\n<!-----\nTemps test Julien: 22mn\n------>\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli2\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\nLe code est maintenant lisible, il obtient à ce stade une note formelle proche de 10.\nMais il n'est pas encore totalement intelligible ou fiable.\nIl y a notamment \nbeaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. \nNéanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script: \njetons d'API et chemin des fichiers.\n\n\n## Étape 3: gestion des paramètres\n\nL'exécution du code et les résultats obtenus\ndépendent de certains paramètres définis dans le code. L'étude de résultats\nalternatifs, en jouant sur \ndes variantes des (hyper)paramètres, est à ce stade compliquée\ncar il est nécessaire de parcourir le code pour trouver\nces paramètres. De plus, certains paramètres personnels\ncomme des jetons\nd'API ou des mots de passe n'ont pas vocation à \nêtre présents dans le code. \n\nIl est plus judicieux de considérer ces paramètres comme des\nvariables d'entrée du script. Cela peut être fait de deux\nmanières:\n\n1. Avec des __arguments optionnels__ appelés depuis la ligne de commande _(Application 3a)_.\nCela peut être pratique pour mettre en oeuvre des tests automatisés mais\nn'est pas forcément pertinent pour toutes les variables. Nous allons montrer\ncet usage avec le nombre d'arbres de notre _random forest_ ;\n2. En utilisant un __fichier de configuration__ dont les valeurs sont importées dans\nle script principal _(Application 3b)_. \n\n\n<details>\n<summary>\nUn exemple de définition d'un argument pour l'utilisation en ligne de commande\n</summary>\n\n```{.python filename=\"prenom.py\"}\nimport argparse\nparser = argparse.ArgumentParser(description=\"Qui êtes-vous?\")\nparser.add_argument(\n    \"--prenom\", type=str, default=\"Toto\", help=\"Un prénom à afficher\"\n)\nargs = parser.parse_args()\nprint(args.prenom)\n```\n\nExemples d'utilisations en ligne de commande\n\n```{.bash filename=\"terminal\"}\npython prenom.py\npython prenom.py --prenom \"Zinedine\"\n```\n\n</details>\n\n::: {.callout-tip}\n## Application 3a: Paramétrisation du script\n\n1. En s'inspirant de l'exemple ci-dessus 👆️,\ncréer une variable `n_trees` qui peut éventuellement être paramétrée en ligne de commande\net dont la valeur par défaut est 20 ;\n2. Tester cette paramétrisation en ligne de commande avec la valeur par défaut\npuis 2, 10 et 50 arbres.\n:::\n\nL'exercice suivant permet de mettre en application le fait de paramétriser\nun script en utilisant des variables définies dans un fichier YAML. \n\n\n::: {.callout-tip}\n## Application 3b: La configuration dans un fichier YAML\n\nNous allons mettre 4 paramètres dans notre YAML. Celui-ci prendra la forme suivante:\n\n```{.yaml filename=\"config.yaml\"}\njeton_api: ####\ntrain_path: ####\ntest_path: ####\ntest_fraction: ####\n```\n\nAvec `####` des valeurs à remplacer.\n\n1. Créer à la racine du projet un fichier `config.yaml` à partir du\nmodèle 👆️ ;\n2. Repérer les valeurs dans le code associées et compléter.\n\nMaintenant, nous allons exploiter ce fichier:\n\n3. Pour éviter d'avoir à le faire plus tard,\ncréer une fonction `import_yaml_config` qui prend en argument le\nchemin d'un fichier `YAML`\net renvoie le contenu de celui-ci en _output_. Vous pouvez suivre\nle conseil du chapitre sur la [Qualité du code](/chapters/code-quality.html)\nen adoptant le _type hinting_ ;\n\n<details>\n<summary>Indice si vous ne trouvez pas comment lire un fichier `YAML`</summary>\n\nSi le fichier s'appelle `toto.yaml`, vous pouvez l'importer de cette manière:\n\n::: {#61d83db6 .cell execution_count=1}\n``` {.python .cell-code}\nwith open(\"toto.yaml\", \"r\", encoding=\"utf-8\") as stream:\n    dict_config = yaml.safe_load(stream)\n```\n:::\n\n\n</details>\n\n4. Dans la fonction `import_yaml_config`,\ncréer une condition logique pour tenir compte du fait que le YAML de configuration\npeut ne pas exister[^fileexist] ;\n\n<details>\n<summary>Indice si vous ne savez comment conditionner la création de la configuration à l'existence du fichier</summary>\n\nVoici la ligne qui peut vous aider. L'idéal\nest d'insérer ceci dans `import_yaml_config`:\n\n::: {#d39567b1 .cell execution_count=2}\n``` {.python .cell-code}\nCONFIG_PATH = 'config.yaml'\nconfig = {}\nif os.path.exists(CONFIG_PATH):\n    # lecture du fichier\n```\n:::\n\n\n</details>\n\n\n5. Utiliser le canevas de code suivant pour créer les variables adéquates\n\n::: {#8f05f0bd .cell execution_count=3}\n``` {.python .cell-code}\nAPI_TOKEN = config.get(\"jeton_api\")\nTRAIN_PATH = config.get(\"train_path\", \"train.csv\")\nTEST_PATH = config.get(\"test_path\", \"test.csv\")\nTEST_FRACTION = config.get(\"test_fraction\", .1)\n```\n:::\n\n\net remplacer dans le code ;\n\n5. Tester en ligne de commande que l'exécution du fichier est toujours\nsans erreur et sinon corriger ;\n6. Refaire un diagnostic avec [`PyLint`](https://pylint.readthedocs.io/en/latest/)\net corriger les éventuels messages ;\n7. Créer un fichier `.gitignore` (cf. [Chapitre `Git`](/chapters/git.qmd)). Ajouter dans ce fichier `config.yaml`\ncar il ne faut pas committer ce fichier. Au passage ajouter `__pycache__/` au `.gitignore`[^pycache], cela\névitera d'avoir à le faire ultérieurement ;\n9. Créer un fichier `README.md` où vous indiquez qu'il faut créer un fichier `config.yaml` pour\npouvoir utiliser l'API. \n\n[^fileexist]: Ici, le jeton d'API n'est pas indispensable pour que le code\n    fonctionne. Afin d'éviter une erreur non nécessaire\n    lorsqu'on automatisera le processus, on peut\n    créer une condition qui vérifie la présence ou non de ce fichier.\n    Le script reste donc reproductible même pour un utilisateur n'ayant pas le fichier\n    `secrets.yaml`. \n\n[^pycache]: Il est normal d'avoir des dossiers `__pycache__` qui traînent en local : ils se créent automatiquement à l'exécution d'un script en `Python`. Néanmoins, il ne faut pas associer ces fichiers à `Git`, voilà pourquoi on les ajoute au `.gitignore`.\n\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli3\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n\n## Étape 4 : Privilégier la programmation fonctionnelle\n\nNous allons **mettre en fonctions les parties importantes de l'analyse**. \nCeci facilitera l'étape ultérieure de modularisation de notre projet. \n\nCet exercice étant chronophage, il n'est __pas obligatoire de le réaliser en entier__. L'important est de\ncomprendre la démarche et d'adopter fréquemment une approche fonctionnelle[^POO]. Pour obtenir \nune chaine entièrement fonctionnalisée, vous pouvez reprendre le _checkpoint_.\n\n[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser\nun processus, il serait possible d'adopter le paradigme de la __programmation orientée objet (POO)__. Celle-ci est\nplus rebutante et demande plus de temps au développeur. L'arbitrage coût-avantage est négatif pour notre\nexemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d'un modèle,\nil est recommandé de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/). \n\n::: {.callout-tip}\n## Application 4: adoption des standards de programmation fonctionnelle \n\n- Créer une fonction qui importe les données d'entraînement (`train.csv`) et de test (`test.csv`) et renvoie des `DataFrames` `Pandas` ;\n- En fonction du temps disponible, créer plusieurs fonctions pour réaliser les étapes de *feature engineering*:\n    + La création de la variable _\"Title\"_ peut être automatisée en vertu du principe _\"do not repeat yourself\"_[^notepandas].\n    + Regrouper ensemble les `fillna` et essayer de créer une fonction généralisant l'opération. \n    + Les _label encoders_ peuvent être transformés en deux fonctions: une première pour encoder une colonne puis une seconde qui utilise\n    la première de manière répétée pour encoder plusieurs colonnes. _Remarquez les erreurs de copier-coller que cela corrige_\n    + Finaliser les dernières transformations avec des fonctions\n- Créer une fonction qui réalise le *split train/test* de validation en fonction d'un paramètre représentant la proportion de l'échantillon de test.\n- Créer une fonction qui entraîne et évalue un classifieur `RandomForest`, et qui prend en paramètre le nombre d'arbres (`n_estimators`). La fonction doit imprimer à la fin la performance obtenue et la matrice de confusion.\n- Déplacer toutes les fonctions ensemble, en début de script.\n:::\n\n[^notepandas]: Au passage vous pouvez noter que mauvaises pratiques discutables,\n    peuvent\n    être corrigées, notamment l'utilisation excessive de `apply` là où\n    il serait possible d'utiliser des méthodes embarquées par `Pandas`.\n    Cela est plutôt de l'ordre du bon style de programmation que de la\n    qualité formelle du script. Ce n'est donc pas obligatoire mais c'est mieux. \n\n\n::: {.callout-important}\nLe fait d'appliquer des fonctions a déjà amélioré la fiabilité du processus\nen réduisant le nombre d'erreurs de copier-coller. Néanmoins, pour vraiment\nfiabiliser le processus, il faudrait utiliser un _pipeline_ de transformations\nde données. \n\nCeci n'est pas encore au programme du cours mais le sera dans une prochaine \nversion. \n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli4\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\nCela ne se remarque pas encore vraiment car nous avons de nombreuses définitions de fonctions\nmais notre chaine de production est beaucoup plus\nconcise (le script fait environ 300 lignes dont 250 de définitions de fonctions génériques).\nCette auto-discipline facilitera grandement\nles étapes ultérieures. Cela aurait été néanmoins beaucoup moins coûteux en temps d'adopter\nces bons gestes de manière plus précoce. \n\n\n# Partie 2 : adoption d'une structure modulaire {#partie2}\n\nDans la partie précédente,\non a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.\nCe faisant, on s'est déjà considérablement rapprochés d'un\npossible partage du code : celui-ci est lisible et intelligible. \nLe code est proprement versionné sur un\ndépôt `GitHub`.\nCependant, le projet est encore perfectible: il est encore difficile de rentrer\ndedans si on ne sait pas exactement ce qu'on recherche. L'objectif de cette partie\nest d'isoler les différentes étapes de notre _pipeline_. \nOutre le gain de clarté pour notre projet, nous économiserons beaucoup de peines\npour la mise en production ultérieure de notre modèle. \n\n<details>\n<summary>\nIllustration de l'état actuel du projet \n</summary>\n![](/schema_post_appli4.png)\n</details>\n\nDans cette partie nous allons continuer les améliorations\nincrémentales de notre projet avec les étapes suivantes:\n\n1. Modularisation du code `Python` pour séparer les différentes\nétapes de notre _pipeline_ ; \n2. Adopter une structure standardisée pour notre projet afin\nd'autodocumenter l'organisation de celui-ci ; \n3. Documenter les _packages_ indispensables à l'exécution du code ;\n4. Stocker les données dans un environnement adéquat\nafin de continuer la démarche de séparer conceptuellement les données du code en de la configuration.\n\n\n## Étape 1 : modularisation\n\nNous allons profiter de la modularisation pour adopter une structure\napplicative pour notre code. Celui-ci n'étant en effet plus lancé\nque la ligne de commande, on peut considérer qu'on construit\nune application générique où un script principal (`main.py`)\nencapsule des éléments issus d'autres scripts. \n\n::: {.callout-tip}\n## Application 5: modularisation\n\n- Déplacer les fonctions dans une série de fichiers dédiés:\n    +  `import_data.py`: fonctions d'import de données \n    +  `build_features.py`: fonctions regroupant les étapes de _feature engineering_ \n    +  `train_evaluate.py`: fonctions d'entrainement et d'évaluation du modèle\n- Spécifier les dépendances (i.e. les packages à importer)\ndans les modules pour que ceux-ci puissent s'exécuter indépendamment ;\n- Renommer `titanic.py` en `main.py` pour suivre la convention de nommage des projets `Python` ;\n- Importer les fonctions nécessaires à partir des modules.\n- Vérifier que tout fonctionne bien en exécutant le _script_ `main` à partir de la ligne de commande :\n\n```{.bash filename=\"terminal\"}\n$ python main.py\n```\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli5\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n\n## Étape 2 : adopter une architecture standardisée de projet\n\nOn dispose maintenant d'une application `Python` fonctionnelle. \nNéanmoins, le projet est certes plus fiable mais sa structuration\nlaisse à désirer et il serait difficile de rentrer à nouveau\ndans le projet dans quelques temps. \n\n<details>\n<summary>Etat actuel du projet 🙈</summary>\n\n```\n├── README.md\n├── train.csv\n├── test.csv\n├── .gitignore\n├── config.yaml\n├── import_data.py\n├── build_features.py\n├── train_evaluate.py\n├── titanic.ipynb\n└── main.py\n```\n\n</details>\n\nComme cela est expliqué dans la\npartie [Structure des projets](/chapters/projects-architecture.html),\non va adopter une structure certes arbitraire mais qui va \nfaciliter l'autodocumentation de notre projet. De plus, une telle structure va faciliter des évolutions optionnelles\ncomme la _packagisation_ du projet. Passer d'une structure modulaire\nbien faite à un _package_ est quasi-immédiat en `Python`. \n\nOn va donc modifier l'architecture de notre projet pour la rendre plus standardisée.\nPour cela, on va s'inspirer des structures\n[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)\nqui génèrent des _templates_ de projet. En l'occurrence\nnotre source d'inspiration sera le [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)\nissu d'un effort communautaire.\n\n::: {.callout-note}\nL'idée de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante : \n\n```{.bash filename=\"terminal\"}\npip install cookiecutter\ncookiecutter https://github.com/drivendata/cookiecutter-data-science\n```\n\nIci, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.\n:::\n\nEn s'inspirant du _cookiecutter data science_\non va adopter la structure suivante:\n\n<details>\n<summary>\nStructure recommandée\n</summary>\n\n```\nensae-reproductibilite-application\n├── main.py\n├── README.md\n├── data\n│   └── raw\n│       ├── test.csv\n│       └── train.csv\n├── configuration\n│   └── config.yaml\n├── notebooks\n│   └── titanic.ipynb\n└── src\n    ├── data\n    │   └── import_data.py\n    ├── features\n    │   └── build_features.py\n    └── models\n        └── train_evaluate.py\n```\n\n</details>\n\n::: {.callout-tip}\n\n## Application 6: adopter une structure lisible\n\n- _(optionnel)_ Analyser et comprendre la [structure de projet](https://drivendata.github.io/cookiecutter-data-science/#directory-structure) proposée par le template ;\n- Modifier l'arborescence du projet selon le modèle ;\n- Mettre à jour l'import des dépendances, le fichier de configuration et `main.py` avec les nouveaux chemins ;\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli6\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n\n:::\n\n\n\n\n## Étape 3: indiquer l'environnement minimal de reproductibilité\n\nLe script `main.py` nécessite un certain nombre de packages pour\nêtre fonctionnel. Chez vous les packages nécessaires sont\nbien sûr installés mais êtes-vous assuré que c'est le cas \nchez la personne qui testera votre code ? \n\nAfin de favoriser la portabilité du projet,\nil est d'usage de _\"fixer l'environnement\"_,\nc'est-à-dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version.\nNous proposons de créer un fichier `requirements.txt` minimal, sur lequel nous reviendrons\ndans la partie consacrée aux environnements reproductibles. \n\nLe fichier `requirements.txt` est conventionnellement localisé à la racine du projet.\nIci on ne va pas fixer les versions, on raffinera ce fichier ultérieurement.\n\n::: {.callout-tip}\n\n## Application 7: création du `requirements.txt`\n\n- Créer un fichier `requirements.txt` avec la liste des packages nécessaires\n- Ajouter une indication dans `README.md` sur l'installation des _packages_ grâce au fichier `requirements.txt` \n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli7\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n## Étape 4 : stocker les données de manière externe {#stockageS3}\n\n::: {.callout-warning collapse=\"true\"}\n## Pour en savoir plus sur le système de stockage `S3`\n\nPour mettre en oeuvre cette étape, il peut être utile de\ncomprendre un peu comme fonctionne le SSP Cloud.\nVous devrez suivre la [documentation du SSP Cloud](https://inseefrlab.github.io/docs.sspcloud.fr/docs/fr/storage.html) pour la réaliser. Une aide-mémoire est également disponible dans le cours\nde 2e année de l'ENSAE [Python pour la _data science_](https://linogaliana-teaching.netlify.app/reads3/#).\n:::\n\n\nLe chapitre sur la [structure des projets](/chapters/projects-architecture.qmd)\ndéveloppe l'idée qu'il est recommandé de converger vers un modèle\noù environnements d'exécution, de stockage du code et des données sont conceptuellement\nséparés. Ce haut niveau d'exigence est un gain de temps important \nlors de la mise en production car au cours de cette dernière, le projet\nest amené à être exécuté sur une infrastructure informatique dédiée\nqu'il est bon d'anticiper. \n\nA l'heure actuelle, les données sont stockées dans le dépôt. C'est une\nmauvaise pratique. En premier lieu, `Git` n'est techniquement\npas bien adapté au stockage de données. Ici ce n'est pas très grave\ncar il ne s'agit pas de données volumineuses et ces dernières ne sont\npas modifiées au cours de notre chaine de traitement. \nLa raison principale\nest que les données traitées par les _data scientists_ \nsont généralement soumises à des clauses de\nconfidentialités ([RGPD](https://www.cnil.fr/fr/rgpd-de-quoi-parle-t-on), [secret statistique](https://www.insee.fr/fr/information/1300624)...). Mettre ces données sous contrôle de version\nc'est prendre le risque de les divulguer à un public non habilité. \nIl est donc recommandé de privilégier des outils techniques adaptés au\nstockage de données.\n\nL'idéal, dans notre cas, est d'utiliser une solution de stockage externe. \nOn va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. \nCela nous permettra de supprimer les données de `Github` tout en maintenant la reproductibilité \nde notre projet [^history].\n\n[^history]: Attention, les données ont été _committées_ au moins une fois. Les supprimer\ndu dépôt ne les efface pas de l'historique. Si cette erreur arrive, le mieux est de supprimer\nle dépôt en ligne, créer un nouvel historique `Git` et partir de celui-ci pour des publications\nultérieures sur `Github`. Néanmoins l'idéal serait de ne pas s'exposer à cela. C'est justement\nl'objet des bonnes pratiques de ce cours: un `.gitignore` bien construit et une séparation des\nenvironnements de stockage du code et\ndes données seront bien plus efficaces pour vous éviter ces problèmes que tout les conseils de \nvigilance que vous pourrez trouver ailleurs. \n\n::: {.callout-tip}\n\n## Application 8: utilisation d'un système de stockage distant\n\nA partir de la ligne de commande,\nutiliser l'utilitaire [MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html)\npour copier les données `data/raw/train.csv` et `data/raw/test.csv` vers votre\nbucket personnel, respectivement dans les dossiers `ensae-reproductibilite/data/raw/train.csv`\net `ensae-reproductibilite/data/raw/test.csv`. \n\n<details>\n<summary>Indice</summary>\n\nStructure à adopter:\n\n```{.bash filename=\"terminal\"}\nmc cp data/raw/train.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv\nmc cp data/raw/test.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv\n```\n\nen modifiant l'emplacement de votre bucket personnel\n</details>\n\nPour se simplifier la vie, on va utiliser des URL de téléchargement des fichiers\n(comme si ceux-ci étaient sur n'importe quel espace de stockage) plutôt que d'utiliser\nune librairie `S3` compatible comme `boto3` ou `s3fs`.\nPar défaut, le contenu de votre _bucket_ est privé, seul vous y avez accès. Néanmoins,\nvous pouvez rendre accessible à tous en lecture le contenu de votre _bucket_ en\nfaisant lui donnant des droits anonymes. \nPour cela, en ligne de\ncommande, faire:\n\n```{.bash filename=\"terminal\"}\nmc anonymous set download s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/\n```\n\nen modifiant `<BUCKET_PERSONNEL>`. Les URL de téléchargement seront de la forme \n`https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv`\net `https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv`\n\n- Modifier `configuration/config.yaml` pour utiliser directement les URL dans l'import ;\n- Modifier les valeurs par défaut dans votre code ; \n- Supprimer les fichiers `.csv` du dossier `data` de votre projet, on n'en a plus besoin vu qu'on les importe de l'extérieur ;\n- Vérifier le bon fonctionnement de votre application.\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli8\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n# Partie 2bis: packagisation de son projet (optionnel)\n\nCette série d'actions n'est pas forcément pertinente pour tous\nles projets. Elle fait un peu la transition entre la modularité\net la portabilité. \n\n## Étape 1 : proposer des tests unitaires (optionnel)\n\nNotre code comporte un certain nombre de fonctions génériques.\nOn peut vouloir tester leur usage sur des données standardisées,\ndifférentes de celles du Titanic.\n\nMême si la notion de tests unitaires\nprend plus de sens dans un _package_, nous pouvons proposer\ndans le projet des exemples d'utilisation de la fonction, ceci peut être pédagogique. \n\nNous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)\npour effectuer des tests unitaires. Cette approche nécessite quelques notions\nde programmation orientée objet ou une bonne discussion avec `ChatGPT`.\n\n::: {.callout-tip}\n\n## Application 9: test unitaire _(optionnel)_\n\nDans le dossier `tests/`, créer un fichier `test_create_variable_title.py`.\n\nEn s'inspirant de l'[exemple de base](https://docs.python.org/3/library/unittest.html#basic-example),\ncréer une classe `TestCreateVariableTitle` qui effectue les opérations suivantes:\n\n- Création d'une fonction `test_create_variable_title_default_variable_name` qui permet \nde comparer les objets suivants:\n\n    + Création d'un `DataFrame` de test :  \n\n    ```python\n    df = pd.DataFrame({\n                'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',\n                        'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath (Lily May Peel)',\n                        'Allen, Mr. William Henry', 'Moran, Mr. James',\n                        'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',\n                        'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',\n                        'Nasser, Mrs. Nicholas (Adele Achem)'],\n                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],\n                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n            })\n    ```\n\n    + Utilisation de la fonction `create_variable_title` sur ce `DataFrame`\n    + Comparaison au `DataFrame` attendu:\n\n    ```python\n    expected_result = pd.DataFrame({\n                'Title': ['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.', 'Mr.', 'Mr.', 'Master.', 'Mrs.', 'Mrs.'],\n                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],\n                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n            })\n    ```\n\n- Effectuer le test unitaire en ligne de commande avec `unittest` (`python -m unittest tests/test_create_variable_title.py`). Corriger le test unitaire en cas d'erreur. \n- Si le temps le permet, proposer des variantes pour tenir compte de paramètres (comme la variable `variable_name`)\nou d'exceptions (comme la gestion du cas _\"Dona\"_).\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli9\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n\n:::\n\n\n\n\n::: {.callout-note}\n\nLorsqu'on effectue des tests unitaires, on cherche généralement\nà tester le plus de lignes possibles de son code. On parle de\n__taux de couverture__ (_coverage rate_) pour désigner\nla statistique mesurant cela. \n\nCela peut s'effectuer de la manière suivante avec le package\n[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):\n\n```{.bash filename=\"terminal\"}\ncoverage run -m unittest tests/test_create_variable_title.py\ncoverage report -m\n```\n\n```{.python}\nName                                  Stmts   Miss  Cover   Missing\n-------------------------------------------------------------------\nsrc/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113\ntests/test_create_variable_title.py      21      1    95%   54\n-------------------------------------------------------------------\nTOTAL                                    55     22    60%\n```\n\nLe taux de couverture est souvent mis en avant par les gros\nprojets comme indicateur de leur qualité. Il existe d'ailleurs\ndes badges `Github` dédiés. \n:::\n\n\n\n\n## Étape 2 : transformer son projet en package (optionnel)\n\nNotre projet est modulaire, ce qui le rend assez simple à transformer\nen _package_, en s'inspirant de la structure du `cookiecutter` adapté, issu\nde [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).\n\nOn va créer un _package_ nommé `titanicml` qui encapsule\ntout notre code et qui sera appelé\npar notre script `main.py`. La structure attendue\nest la suivante:\n\n<details>\n<summary>Structure visée</summary>\n\n```\nensae-reproductibilite-application\n├── docs                                    ┐ \n│   ├── main.py                             │ \n│   └── notebooks                           │ Package documentation and examples\n│       └── titanic.ipynb                   │ \n├── configuration                           ┐ Configuration (pas à partager avec Git)\n│   └── config.yaml                         ┘ \n├── README.md                                \n├── pyproject.toml                          ┐ \n├── requirements.txt                        │\n├── titanicml                               │                \n│   ├── __init__.py                         │ Package source code, metadata\n│   ├── data                                │ and build instructions \n│   │   ├── import_data.py                  │  \n│   │   └── test_create_variable_title.py   │   \n│   ├── features                            │\n│   │   └── build_features.py               │\n│   └── models                              │\n│       └── train_evaluate.py               ┘\n└── tests                                   ┐\n    └── test_create_variable_title.py       ┘ Package tests\n```\n</details>\n\n<details>\n<summary>Rappel: structure actuelle</summary>\n\n```\nensae-reproductibilite-application\n├── notebooks                                 \n│   └── titanic.ipynb                  \n├── configuration                                 \n│   └── config.yaml                  \n├── main.py                              \n├── README.md                 \n├── requirements.txt                      \n└── src \n    ├── data                                \n    │   ├── import_data.py                    \n    │   └── test_create_variable_title.py      \n    ├── features                           \n    │   └── build_features.py      \n    └── models                          \n        └── train_evaluate.py              \n```\n</details>\n\nIl existe plusieurs \n_frameworks_ pour\nconstruire un _package_. Nous\nallons privilégier [`Poetry`](https://python-poetry.org/)\nà [`Setuptools`](https://pypi.org/project/setuptools/). \n\n\n::: {.callout-note}\n\nPour créer la structure minimale d'un _package_, le plus simple est\nd'utiliser le `cookiecutter` adapté,\nissu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).\n\nComme on a déjà une structure très modulaire, on va plutôt recréer cette\nstructure dans notre projet déjà existant. En fait, il ne manque qu'un fichier essentiel, \nle principal distinguant un projet classique d'un package : `pyproject.toml`.\n\n```{.bash filename=\"terminal\"}\ncookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git\n```\n\n<details>\n<summary>Dérouler pour voir les choix possibles</summary>\n```{.python}\nauthor_name [Monty Python]: Daffy Duck\npackage_name [mypkg]: titanicml\npackage_short_description []: Impressive Titanic survival analysis\npackage_version [0.1.0]: \npython_version [3.9]: \nSelect open_source_license:\n1 - MIT\n2 - Apache License 2.0\n3 - GNU General Public License v3.0\n4 - Creative Commons Attribution 4.0\n5 - BSD 3-Clause\n6 - Proprietary\n7 - None\nChoose from 1, 2, 3, 4, 5, 6 [1]: \nSelect include_github_actions:\n1 - no\n2 - ci\n3 - ci+cd\nChoose from 1, 2, 3 [1]:\n```\n</details>\n\n:::\n\n::: {.callout-tip}\n\n## Application 10: packagisation _(optionnel)_\n\n- Renommer le dossier `titanicml` pour respecter la nouvelle\narborescence ;\n- Créer un fichier `pyproject.toml` sur cette base ;\n\n::: {#5dae9aa5 .cell filename='pyproject.toml' execution_count=4}\n``` {.python .cell-code code-summary=\"pyproject.toml\"}\n[tool.poetry]\nname = \"titanicml\"\nversion = \"0.0.1\"\ndescription = \"Awesome Machine Learning project\"\nauthors = [\"Daffy Duck <daffy.duck@fauxmail.fr>\", \"Mickey Mouse\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = \"WARNING\"\nlog_cli_format = \"%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)\"\nlog_cli_date_format = \"%Y-%m-%d %H:%M:%S\"\n```\n:::\n\n\n- Créer le dossier `docs` et mettre les fichiers indiqués dedans\n- Dans `titanicml/`, créer un fichier `__init__.py`[^init]\n\n::: {#c1a330fa .cell filename='__init__.py' execution_count=5}\n``` {.python .cell-code code-summary=\"__init__.py\"}\nfrom .import_data import (\n    import_data, import_yaml_config\n)\nfrom .build_features import (\n    create_variable_title,\n    fill_na_titanic,\n    label_encoder_titanic,\n    check_has_cabin,\n    ticket_length\n)\nfrom .train_evaluate import random_forest_titanic\n\n__all__ = [\n    \"import_data\", \"import_yaml_config\",\n    \"create_variable_title\",\n    \"fill_na_titanic\",\n    \"label_encoder_titanic\",\n    \"check_has_cabin\",\n    \"ticket_length\",\n    \"random_forest_titanic\"\n]\n```\n:::\n\n\n- Installer le package en local avec `pip install -e .`\n- Modifier le contenu de `docs/main.py` pour importer les fonctions de notre _package_ `titanicml` et tester en \nligne de commande notre fichier `main.py`\n:::\n\n[^init]: Le fichier `__init__.py` indique à `Python` que le dossier\nest un _package_. Il permet de proposer certaines configurations\nlors de l'import du _package_. Il permet également de contrôler\nles objets exportés (c'est-à-dire mis à disposition de l'utilisateur)\npar le _package_ par rapport aux objets internes au _package_. \nEn le laissant vide, nous allons utiliser ce fichier \npour importer l'ensemble des fonctions de nos sous-modules. \nCe n'est pas la meilleure pratique mais un contrôle plus fin des\nobjets exportés demanderait un investissement qui ne vaut, ici, pas\nle coût. \n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli10\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n\n:::\n\n\n\n# Partie 3 : construction d'un projet portable et reproductible {#partie3}\n\nDans la partie précédente,\non a appliqué de manière incrémentale de nombreuses bonnes pratiques vues\ndans les chapitres [Qualité du code](/chapters/code-quality.html)\net [Structure des projets](/chapters/projects-architecture.html)\ntout au long du cours.\n\nCe faisant, on s'est déjà considérablement rapprochés d'une\npossible mise en production : le code est lisible,\nla structure du projet est normalisée et évolutive,\net le code est proprement versionné sur un\ndépôt `GitHub` {{< fa brands github >}}.\n\n\n<details>\n<summary>\nIllustration de l'état actuel du projet \n</summary>\n![](/schema_post_appli8.png)\n</details>\n\n\n\nA présent, nous avons une version du projet qui est largement partageable.\nDu moins en théorie, car la pratique est souvent plus compliquée :\nil y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),\nles choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.\n\nDans cette troisème partie de notre travail vers la mise en production,\nnous allons voir \ncomment **normaliser l'environnement d'exécution afin de produire un projet portable**.\nAutrement dit, nous n'allons plus nous contenter de modularité mais allons rechercher\nla portabilité.\nOn sera alors tout proche de pouvoir mettre le projet en production.\n\nOn progressera dans l'échelle de la reproductibilité \nde la manière suivante: \n\n1. [**Environnements virtuels**](#anaconda) ;\n2. Créer un [script shell](#shell) qui permet, depuis un environnement minimal, de construire l'application de A à Z ;\n3. [**Images et conteneurs `Docker`**](#docker).\n\n\nNous allons repartir de l'application 8, c'est-à-dire d'un projet\nmodulaire mais qui n'est pas, à strictement parler, un _package_\n(objet des applications optionnelles suivantes 9 et 10). \n\nPour se replacer dans l'état du projet à ce niveau,\nil est possible d'utiliser le _tag_ _ad hoc_.\n\n```{.bash filename=\"terminal\"}\ngit checkout appli8\n```\n\n\n## Étape 1 : un environnement pour rendre le projet portable {#anaconda}\n\nPour qu'un projet soit portable, il doit remplir deux conditions:\n\n- Ne pas nécessiter de dépendance\nqui ne soient pas renseignées quelque part ;\n- Ne pas proposer des dépendances inutiles, qui ne\nsont pas utilisées dans le cadre du projet. \n\nLe prochain exercice vise à mettre ceci en oeuvre.\nComme expliqué dans le [chapitre portabilité](/chapters/portability.qmd),\nle choix du gestionnaire d'environnement est laissé\nlibre. Il est recommandé de privilégier `venv` si vous découvrez\nla problématique de la portabilité. \n\n::: {.panel-tabset group=\"language\"}\n\n## Environnement virtuel `venv`\n\nL'approche la plus légère est l'environnement virtuel. \nNous avons en fait implicitement déjà commencé à aller vers\ncette direction\nen créant un fichier `requirements.txt`. \n\n:::: {.callout-tip}\n\n## Application 11a: environnement virtuel `venv` \n\n1. Exécuter `pip freeze` en ligne de commande et observer la (très) longue\nliste de package\n2. Créer l'environnement virtuel `titanic` en s'inspirant de [la documentation officielle](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)[^pythonversion] ou du [chapitre dédié](/chapters/portability.qmd)\n3. Utiliser `ls` pour observer et comprendre le contenu du dossier `titanic/bin` installé\n4. Activer l'environnement et vérifier l'installation de `Python` maintenant utilisée par votre machine \n<!---source titanic/bin/activate && which python---->\n5. Vérifier directement depuis la ligne de commande que `Python` exécute bien une commande[^option] avec:\n\n```{.bash filename=\"terminal\"}\npython -c \"print('Hello')\"\n```\n\n6. Faire la même chose mais avec `import pandas as pd`\n7. Installer les _packages_ à partir du `requirements.txt`. Tester à nouveau `import pandas as pd` pour comprendre la différence. \n8. Exécuter `pip freeze` et comprendre la différence avec la situation précédente.\n9. Vérifier que le script `main.py` fonctionne bien. Sinon ajouter les _packages_ manquants dans le `requirements.txt` et reprendre de manière itérative à partir de la question 7.\n10. Ajouter le dossier `titanic/` au `.gitignore` pour ne pas ajouter ce dossier à `Git`.\n\n\n<details>\n<summary>Aide pour la question 4</summary>\n\nAprès l'activation, vous pouvez vérifier quel `python` \nest utilisé de cette manière\n\n```{.bash filename=\"terminal\" env=\"titanic\"}\nwhich python\n```\n\n</details>\n\n::::\n\n[^pythonversion]: Si vous désirez aussi contrôler la version de `Python`, ce qui peut être important\ndans une perspective de portabilité, vous pouvez ajouter une option, par exemple `-p python3.10`. Néanmoins\nnous n'allons pas nous embarasser de cette nuance pour la suite car nous pourrons contrôler\nla version de `Python` plus finement par le biais de `Docker`.\n[^option]: L'option `-c` passée après la commande `python` permet d'indiquer à `Python` que la commande\nne se trouve pas dans un fichier mais sera dans le texte qu'on va directement lui fournir. \n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli11a\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n\n## Environnement `conda`\n\nLes environnements `conda` sont plus lourds à mettre en oeuvre que les \nenvironnements virtuels mais peuvent permettre un contrôle\nplus formel des dépendances. \n\n:::: {.callout-tip}\n\n## Application 11b: environnement `conda` \n\n1. Exécuter `conda env export` en ligne de commande et observer la (très) longue\nliste de package\n2. Créer un environnement `titanic`\navec [`conda create`](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands)\n4. Activer l'environnement et vérifier l'installation de `Python` maintenant utilisée par votre machine \n<!---conda activate titanic && which python---->\n5. Vérifier directement depuis la ligne de commande que `Python` exécute bien une commande[^option] avec:\n\n```{.bash filename=\"terminal\"}\npython -c \"print('Hello')\"\n```\n\n6. Faire la même chose mais avec `import pandas as pd`\n7. Installer les _packages_ qu'on avait listé dans le `requirements.txt` précédemment. Ne pas faire un `pip install -r requirements.txt` afin de privilégier `conda install`\n8. Exécuter à nouveau `conda env export` et comprendre la différence avec la situation précédente[^splitscreen].\n9. Vérifier que le script `main.py` fonctionne bien. Sinon installer les _packages_ manquants\net reprndre de manière itérative\nà partir de la question 7.\n10. Quand `main.py` fonctionne, faire `conda env export > environment.yml` pour figer l'environnement de travail.\n\n::::\n\n[^splitscreen]: Pour comparer les deux listes, vous pouvez utiliser la fonctionnalité de _split_ \ndu terminal sur `VSCode` pour comparer les outputs de `conda env export` en les mettant \nen face à face. \n\n:::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli11b\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n::::\n\n\n\n:::\n\n\n## Étape 2: construire l'environnement de notre application via un script `shell` {#shell}\n\nLes environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L'idée est de construire une machine, en partant d'une base quasi-vierge, qui permette de construire étape par étape l'environnement nécessaire au bon fonctionnement de notre projet. C'est le principe des conteneurs `Docker` {{< fa brands docker >}}.\n\nLeur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production. \n\n- Nous allons d'abord créer un script `shell`, c'est à dire une suite de commandes `Linux` permettant de construire l'environnement à partir d'une machine vierge ;\n- Nous transformerons celui-ci en `Dockerfile` dans un deuxième temps. C'est l'objet de l'étape suivante. \n\n::: {.panel-tabset group=\"language\"}\n\n## Environnement virtuel `venv`\n\n:::: {.callout-tip}\n\n## Application 12a : créer un fichier d'installation de A à Z\n\n1. Créer un service `ubuntu` sur le SSP Cloud\n2. Ouvrir un terminal\n3. Cloner le dépôt \n4. Se placer dans le dossier du projet avec `cd`\n5. Se placer au niveau du checkpoint 11a avec `git checkout appli11a`\n6. Via l'explorateur de fichiers, créer le fichier `install.sh` à la racine du projet avec le contenu suivant:\n\n<details>\n<summary>Script à créer sous le nom `install.sh` </summary>\n```{.bash filename=\"install.sh\" no-prefix=true}\n#!/bin/bash\n\n# Install Python\napt-get -y update\napt-get install -y python3-pip python3-venv\n\n# Create empty virtual environment\npython3 -m venv titanic\nsource titanic/bin/activate\n\n# Install project dependencies\npip install -r requirements.txt\n```\n</details>\n\n6. Changer les permissions sur le script pour le rendre exécutable\n\n```{.bash filename=\"terminal\"}\nchmod +x install.sh\n```\n\n7. Exécuter le script depuis la ligne de commande avec des droits de super-utilisateur (nécessaires pour installer des *packages* via `apt`)\n\n```{.bash filename=\"terminal\"}\nsudo ./install.sh\n```\n\n8. Vérifier que le script `main.py` fonctionne correctement dans l'environnement virtuel créé \n\n```{.bash filename=\"terminal\"}\nsource titanic/bin/activate\npython3 main.py\n```\n\n::::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli12a\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n## Environnement `conda`\n\n:::: {.callout-tip}\n\n## Application 12b : créer un fichier d'installation de A à Z\n\n1. Créer un service `ubuntu` sur le SSP Cloud\n2. Ouvrir un terminal\n3. Cloner le dépôt \n4. Se placer dans le dossier du projet avec `cd`\n5. Se placer au niveau du checkpoint 11b avec `git checkout appli11b`\n6. Via l'explorateur de fichiers, créer le fichier `install.sh` à la racine du projet avec le contenu suivant:\n\n<details>\n<summary>Script à créer sous le nom `install.sh` </summary>\n```{.bash filename=\"install.sh\" no-prefix=true}\napt-get -y update && apt-get -y install wget\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \\\n    rm -f Miniconda3-latest-Linux-x86_64.sh\n\nPATH=\"/miniconda/bin:${PATH}\"\n\n# Create environment\nconda create -n titanic pandas PyYAML scikit-learn -c conda-forge\nconda activate titanic\n\nPATH=\"/miniconda/envs/titanic/bin:${PATH}\"\n\npython main.py\n```\n</details>\n\n6. Changer les permissions sur le script pour le rendre exécutable\n\n```{.bash filename=\"terminal\"}\nchmod +x install.sh\n```\n\n7. Exécuter le script depuis la ligne de commande avec des droits de super-utilisateur (nécessaires pour installer des *packages* via `apt`)\n\n```{.bash filename=\"terminal\"}\nsudo ./install.sh\n```\n\n8. Vérifier que le script `main.py` fonctionne correctement dans l'environnement virtuel créé \n\n```{.bash filename=\"terminal\"}\nconda activate titanic\npython3 main.py\n```\n\n::::\n\n:::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli12b\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n::::\n\n\n\n:::\n\n\n## Étape 3: conteneuriser l'application avec `Docker` {#docker}\n\n\n::: {.callout-note}\nCette application nécessite l'accès à une version interactive de `Docker`.\nIl n'y a pas beaucoup d'instances en ligne disponibles.\n\nNous proposons deux solutions:\n\n- [Installer `Docker`](https://docs.docker.com/get-docker/) sur sa machine ;\n- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_\n\nSinon, elle peut être réalisée en essai-erreur par le biais des services d'intégration continue de `Github` {{< fa brands github >}} ou `Gitlab` {{< fa brands gitlab >}}. Néanmoins, nous présenterons l'utilisation de ces services plus tard, dans la prochaine partie. \n:::\n\nMaintenant qu'on sait que ce script préparatoire fonctionne, on va le transformer en `Dockerfile` pour anticiper la mise en production.  Comme la syntaxe `Docker` est légèrement différente de la syntaxe `Linux` classique (voir le [chapitre portabilité](/chapters/portability.qmd)), il va être nécessaire de changer quelques instructions mais ceci sera très léger.\n\nOn va tester le `Dockerfile` dans un environnement bac à sable pour ensuite\npouvoir plus facilement automatiser la construction de l'image\n`Docker`.\n\n::: {.callout-tip}\n\n## Application 13: création de l'image `Docker` \n\nSe placer dans un environnement avec `Docker`, par\nexemple _[Play with Docker](https://labs.play-with-docker.com)_\n\n#### Création du `Dockerfile`\n\n- Dans le terminal `Linux`, cloner votre dépôt `Github` \n- Repartir de la dernière version à disposition. Par exemple, si vous avez privilégié \nl'environnement virtuel `venv`, ce sera:\n\n```{.bash filename=\"terminal\"}\ngit checkout appli12a\n```\n\n- Créer via la ligne de commande un fichier texte vierge nommé `Dockerfile` (la majuscule au début du mot est importante)\n\n<details><summary>Commande pour créer un `Dockerfile` vierge depuis la ligne de commande</summary>\n```{.bash filename=\"terminal\"}\ntouch Dockerfile\n```\n</details>\n\n- Ouvrir ce fichier via un éditeur de texte et copier le contenu suivant dedans:\n\n<details><summary>Premier `Dockerfile`</summary>\n\n```{.bash filename=\"terminal\" no-prefix=true}\nFROM ubuntu:22.04\n\nWORKDIR ${HOME}/titanic\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCMD [\"python3\", \"main.py\"]\n```\n</details>\n\n#### Construire (_build_) l'image\n\n- Utiliser `docker build` pour créer une image avec le tag `my-python-app`\n\n```{.bash filename=\"terminal\"}\ndocker build . -t my-python-app\n```\n\n- Vérifier les images dont vous disposez. Vous devriez avoir un résultat proche de celui-ci :\n\n```{.bash filename=\"terminal\"}\ndocker images\n```\n\n```{.python}\nREPOSITORY      TAG       IMAGE ID       CREATED              SIZE\nmy-python-app   latest    188957e16594   About a minute ago   879MB\n```\n\n#### Tester l'image: découverte du cache\n\nL'étape de `build` a fonctionné: une image a été construite.\n\nMais fait-elle effectivement ce que l'on attend d'elle ?\n\nPour le savoir, il faut passer à l'étape suivante, l'étape de `run`.\n\n```{.bash filename=\"terminal\"}\ndocker run -it my-python-app\n```\n\n```{.python}\npython3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory\n```\n\nLe message d'erreur est clair : `Docker` ne sait pas où trouver le fichier `main.py`. D'ailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont nécessaires pour faire tourner le code, par exemple le dossier `src`.\n\n- Avant l'étape `CMD`, copier les fichiers nécessaires sur l'image afin que l'application dispose de tous les éléments nécessaires pour être en mesure de fonctionner.\n\n<details>\n<summary>Nouveau `Dockerfile` </summary>\n```{.bash filename=\"terminal\" no-prefix=true}\nFROM ubuntu:22.04\n\nWORKDIR ${HOME}/titanic\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY main.py .\nCOPY src ./src\nCMD [\"python3\", \"main.py\"]\n```\n</details>\n\n- Refaire tourner l'étape de `build`\n\n- Refaire tourner l'étape de `run`. A ce stade, la matrice de confusion doit fonctionner 🎉.\nVous avez créé votre première application reproductible !\n\n:::\n\n::: {.callout-note}\n\nIci, le _cache_ permet d'économiser beaucoup de temps. Par besoin de \nrefaire tourner toutes les étapes, `Docker` agit de manière intelligente\nen faisant tourner uniquement les étapes qui ont changé.\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli13\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n\n# Partie 4 : automatisation avec l'intégration continue\n\n\nImaginez que vous êtes au restaurant\net qu'on ne vous serve pas le plat mais seulement la recette\net que, de plus, on vous demande de préparer le plat\nchez vous avec les ingrédients dans votre frigo.\nVous seriez quelque peu déçu. En revanche, si vous avez goûté\nau plat, que vous êtes un réel cordon bleu\net qu'on vous donne la recette pour refaire ce plat ultérieurement,\npeut-être\nque vous appréciriez plus. \n\nCette analogie illustre l'enjeu de définir\nle public cible et ses attentes afin de fournir un livrable adapté. \nUne image `Docker` est un livrable qui n'est pas forcément intéressant\npour tous les publics. Certains préféreront avoir un plat bien préparé\nqu'une recette ; certains apprécieront avoir une image `Docker` mais\nd'autres ne seront pas en mesure de construire celle-ci ou ne sauront\npas la faire fonctionner. Une image `Docker` est plus souvent un \nmoyen pour faciliter la mise en service d'une production qu'une fin en soi. \n\nNous allons donc proposer\nplusieurs types de livrables plus classiques par la suite. Ceux-ci\ncorrespondront mieux aux attendus des publics utilisateurs de services\nconstruits à partir de techniques de _data science_. `Docker` est néanmoins\nun passage obligé car l'ensemble des types de livrables que nous allons\nexplorer reposent sur la standardisation permise par les conteneurs. \n\nCette approche nous permettra de quitter le domaine de l'artisanat pour\ns'approcher d'une industrialisation de la mise à disposition \nde notre projet. Ceci va notamment nous amener à mettre en oeuvre\nl'approche pragmatique du `DevOps` qui consiste à intégrer dès la phase de\ndéveloppement d'un projet les contraintes liées à sa mise à disposition\nau public cible (cette approche est détaillée plus\namplement dans le chapitre sur la [mise en production](/chapters/deployment.qmd)). \n\nL'automatisation et la mise à disposition automatisée de nos productions\nsera faite progressivement, au cours des prochaines parties. Tous les \nprojets n'ont pas vocation à aller aussi loin dans ce domaine. \nL'opportunité doit être comparée aux coûts humains et financiers\nde leur mise en oeuvre et de leur cycle de vie. \nAvant de faire une production en série de nos modèles,\nnous allons déjà commencer\npar automatiser quelques tests de conformité de notre code. \nOn va ici utiliser l'intégration continue pour deux objectifs distincts:\n\n- la mise à disposition de l'image `Docker` ;\n- la mise en place de tests automatisés de la qualité du code\nsur le modèle de notre `linter` précédent.\n\nNous allons utiliser `Github Actions` pour cela. Il s'agit de serveurs\nstandardisés mis à disposition gratuitement par `Github` {{<fa brands github >}}.\n`Gitlab` {{<fa brands gitlab >}}, l'autre principal acteur du domaine,\npropose des services similaires. L'implémentation est légèrement différente\nmais les principes sont identiques. \n\n\n::: {.callout-caution collapse=\"true\"}\n## Si vous prenez ce projet fil rouge en cours de route\n\n```{.bash filename=\"terminal\"}\ngit checkout appli13\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n## Étape 1: mise en place de tests automatisés\n\nAvant d'essayer de mettre en oeuvre la création de notre image\n`Docker` de manière automatisée, nous allons présenter la logique\nde l'intégration continue en testant de manière automatisée\nnotre script `main.py`.\n\nPour cela, nous allons partir de la structure proposée dans l'[action officielle](https://github.com/actions/setup-python). \nLa documentation associée est [ici](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python).\nDes éléments succincts de présentation de la logique déclarative des actions `Github` \nsont disponibles dans le chapitre sur la [mise en production](/chapters/deployment.qmd). Néanmoins, la meilleure\nécole pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d'observer\nles actions `Github` mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !\n\n\n::: {.callout-tip}\n\n## Application 14: premier script d'intégration continue\n\nA partir de l'exemple présent\ndans la [documentation officielle](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python#using-a-specific-python-version)\nde `Github` {{< fa brands github >}}, on a déjà une base de départ qui peut être modifiée. \nLes questions suivantes permettront d'automatiser les tests et le diagnostic qualité de\nnotre code[^failure]\n\n[^failure]: Il est tout à fait normal de ne pas parvenir à créer une action fonctionnelle\ndu premier coup. N'hésitez pas à _pusher_ votre code après chaque question pour vérifier\nque vous parvenez bien à réaliser chaque étape. Sinon vous risquez de devoir corriger\nbout par bout un fichier plus conséquent. \n\n1. Créer un fichier `.github/workflows/test.yaml` avec le contenu de l'exemple de la documentation\n3. Avec l'aide de la documentation, introduire une étape d'installation des dépendances. \nUtiliser le fichier `requirements.txt` pour installer les dépendances. \n4. Utiliser `pylint` pour vérifier la qualité du code. Ajouter l'argument `--fail-under=6` pour\nrenvoyer une erreur en cas de note trop basse[^hook]\n5. Utiliser une étape appelant notre application en ligne de commande (`python main.py`)\npour tester que la matrice de confusion s'affiche bien.\n6. Aller voir votre test automatiser dans l'onglet `Actions` de votre dépôt sur `Github`\n\n[^hook]: Il existe une approche alternative pour faire des tests\n    réguliers: les _hooks_ `Git`.\n    Il s'agit de règles qui doivent être satisfaites pour que le \n    fichier puisse être committé. Cela assure que chaque `commit` remplisse\n    des critères de qualité afin d'éviter le problème de la procrastination.\n    \n    La [documentation de pylint](https://pylint.pycqa.org/en/latest/user_guide/pre-commit-integration.html) offre des explications supplémentaires. \n    Ici, nous allons adopter une approche moins ambitieuse en demandant à notre\n    action de faire ce travail d'évaluation de la qualité de notre code\n\n\n\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli14\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n \nMaintenant, nous pouvons observer que l'onglet `Actions`\ns'est enrichi. Chaque `commit` va entraîner une séries d'actions automatisées.\n\nSi l'une des étapes échoue, ou si la note de notre projet est mauvaise, nous aurons\nune croix rouge (et nous recevrons un mail). On pourra ainsi détecter,\nen développant son projet, les moments où on dégrade la qualité du script \nafin de la rétablir immédiatemment. \n\n\n\n## Étape 2: Automatisation de la livraison de l'image `Docker`\n\nMaintenant, nous allons automatiser la mise à disposition de notre image\nsur `DockerHub` (le lieu de partage des images `Docker`). Cela facilitera sa réutilisation mais aussi des\nvalorisations ultérieures.\n\nLà encore, nous allons utiliser une série d'actions pré-configurées.\n\nPour que `Github` puisse s'authentifier auprès de `DockerHub`, il va \nfalloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser\nun jeton (_token_) `DockerHub` que nous allons mettre dans un espace\nsécurisé associé à votre dépôt `Github`.\n\n\n::: {.callout-tip}\n## Application 15a: configuration\n\n- Se rendre sur\nhttps://hub.docker.com/ et créer un compte. Il est recommandé\nd'associer ce compte à votre compte `Github`. \n- Créer un dépôt public `application-correction`\n- Aller dans les [paramètres de votre compte](https://hub.docker.com/settings/general)\net cliquer, à gauche, sur `Security`\n- Créer un jeton personnel d'accès, ne fermez pas l'onglet en question,\nvous ne pouvez voir sa valeur qu'une fois. \n- Dans le dépôt `Github` de votre projet, cliquer sur l'onglet `Settings` et cliquer,\nà gauche, sur `Secrets and variables` puis\ndans le menu déroulant en dessous sur `Actions`. Sur la page qui s'affiche, aller\ndans la section `Repository secrets`\n- Créer un jeton `DOCKERHUB_TOKEN` à partir du jeton que vous aviez créé sur `Dockerhub`. Valider\n- Créer un deuxième secret nommé `DOCKERHUB_USERNAME` ayant comme valeur le nom d'utilisateur\nque vous avez créé sur `Dockerhub`\n\n<details>\n<summary>\nEtape optionnelle supplémentaire si on met en production un site web\n</summary>\n\n- Dans le dépôt `Github` de votre projet, cliquer sur l'onglet `Settings` et cliquer,\nà gauche, sur `Actions`. Donner les droits d'écriture à vos actions sur le dépôt\ndu projet (ce sera nécessaire pour `Github Pages`)\n\n![](/permissions.png)\n\n</details>\n\n:::\n\n\n\n\nA ce stade, nous avons donné les moyens à `Github` de s'authentifier avec\nnotre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action\nen s'inspirant de la [documentation officielle](https://github.com/docker/build-push-action/#usage).\nOn ne va modifier que trois éléments dans ce fichier. Effectuer les \nactions suivantes:\n\n\n::: {.callout-tip}\n\n## Application 15b: automatisation de l'image `Docker`\n\n- En s'inspirant de ce [_template_](https://github.com/marketplace/actions/build-and-push-docker-images), créer le fichier `.github/workflows/prod.yml` qui va *build* et *push* l'image sur le `DockerHub`. Il va être nécessaire de changer légèrement ce modèle :\n    + Retirer la condition restrictive sur les _commits_ pour lesquels sont lancés cette automatisation. Pour cela, remplacer le contenu de `on` de sorte à avoir `on: [push]`\n    + Changer le tag à la fin pour mettre `username/application-correction:latest`\noù `username` est le nom d'utilisateur sur `DockerHub`;\n    + Optionnel: changer le nom de l'action\n\n- Faire un `commit` et un `push` de ces fichiers\n\nComme on est fier de notre travail, on va afficher ça avec un badge sur le \n`README` _(partie optionnelle)_. \n\n- Se rendre dans l'onglet `Actions` et cliquer sur une des actions listées. \n- En haut à droite, cliquer sur `...`\n- Sélectionner `Create status badge`\n- Récupérer le code `Markdown` proposé\n- Copier dans votre `README.md` le code _markdown_ proposé\n\n<details>\n<summary>\nCréer le badge\n</summary>\n![](/create-badge.png)\n</details>\n\n:::\n\nMaintenant, il nous reste à tester notre application dans l'espace bac à sable\nou en local, si `Docker` est installé.\n\n\n::: {.callout-tip}\n\n## Application 15b (partie optionnelle): Tester l'application\n\n- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_\nou dans votre environnement `Docker` de prédilection.\n- Récupérer et lancer l'image :\n\n```yaml\ndocker run -it username/application-correction:latest\n```\n\n🎉 La matrice de confusion doit s'afficher ! Vous avez grandement\nfacilité la réutilisation de votre image. \n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli15\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\n\n\n\n# Partie 5: expérimenter en local des valorisations puis automatiser leur production\n\n\nNous avons automatisé les étapes intermédiaires de notre projet. \nNéanmoins nous n'avons pas encore réfléchi à la valorisation\nà mettre en oeuvre pour notre projet. On va supposer que notre\nprojet s'adresse à des _data scientists_ mais aussi à une audience\nmoins technique. Pour ces premiers, nous pourrions nous contenter\nde valorisations techniques, comme des API, \nmais pour ces derniers il est\nconseillé de privilégier des formats plus _user friendly_. \n\n::: {.callout-caution collapse=\"true\"}\n## Si vous prenez ce projet fil rouge en cours de route\n\n```{.bash filename=\"terminal\"}\ngit checkout appli15\n```\n\n![](/checkpoint.jpg){width=80% fig-align=\"center\"}\n\n:::\n\n\nAfin de faire le parallèle avec les parcours possibles pour l'évaluation,\nnous allons proposer trois valorisations[^valorisation]:\n\n- Une API facilitant la réutilisation du modèle en \"production\" ;\n- Un site web statique exploitant cette API pour exposer les prédictions\nà une audience moins technique. Pour illustrer\nles enjeux spécifiques aux applications réactives mises en oeuvre\navec `Streamlit`, nous proposerons aussi cette valorisation même si, dans\nnotre cas, elle n'apportera pas plus que le site statique. \n\n[^valorisation]: Vous n'êtes pas obligés pour l'évaluation de mettre en oeuvre\nles jalons de plusieurs parcours. Néanmoins, vous découvrirez que \nchaque nouveau pas en avant est moins coûteux que le\nprécédent si vous avez mis en oeuvre les réflexes des bonnes\npratiques.  \n\n::: {.callout-warning collapse=\"true\"}\n## Site statique vs application réactive\n\nLa solution que nous allons proposer \npour les sites statiques, `Quarto` associé\nà `Github Pages`, peut être utilisée dans le cadre des parcours \n_\"rapport reproductible\"_ ou _\"dashboard / application interactive\"_. \n\nPour ce dernier\nparcours, d'autres approches techniques sont néanmoins possibles,\ncomme `Streamlit`. Celles-ci sont plus exigeantes sur le plan technique\npuisqu'elles nécessitent de mettre en production sur des serveurs\nconteuneurisés (comme la mise en production de l'API)\nlà où le site statique ne nécessite qu'un serveur web, mis à disposition\ngratuitement par `Github`. \n\n\nLa distinction principale entre ces deux approches est qu'elles\ns'appuient sur des serveurs différents. Un site statique repose\nsur un serveur web là où `Streamlit` s'appuie sur \nserveur classique en _backend_. La différence principale\nentre ces deux types de serveurs\nréside principalement dans leur fonction et leur utilisation:\n\n- Un __serveur web__ est spécifiquement conçu pour stocker, traiter et livrer des pages web aux clients. Cela inclut des fichiers HTML, CSS, JavaScript, images, etc. Les serveurs web écoutent les requêtes HTTP/HTTPS provenant des navigateurs des utilisateurs et y répondent en envoyant les données demandées.\n- Un **serveur _backend_** classique est conçu pour effectuer des opérations en réponse à un _front_, en l'occurrence une page web. \nDans le contexte d'une application `Streamlit`, il s'agit d'un serveur avec l'environnement `Python` _ad hoc_ pour\nexécuter le code nécessaire à répondre à toute action d'un utilisateur de l'appliacation. \n\n:::\n\n\n## Étape préliminaire: création d'un _pipeline_ `scikit`\n\nLa mise en\nproduction nécessite d'être exigeant sur la mise en oeuvre opérationnelle\nde notre _pipeline_. Nous avons néanmoins un _pipeline_ un peu bancal\ncar il requiert d'être vigilant dans la manière d'enchaîner les\nétapes de _preprocessing_, d'entraînement et d'évaluation.\n\nQuand on utilise `scikit`, la bonne pratique est d'utiliser\nles [_pipelines_](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\nqui sécurisent les étapes de _feature engineering_ nécessaires avant la mise en oeuvre d'un modèle, qu'il\nque\nce soit pour l'entraînement ou pour appliquer les mêmes opérations avec les mêmes paramètres sur \nsur un nouveau jeu de données avant de faire un _predict_. \n\nOn va donc devoir refactoriser notre application pour utiliser un _pipeline_ `scikit`. \nLes raisons sont expliquées plus en détail [ici](https://scikit-learn.org/stable/common_pitfalls.html).\nCela aura\négalement l'avantage de rendre les étapes de notre _pipeline_ plus lisibles lorsqu'on passera à\nl'étape d'industrialisation avec `MLFLow`. \n\n\n::: {.callout-tip}\n\n## Application 16 _(optionnelle)_: Un _pipeline_ de _machine learning_\n\nCette application est __optionnelle__ car elle relève plutôt d'un cours\nde _machine learning_ que de cet enseignement. \nLes instructions sont donc minimales pour laisser de la marge de manoeuvre.\n\n- Simplifier le code de `split_train_test_titanic` pour\nrenvoyer deux `DataFrames`: `train` et `test` au lieu\ndes 4 _arrays_ `Numpy` comme jusqu'à présent\n\n- Créer une fonction `build_pipeline` dans `src/models/train_evaluate.py`\nqui :\n    + Reprend les arguments de `random_forest_titanic`\n    + Effectue le _preprocessing_ suivant pour les variables numériques (à définir):\nune imputation à la valeur médiane, un `MinMaxScaler` ensuite\n    + Effectue le _preprocessing_ suivant pour les variables catégorielles (à définir):\nune imputation à la valeur la plus fréquente, un _one hot encoding_ ensuite\n    + Définit une _random forest_ avec le nombre d'arbre donné en argument de la fonction\n\n- Modifier `main.py` pour que ce soit à ce niveau\nqu'a lieu le découpage en train/test, l'entrainement\net l'évaluation\ndu modèle (qui est donc à exfiltrer de `src/models/train_evaluate.py`)\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli16\n```\n\n:::\n\n## Étape 2: développer une API en local\n\n::: {.callout-tip}\n\n## Application 17: Mise à disposition sous forme d'API locale\n\n- Créer un nouveau service `SSPCloud` en paramétrant dans l'onglet\n`Networking` le port 5000 ;\n- Cloner le dépôt et se placer au niveau de l'application précédente (`git checkout appli16`)\n- Installer `fastAPI` et `uvicorn` puis les ajouter au `requirements.txt`\n- Renommer le fichier `main.py` en `train.py` et insérer le contenu suivant dedans :\n\n<details>\n<summary>\nFichier `train.py`\n</summary>\nRécupérer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/appli17/train.py)\n</details>\n\n- Créer le fichier `api.py` permettant d'initialiser l'API:\n\n<details>\n<summary>\nFichier `api.py`\n</summary>\nRécupérer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/appli17/api.py)\n</details>\n\n- Exécuter `train.py` pour stocker en local le modèle entraîné\n- Ajouter `model.joblib` au `.gitignore`\n- Déployer en local l'API avec la commande\n\n```{.bash filename=\"terminal\"}\nuvicorn api:app --reload --host \"0.0.0.0\" --port 5000\n```\n\n- A partir du `README` du service, se rendre sur l'URL de déploiement, \najouter `/docs/` à celui-ci et observer la documentation de l'API \n- Se servir de la documentation pour tester les requêtes `/predict`\n- Récupérer l'URL d'une des requêtes proposées. La tester dans le navigateur\net depuis `Python` avec `requests` (`requests.get(url).json()`)\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli17\n```\n\n:::\n\n## Étape 3: déployer l'API\n\nA ce stade, nous avons déployé l'API seulement localement, dans le cadre d'un service. Ce mode de déploiement est très pratique pour la phase de développement, afin de s'assurer que l'API fonctionne comme attendu. A présent, il est temps de passer à l'étape de déploiement, qui permettra à notre API d'être accessible via une URL sur le web, et donc aux utilisateurs potentiels de la requêter. Pour se faire, on va utiliser les possibilités offertes par `Kubernetes`, sur lequel est basé le [SSP Cloud](https://datalab.sspcloud.fr).\n\n::: {.callout-tip}\n\n## Application 18: Dockeriser l'API\n\n- Modifier le `Dockerfile` pour tenir compte des changements dans les noms de fichier effecutés dans l'application précédente\n\n- Créer un script `run.sh` à la racine du projet qui lance le script `train.py` puis déploie localement l'API \n\n<details>\n<summary>Fichier `run.sh`</summary>\n\n```{.bash filename=\"terminal\"}\n#/bin/bash\n\npython3 train.py\nuvicorn api:app --reload --host \"0.0.0.0\" --port 5000\n```\n</details>\n\n- Donner au script `run.sh` des permissions d'exécution : `chmod +x run.sh`\n\n- Changer l'instruction `CMD` du `Dockerfile` pour exécuter le script `run.sh` au lancement du conteneur\n\n- *Commit* et *push* les changements\n\n- Une fois le CI terminé, récupérer la nouvelle image dans l'environnement de test et vérifier que l'API se déploie correctement\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli18\n```\n:::\n\n::: {.callout-tip}\n\n## Application 19: Déployer l'API\n\n- Créer un dossier `deployment` à la racine du projet qui va contenir les fichiers de configuration nécessaires pour déployer sur un cluster `Kubernetes`\n\n- En vous inspirant de la [documentation](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), y ajouter un premier fichier `deployment.yaml` qui va spécifier la configuration du *Pod* à lancer sur le cluster\n\n<details>\n<summary>Fichier `deployment/deployment.yaml`</summary>\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n        image: linogaliana/application-correction:latest\n        ports:\n        - containerPort: 5000\n```\n</details>\n\n- En vous inspirant de la [documentation](https://kubernetes.io/fr/docs/concepts/services-networking/service/#d%C3%A9finition-d-un-service), y ajouter un second fichier `service.yaml` qui va créer une ressource `Service` permettant de donner une identité fixe au `Pod` précédemment créé au sein du cluster\n\n<details>\n<summary>Fichier `deployment/service.yaml`</summary>\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: titanic-service\nspec:\n  selector:\n    app: titanic\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 5000\n```\n</details>\n\n- En vous inspirant de la [documentation](https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource), y ajouter un troisième fichier `ingress.yaml` qui va créer une ressource `Ingress` permettant d'exposer le service via une URL en dehors du cluster\n\n<details>\n<summary>Fichier `deployment/ingress.yaml`</summary>\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: titanic-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - titanic.kub.sspcloud.fr\n  rules:\n  - host: titanic.kub.sspcloud.fr\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: titanic-service\n            port:\n              number: 80\n```\n</details>\n\n- Appliquer ces fichiers de configuration sur le cluster : `kubectl apply -f deployement/`\n\n- Si tout a correctement fonctionné, vous devriez pouvoir accéder à l'API à l'URL spécifiée dans le fichier `deployment/ingress.yaml`\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli19\n```\n\n:::\n\n\n# Partie 6: un workflow complet de MLOps\n\nCe sera l'an prochain, désolé !\n\n# Partie 7: livrer un site web de manière automatisée\n\nOn va proposer un nouveau livrable pour parler à un public plus large.\nPour cela, on va déployer un site web statique qui permet de visualiser\nrapidement les résultats du modèle.\n\nOn propose de créer un site web qui permet de comprendre, avec l'appui\ndes [valeurs de Shapley](https://christophm.github.io/interpretable-ml-book/shapley.html),\nles facteurs qui auraient pu nous mettre la puce\nà l'oreille sur les destins de Jake et de Rose. \n\nPour faire ce site web,\non va utiliser `Quarto` et déployer sur `Github Pages`.\nDes étapes préliminaires sont réalisées en `Python` \npuis l'affichage interactif \nsera contrôlé par du `JavaScript` grâce\nà des [blocs `Observable`](https://quarto.org/docs/interactive/ojs/). \n\n\n::: {.callout-tip}\n\n## Application 19: Déploiement automatisé d'un site web\n\nDans un premier temps, on va créer un projet `Quarto`\nau sein de notre dépôt: \n\n- Installer `Quarto` dans votre environnement local (s'il n'est pas déjà disponible) ;\n- Dans le projet, utiliser la commande `quarto create-project` pour initialiser le projet `Quarto` ;\n- Supprimer le fichier automatiquement généré avec l'extension `.qmd` ;\n- Récupérer le contenu du modèle de fichier `Quarto Markdown` [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/index.qmd). Celui-ci permet de générer la page d'accueil de notre site. Enregistrer dans un fichier nommé `index.qmd`\n\nOn teste ensuite la compilation en local du fichier:\n\n- Modifier le fichier `train.py` à partir de [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/train.py) pour être en mesure de compiler le fichier \n- Exécuter le fichier `train.py`\n- En ligne de commande, faire `quarto preview` (ajouter les arguments `--port 5000 --host 0.0.0.0` si vous passez par le `SSPCloud`)\n- Observer le site web généré en local\n\nEnfin, on va construire et déployer automatiquement ce site web grâce au\ncombo `Github Actions` et `Github Pages`:\n\n- Créer une branche `gh-pages` à partir du contenu de [cette page](https://quarto.org/docs/publishing/github-pages.html)\n- Créer un fichier `.github/workflows/website.yaml` avec le contenu de [ce fichier](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/.github/workflows/publish.yaml)\n\n:::\n\n::: {.callout-note}\n\nOn doit dans cette application modifier le fichier `train.py`\npour enregistrer en local une duplication du modèle\nde _machine learning_ et de l'ensemble d'entraînement\ncar pour ces deux éléments\non n'est pas allé au bout de la démarche MLOps\nd'enregistrement dans un _model registry_ et un\n_feature store_.\n\nDans la prochaine version de ce cours, qui\nintègrera `MLFlow`, on aura une démarche plus \npropre car on utilisera bien le modèle de production\net le jeu d'entrainement associé. \n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## Checkpoint\n\n```{.bash filename=\"terminal\"}\ngit checkout appli20\n```\n\n:::\n\n",
    "supporting": [
      "application_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}